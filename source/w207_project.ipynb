{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4dbced162440c393a0a5b7e5aee344711a30e994"
   },
   "source": [
    "# Facial Keypoint Detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "369fa247a546e39d82bdfdc5b7d4ed58baa40e4f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/user1/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/user1/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/user1/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/user1/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/user1/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/user1/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.backend.tensorflow_backend import clear_session\n",
    "from keras.backend.tensorflow_backend import get_session\n",
    "from tensorflow.keras.layers import LeakyReLU, ReLU\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Activation, Convolution2D, BatchNormalization, Flatten\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, MaxPool2D\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "from keras import backend\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import cv2\n",
    "import os, gc, json, math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n",
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print(keras.__version__)\n",
    "import tensorflow\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pixels(data):\n",
    "    \"\"\"\n",
    "    Convert pixels to the right intensity 0-1 and in a square matrix.\n",
    "    \"\"\"\n",
    "    data = np.array([row.split(' ') for row in data['Image']],dtype='float') / 255.0\n",
    "    data = data.reshape(-1,96,96,1)\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_img(sample_img,coord=None):\n",
    "    \"\"\"\n",
    "    Display an image. For debugging, display a coordinate on the image.\n",
    "    input:\n",
    "        - sample_img: numpy array. image to be displayed\n",
    "        - coord: lst. of coordinates in form [[x_coordinate,y_coordinate],[x_coordinate,y_coordinate]]\n",
    "    TODO handle multiple coordinates. Work out bugs with multiple coordinates\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.imshow(sample_img.reshape(96,96),cmap='gray')\n",
    "    if coord is not None:\n",
    "        plt.scatter(coord[0],coord[1],marker = '*',c='r')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_facial_keypoints(data,ind):\n",
    "    \"\"\"\n",
    "    Structure the coordinates for all facial keypoints for a single image.\n",
    "    inputs:\n",
    "        - data: numpy array containing rows as each image sample and columns as facial keypoint coordinates\n",
    "        - ind: index of the image\n",
    "    output:\n",
    "        - numpy array with format [[list of x-coordinates],[list of y-coordinates]]\n",
    "    \"\"\"\n",
    "    data[ind]\n",
    "    it = iter(data[ind])\n",
    "    x_coord = []\n",
    "    y_coord = []\n",
    "\n",
    "    for x in it:\n",
    "        x_coord.append(x)\n",
    "        y_coord.append(next(it))\n",
    "    \n",
    "    return(np.array([x_coord,y_coord]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Reset Keras Session\n",
    "def reset_keras():\n",
    "    sess = get_session()\n",
    "    clear_session()\n",
    "    sess.close()\n",
    "    sess = get_session()\n",
    "\n",
    "    try:\n",
    "        del model # this is from global space - change this as you need\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    print(gc.collect()) # if it's done something you should see a number being outputted\n",
    "\n",
    "    # use the same config as you used to create the session\n",
    "    config = tensorflow.ConfigProto()\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = 1\n",
    "    config.gpu_options.visible_device_list = \"0\"\n",
    "    set_session(tensorflow.Session(config=config))\n",
    "    \n",
    "reset_keras()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "fa1b76273d02502e3fd668dddf74ecf522044524"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('../output/'):\n",
    "    os.makedirs('../output/model')\n",
    "    os.makedirs('../output/history')\n",
    "    \n",
    "    \n",
    "model_dir = \"../output/model/\"\n",
    "history_dir = \"../output/history/\"\n",
    "\n",
    "train_file = '../input/training/training.csv'\n",
    "test_file = '../input/test/test.csv'\n",
    "train_data = pd.read_csv(train_file)  \n",
    "#test_data = pd.read_csv(test_file)\n",
    "\n",
    "\n",
    "bad_samples = [1747, 1731, 1877, 1881, 1979, 2199, 2289, 2321, 2453, 3173, 3296, 3447, 4180, 6859,\n",
    "              2090, 2175, 1907, 2562, 2818, 3296, 3447, 4263, 4482, 4490, 4636, 5059, 6493, 6585, 6906]\n",
    "\n",
    "train_data = train_data.drop(bad_samples).reset_index(drop=True)\n",
    "train_clean = train_data.dropna()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e1b88f1528838c0a8fec61f9a02a70b5077312e9"
   },
   "source": [
    "Create training vector with images and normalize thee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = convert_pixels(train_data)\n",
    "x_clean = convert_pixels(train_clean)\n",
    "y_train = train_data[[col for col in train_data.columns if col != 'Image']].to_numpy()\n",
    "y_clean = train_clean[[col for col in train_clean.columns if col != 'Image']].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a833f4cc5e559774d3a310fd09d40d31e49e71da"
   },
   "source": [
    "Generate labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "e9d804a035809cdf8ffda19f41ce3feb278a38fb"
   },
   "outputs": [],
   "source": [
    "\n",
    "#imputer = KNNImputer(n_neighbors=3, weights='distance')\n",
    "\n",
    "imputer = IterativeImputer(max_iter=1000, tol=0.01, random_state=42)\n",
    "y_train = imputer.fit_transform(y_train)\n",
    "\n",
    "\n",
    "\n",
    "bad_bottom_lip = [210, 350, 499, 512, 810, 839, 895, 1058, 1194,1230, 1245, 1546, 1548]\n",
    "\n",
    "def adjust_mouth_coord(y_train):\n",
    "    \n",
    "\n",
    "    for sample in bad_bottom_lip:\n",
    "        y_train[sample][29] = 94\n",
    "        y_train[sample][28] = y_train[sample][26]\n",
    "          \n",
    "    \n",
    "    \n",
    "    for sample in range(len(y_train)):\n",
    "        if(y_train[sample][29] < y_train[sample][27]+1):\n",
    "             y_train[sample][27] = y_train[sample][29] -1\n",
    "   \n",
    "        if((y_train[sample][23] + y_train[sample][25]) > (2*y_train[sample][29]-1)):\n",
    "             diff = y_train[sample][23] - y_train[sample][25]\n",
    "       \n",
    "             if(diff > 0):\n",
    "                y_train[sample][23] = y_train[sample][29] -1\n",
    "                y_train[sample][25] = y_train[sample][29] -1 - diff\n",
    "           \n",
    "             else:\n",
    "                y_train[sample][23] = y_train[sample][29] -1 + diff\n",
    "                y_train[sample][25] = y_train[sample][29] -1 \n",
    "                \n",
    "    return(y_train)           \n",
    "\n",
    "y_train = adjust_mouth_coord(y_train)    \n",
    " \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set feature engineering parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_na = False\n",
    "add_flip_horiz = True\n",
    "add_blur_img = False\n",
    "add_rotate_img = True\n",
    "add_contrast_img = True\n",
    "add_translate_img = False\n",
    "orig_x_train = x_train.copy()\n",
    "orig_y_train = y_train.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill NA in the training labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if fill_na:\n",
    "    # https://stackoverflow.com/questions/18689235/numpy-array-replace-nan-values-with-average-of-columns\n",
    "    # get column means\n",
    "    col_mean = np.nanmean(y_train,axis=0)\n",
    "\n",
    "    # find the x,y indices that are missing from y_train\n",
    "    inds = np.where(np.isnan(y_train))\n",
    "\n",
    "    # fill in missing values in y_train with the column means. \"take\" is much more efficient than fancy indexing\n",
    "    y_train[inds] = np.take(col_mean, inds[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flip images horizontally and add to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_img_horiz(train_data):\n",
    "    \"\"\"\n",
    "    Flip images horizontally for all training images\n",
    "    \"\"\"\n",
    "    # Flip images\n",
    "    x_train = convert_pixels(train_data)\n",
    "    flip_img = np.array([np.fliplr(x_train[[ind]][0]) for ind in range(x_train.shape[0])])\n",
    "    \n",
    "    # Flip coordinates\n",
    "    train_data_flip = train_data.copy()\n",
    "    x_columns = [col for col in train_data.columns if '_x' in col]\n",
    "    train_data_flip[x_columns] = train_data[x_columns].applymap(lambda x: 96-x)\n",
    "    \n",
    "    #left and right are swapped so undo\n",
    "    left_columns = [col for col in train_data.columns if 'left' in col]\n",
    "    right_columns = [col for col in train_data.columns if 'right' in col]\n",
    "    train_data_flip[left_columns+right_columns] = train_data_flip[right_columns+left_columns]\n",
    "    \n",
    "    flip_coord = train_data_flip[[col for col in train_data if col != 'Image']].to_numpy()\n",
    "    return(flip_img,flip_coord)\n",
    "\n",
    "if add_flip_horiz:\n",
    "    # Apply the augmentation and add the new data to the training set\n",
    "    flipped_img,flipped_coord = flip_img_horiz(train_data)\n",
    "    \n",
    "    imputer = IterativeImputer(max_iter=1000, tol=0.01, random_state=3)\n",
    "    flipped_coord = imputer.fit_transform(flipped_coord)\n",
    "    flipped_coord = adjust_mouth_coord(flipped_coord)\n",
    "    x_train = np.append(x_train,flipped_img,axis=0)\n",
    "    y_train = np.append(y_train,flipped_coord,axis=0)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Gaussian blurring with a 5x5 filter with $\\sigma$ = 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blur_img():\n",
    "    \"\"\"\n",
    "    Add Gaussian blurring to the images\n",
    "    \"\"\"\n",
    "    # https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_filtering/py_filtering.html\n",
    "    blur_img = np.array([cv2.GaussianBlur(orig_x_train[[ind]][0],(5,5),2).reshape(96,96,1) for ind in range(orig_x_train.shape[0])])\n",
    "    \n",
    "    return(blur_img)\n",
    "\n",
    "if add_blur_img:\n",
    "    x_train = np.append(x_train,blur_img(),axis=0)\n",
    "    y_train = np.append(y_train,orig_y_train,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add image rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_img(x_train, y_train):\n",
    "    \"\"\"\"\n",
    "    Rotate images by angles between [5, 10, 14 degrees]\n",
    "    \"\"\"\n",
    "    angles = [5, -5, 10, -10, 14, -14]\n",
    "    b = np.ones((1,3))\n",
    "    rows,cols = (96,96)\n",
    "    x_train_rot = []\n",
    "    y_train_rot = y_train.copy()\n",
    "    M_angles = [cv2.getRotationMatrix2D((cols/2,rows/2),angle,1) for angle in angles]\n",
    "    \n",
    "    for i in range(x_train.shape[0]):\n",
    "        #M = cv2.getRotationMatrix2D((cols/2,rows/2),np.random.choice(angles,1),1)\n",
    "        M = M_angles[np.random.choice(len(M_angles))]\n",
    "        x_train_rot.append((cv2.warpAffine(x_train[[i]].reshape(rows,cols,1),M,(cols,rows)).reshape(96,96,1)))\n",
    "       \n",
    "        #apply affine transformation to (x,y) labels\n",
    "        for j in range(int(y_train.shape[1]/2)):\n",
    "            b[:,0:2] = y_train[i,2*j:2*j+2]\n",
    "            y_train_rot[i,2*j:2*j+2] = np.dot(b,M.transpose()) \n",
    "    \n",
    "    x_train_rot = np.array(x_train_rot)\n",
    "    return x_train_rot, y_train_rot\n",
    "\n",
    "if add_rotate_img:\n",
    "    \n",
    "    x_rotate, y_rotate = rotate_img(orig_x_train,orig_y_train)\n",
    "    x_train = np.append(x_train,x_rotate,axis=0)\n",
    "    y_train = np.append(y_train,y_rotate,axis=0)   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add image contrast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrast_img(apply_random,brightness=.5):\n",
    "    \"\"\"\n",
    "    Add brighter and darker images to the training set with the range of pixel values allowed set at 0 and 1\n",
    "    Only applies contrast to the original images. Need to make sure we're not setting random seed\n",
    "    input:\n",
    "        - brightness: float between -1 and 1\n",
    "        - apply_random: boolean. Applies random brightness to every sample in the data set.\n",
    "          Ignores brightness setting\n",
    "    \"\"\"\n",
    "    if apply_random:\n",
    "        # uses a .1 increment [-.3,.8) to pick out a brightness number for each sample\n",
    "        # numbers are chosen such that they are still realistically visible and\n",
    "        # the added training data has an appreciable change in contrast\n",
    "        brightness = np.random.choice(np.round(np.arange(-.3,.8,.1),2),size=orig_x_train.shape[0])\n",
    "        bright_img = np.array([orig_x_train[[ind]][0]+brightness[ind] for ind in range(orig_x_train.shape[0])])\n",
    "    else:\n",
    "        bright_img = orig_x_train + brightness\n",
    "\n",
    "    bright_img[bright_img > 1] = 1\n",
    "    bright_img[bright_img < 0] = 0\n",
    "    return(bright_img)\n",
    "    \n",
    "if add_contrast_img:\n",
    "    # create two sets of images undergoing contrast changes\n",
    "    x_train = np.append(x_train,contrast_img(apply_random=True),axis=0)\n",
    "    y_train = np.append(y_train,orig_y_train,axis=0)\n",
    "    \n",
    "# testing code\n",
    "# view_img(contrast_img(apply_random=True)[[30]])\n",
    "# view_img(contrast_img(apply_random=True)[[30]])\n",
    "# view_img(contrast_img(apply_random=True)[[30]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_img():\n",
    "    \"\"\"\n",
    "    Add translational shift to the images randomly\n",
    "    \"\"\"\n",
    "    trans_train_data = train_data.copy()\n",
    "    \n",
    "    rows,cols = (96,96)\n",
    "    shift_x = 96*np.random.choice(np.arange(-.4,.4,.05),size=trans_train_data.shape[0])\n",
    "    shift_y = 96*np.random.choice(np.arange(-.4,.4,.05),size=trans_train_data.shape[0])\n",
    "    M = [np.float32([[1,0,shift_x[ind]],[0,1,shift_y[ind]]]) for ind in range(trans_train_data.shape[0])]\n",
    "    trans_img = np.array([cv2.warpAffine(x_train[[ind]].reshape(96,96,1),M[ind],(cols,rows)).reshape(96,96,1) for ind in range(trans_train_data.shape[0])])\n",
    "    \n",
    "    x_col = [col for col in train_data.columns if ((col != 'Image') & ('_x' in col))]\n",
    "    y_col = [col for col in train_data.columns if ((col != 'Image') & ('_y' in col))]\n",
    "\n",
    "    shift_x_array = np.array([np.repeat(x,len(x_col))for x in shift_x])\n",
    "    shift_y_array = np.array([np.repeat(y,len(y_col))for y in shift_y])\n",
    "    \n",
    "    trans_train_data[x_col] += shift_x_array\n",
    "    trans_train_data[y_col] += shift_y_array\n",
    "    \n",
    "    trans_coord = np.array(trans_train_data[[col for col in trans_train_data.columns if col != 'Image']])\n",
    "\n",
    "    # TODO should we force these to be nan or leave as is? If leave as is, comment this out.\n",
    "    trans_coord[trans_coord > 96]= np.nan\n",
    "    trans_coord[trans_coord < 0]= np.nan\n",
    "    return(trans_img,trans_coord)\n",
    "\n",
    "if add_translate_img:\n",
    "    trans_img,trans_coord = translate_img()\n",
    "    x_train = np.append(x_train,trans_img,axis=0)\n",
    "    y_train = np.append(y_train,trans_coord,axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callback function if detailed log required\n",
    "class History(tensorflow.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.train_loss = []\n",
    "        self.train_rmse = []\n",
    "        self.val_rmse = []\n",
    "        self.val_loss = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.train_loss.append(logs.get('loss'))\n",
    "        self.train_rmse.append(logs.get('rmse'))\n",
    "        \n",
    "    def on_epoch_end(self, batch, logs={}):    \n",
    "        self.val_rmse.append(logs.get('val_rmse'))\n",
    "        self.val_loss.append(logs.get('val_loss'))\n",
    "        \n",
    "# Implement ModelCheckPoint callback function to save CNN model\n",
    "class CNN_ModelCheckpoint(tensorflow.keras.callbacks.Callback):\n",
    "\n",
    "    def __init__(self, model, filename):\n",
    "        self.filename = filename\n",
    "        self.cnn_model = model\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.max_val_rmse = math.inf\n",
    "        \n",
    " \n",
    "    def on_epoch_end(self, batch, logs={}):    \n",
    "        val_rmse = logs.get('val_rmse')\n",
    "        if(val_rmse < self.max_val_rmse):\n",
    "           self.max_val_rmse = val_rmse\n",
    "           self.cnn_model.save_weights(self.filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/user1/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/user1/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 96, 96, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 96, 96, 32)        288       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 96, 96, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_1 (Ba (None, 96, 96, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 48, 48, 64)        18432     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 48, 48, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_2 (Ba (None, 48, 48, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 48, 48, 64)        36864     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 48, 48, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_3 (Ba (None, 48, 48, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 24, 24, 96)        55296     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 24, 24, 96)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_4 (Ba (None, 24, 24, 96)        384       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 24, 24, 96)        82944     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 24, 24, 96)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_5 (Ba (None, 24, 24, 96)        384       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 12, 12, 96)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 12, 12, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 12, 12, 128)       110592    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_6 (Ba (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 12, 12, 128)       147456    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_7 (Ba (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 6, 6, 256)         294912    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_8 (Ba (None, 6, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 6, 6, 256)         589824    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_9 (Ba (None, 6, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 3, 3, 512)         1179648   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 3, 3, 512)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_10 (B (None, 3, 3, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 3, 3, 512)         2359296   \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_11 (B (None, 3, 3, 512)         2048      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               2359808   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                15390     \n",
      "=================================================================\n",
      "Total params: 7,259,326\n",
      "Trainable params: 7,255,038\n",
      "Non-trainable params: 4,288\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def final_model():\n",
    "    model_input = Input(shape=(96,96,1))\n",
    "\n",
    "    x = Convolution2D(32, (3,3), padding='same', use_bias=False)(model_input)\n",
    "    x = LeakyReLU(alpha = 0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Convolution2D(32, (3,3), padding='same', use_bias=False)(model_input)\n",
    "    x = LeakyReLU(alpha = 0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Convolution2D(64, (3,3), padding='same', use_bias=False)(x)\n",
    "    x = LeakyReLU(alpha = 0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Convolution2D(64, (3,3), padding='same', use_bias=False)(x)\n",
    "    x = LeakyReLU(alpha = 0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Convolution2D(96, (3,3), padding='same', use_bias=False)(x)\n",
    "    x = LeakyReLU(alpha = 0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Convolution2D(96, (3,3), padding='same', use_bias=False)(x)\n",
    "    x = LeakyReLU(alpha = 0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Convolution2D(128, (3,3),padding='same', use_bias=False)(x)\n",
    "    x = LeakyReLU(alpha = 0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Convolution2D(128, (3,3),padding='same', use_bias=False)(x)\n",
    "    x = LeakyReLU(alpha = 0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    #x = Dropout(0.1)(x)\n",
    "    x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Convolution2D(256, (3,3),padding='same',use_bias=False)(x)\n",
    "    x = LeakyReLU(alpha = 0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Convolution2D(256, (3,3),padding='same',use_bias=False)(x)\n",
    "    x = LeakyReLU(alpha = 0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Convolution2D(512, (3,3), padding='same', use_bias=False)(x)\n",
    "    x = LeakyReLU(alpha = 0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Convolution2D(512, (3,3), activation='relu', padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(512,activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    model_output = Dense(30)(x)\n",
    "    model = Model(model_input, model_output, name=\"final_model\")\n",
    "    return model\n",
    "\n",
    "model = final_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/user1/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/utils/losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "# Custom RMSE metric\n",
    "def rmse(y_true, y_pred):\n",
    "    return backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))\n",
    "\n",
    "#from tensorflow.keras.optimizers.schedules import InverseTimeDecay\n",
    "\n",
    "# Use Nadam optimizer with variable learning rate\n",
    "optimizer = Nadam(lr=0.00001,\n",
    "                  beta_1=0.9,\n",
    "                  beta_2=0.999,\n",
    "                  epsilon=1e-08,\n",
    "                  schedule_decay=0.004)\n",
    "\n",
    "\n",
    "# Loss: MSE and Metric = RMSE\n",
    "model.compile(optimizer= optimizer, \n",
    "              loss='mean_squared_error',\n",
    "              metrics=[rmse])\n",
    "\n",
    "#Callback to save the best model\n",
    "saveBase_Model = CNN_ModelCheckpoint(model, model_dir+\"final_model_weights.h5\")\n",
    "\n",
    "#define callback functions\n",
    "callbacks = [#EarlyStopping(monitor='val_rmse', patience=3, verbose=2),\n",
    "             saveBase_Model]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e4cf4686b410841f2e34dbb081f3429d1b0f67e9"
   },
   "source": [
    "Run for 1000 epochs and keeping 20% train-valid split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "894af9cbfcf2dca50e7407946cad318157b77d0a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22470 samples, validate on 5618 samples\n",
      "WARNING:tensorflow:From /home/user1/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/1500\n",
      "22470/22470 [==============================] - 26s 1ms/sample - loss: 1980.7975 - rmse: 44.1421 - val_loss: 2643.4386 - val_rmse: 51.3827\n",
      "Epoch 2/1500\n",
      "22470/22470 [==============================] - 18s 801us/sample - loss: 738.7576 - rmse: 26.7694 - val_loss: 2256.1264 - val_rmse: 47.4646\n",
      "Epoch 3/1500\n",
      "22470/22470 [==============================] - 18s 799us/sample - loss: 241.2655 - rmse: 15.1525 - val_loss: 1614.1735 - val_rmse: 40.1313\n",
      "Epoch 4/1500\n",
      "22470/22470 [==============================] - 18s 805us/sample - loss: 85.0213 - rmse: 8.9339 - val_loss: 1065.2654 - val_rmse: 32.5416\n",
      "Epoch 5/1500\n",
      "22470/22470 [==============================] - 18s 808us/sample - loss: 51.0304 - rmse: 6.9339 - val_loss: 648.0624 - val_rmse: 25.2301\n",
      "Epoch 6/1500\n",
      "22470/22470 [==============================] - 18s 811us/sample - loss: 45.6771 - rmse: 6.5666 - val_loss: 321.6663 - val_rmse: 17.5983\n",
      "Epoch 7/1500\n",
      "22470/22470 [==============================] - 19s 828us/sample - loss: 44.3709 - rmse: 6.4735 - val_loss: 133.6610 - val_rmse: 11.2165\n",
      "Epoch 8/1500\n",
      "22470/22470 [==============================] - 19s 832us/sample - loss: 43.4801 - rmse: 6.4075 - val_loss: 63.7459 - val_rmse: 7.6075\n",
      "Epoch 9/1500\n",
      "22470/22470 [==============================] - 18s 816us/sample - loss: 42.7837 - rmse: 6.3527 - val_loss: 38.9328 - val_rmse: 5.8372\n",
      "Epoch 10/1500\n",
      "22470/22470 [==============================] - 18s 818us/sample - loss: 42.0838 - rmse: 6.2965 - val_loss: 28.4539 - val_rmse: 4.9357\n",
      "Epoch 11/1500\n",
      "22470/22470 [==============================] - 18s 818us/sample - loss: 41.2708 - rmse: 6.2353 - val_loss: 24.3997 - val_rmse: 4.5463\n",
      "Epoch 12/1500\n",
      "22470/22470 [==============================] - 18s 819us/sample - loss: 40.6086 - rmse: 6.1843 - val_loss: 22.1465 - val_rmse: 4.2960\n",
      "Epoch 13/1500\n",
      "22470/22470 [==============================] - 18s 817us/sample - loss: 39.8243 - rmse: 6.1230 - val_loss: 21.1411 - val_rmse: 4.1989\n",
      "Epoch 14/1500\n",
      "22470/22470 [==============================] - 18s 821us/sample - loss: 39.3301 - rmse: 6.0822 - val_loss: 20.8254 - val_rmse: 4.1703\n",
      "Epoch 15/1500\n",
      "22470/22470 [==============================] - 18s 818us/sample - loss: 38.3553 - rmse: 6.0056 - val_loss: 19.3707 - val_rmse: 4.0043\n",
      "Epoch 16/1500\n",
      "22470/22470 [==============================] - 18s 813us/sample - loss: 37.5257 - rmse: 5.9368 - val_loss: 20.0740 - val_rmse: 4.0987\n",
      "Epoch 17/1500\n",
      "22470/22470 [==============================] - 18s 814us/sample - loss: 36.4300 - rmse: 5.8522 - val_loss: 20.0762 - val_rmse: 4.1236\n",
      "Epoch 18/1500\n",
      "22470/22470 [==============================] - 18s 817us/sample - loss: 35.1268 - rmse: 5.7469 - val_loss: 18.9137 - val_rmse: 3.9871\n",
      "Epoch 19/1500\n",
      "22470/22470 [==============================] - 18s 817us/sample - loss: 33.9054 - rmse: 5.6439 - val_loss: 18.0905 - val_rmse: 3.8904\n",
      "Epoch 20/1500\n",
      "22470/22470 [==============================] - 18s 821us/sample - loss: 32.7656 - rmse: 5.5474 - val_loss: 15.5965 - val_rmse: 3.5573\n",
      "Epoch 21/1500\n",
      "22470/22470 [==============================] - 19s 827us/sample - loss: 31.8850 - rmse: 5.4712 - val_loss: 14.6591 - val_rmse: 3.4271\n",
      "Epoch 22/1500\n",
      "22470/22470 [==============================] - 18s 816us/sample - loss: 31.3989 - rmse: 5.4308 - val_loss: 13.1086 - val_rmse: 3.1884\n",
      "Epoch 23/1500\n",
      "22470/22470 [==============================] - 18s 820us/sample - loss: 30.7974 - rmse: 5.3762 - val_loss: 12.4473 - val_rmse: 3.0807\n",
      "Epoch 24/1500\n",
      "22470/22470 [==============================] - 19s 849us/sample - loss: 30.1703 - rmse: 5.3156 - val_loss: 11.8611 - val_rmse: 2.9853\n",
      "Epoch 25/1500\n",
      "22470/22470 [==============================] - 18s 804us/sample - loss: 29.5602 - rmse: 5.2605 - val_loss: 11.8718 - val_rmse: 2.9930\n",
      "Epoch 26/1500\n",
      "22470/22470 [==============================] - 18s 801us/sample - loss: 28.9152 - rmse: 5.2001 - val_loss: 11.9549 - val_rmse: 3.0126\n",
      "Epoch 27/1500\n",
      "22470/22470 [==============================] - 18s 810us/sample - loss: 28.4139 - rmse: 5.1574 - val_loss: 11.4547 - val_rmse: 2.9364\n",
      "Epoch 28/1500\n",
      "22470/22470 [==============================] - 18s 809us/sample - loss: 27.7952 - rmse: 5.1011 - val_loss: 11.1468 - val_rmse: 2.8907\n",
      "Epoch 29/1500\n",
      "22470/22470 [==============================] - 18s 804us/sample - loss: 27.3436 - rmse: 5.0525 - val_loss: 11.2987 - val_rmse: 2.9182\n",
      "Epoch 30/1500\n",
      "22470/22470 [==============================] - 18s 805us/sample - loss: 26.7169 - rmse: 4.9950 - val_loss: 10.9141 - val_rmse: 2.8676\n",
      "Epoch 31/1500\n",
      "22470/22470 [==============================] - 18s 808us/sample - loss: 26.2102 - rmse: 4.9471 - val_loss: 10.5109 - val_rmse: 2.7985\n",
      "Epoch 32/1500\n",
      "22470/22470 [==============================] - 18s 805us/sample - loss: 25.6745 - rmse: 4.8956 - val_loss: 10.5259 - val_rmse: 2.8120\n",
      "Epoch 33/1500\n",
      "22470/22470 [==============================] - 18s 802us/sample - loss: 25.2773 - rmse: 4.8571 - val_loss: 10.4713 - val_rmse: 2.8117\n",
      "Epoch 34/1500\n",
      "22470/22470 [==============================] - 18s 808us/sample - loss: 24.6000 - rmse: 4.7922 - val_loss: 10.2843 - val_rmse: 2.7795\n",
      "Epoch 35/1500\n",
      "22470/22470 [==============================] - 18s 808us/sample - loss: 24.3374 - rmse: 4.7637 - val_loss: 9.7913 - val_rmse: 2.6964\n",
      "Epoch 36/1500\n",
      "22470/22470 [==============================] - 18s 807us/sample - loss: 23.8398 - rmse: 4.7179 - val_loss: 9.5952 - val_rmse: 2.6643\n",
      "Epoch 37/1500\n",
      "22470/22470 [==============================] - 18s 803us/sample - loss: 23.3980 - rmse: 4.6702 - val_loss: 9.9143 - val_rmse: 2.7425\n",
      "Epoch 38/1500\n",
      "22470/22470 [==============================] - 18s 804us/sample - loss: 23.0004 - rmse: 4.6332 - val_loss: 9.6369 - val_rmse: 2.6968\n",
      "Epoch 39/1500\n",
      "22470/22470 [==============================] - 18s 815us/sample - loss: 22.5052 - rmse: 4.5827 - val_loss: 9.0748 - val_rmse: 2.5998\n",
      "Epoch 40/1500\n",
      "22470/22470 [==============================] - 18s 806us/sample - loss: 22.0821 - rmse: 4.5414 - val_loss: 9.0839 - val_rmse: 2.5983\n",
      "Epoch 41/1500\n",
      "22470/22470 [==============================] - 18s 809us/sample - loss: 21.7104 - rmse: 4.5029 - val_loss: 8.7466 - val_rmse: 2.5498\n",
      "Epoch 42/1500\n",
      "22470/22470 [==============================] - 18s 802us/sample - loss: 21.2569 - rmse: 4.4586 - val_loss: 8.7559 - val_rmse: 2.5586\n",
      "Epoch 43/1500\n",
      "22470/22470 [==============================] - 18s 806us/sample - loss: 20.9380 - rmse: 4.4254 - val_loss: 8.3833 - val_rmse: 2.4934\n",
      "Epoch 44/1500\n",
      "22470/22470 [==============================] - 18s 805us/sample - loss: 20.4531 - rmse: 4.3754 - val_loss: 8.4459 - val_rmse: 2.5044\n",
      "Epoch 45/1500\n",
      "22470/22470 [==============================] - 18s 807us/sample - loss: 20.1899 - rmse: 4.3487 - val_loss: 8.1396 - val_rmse: 2.4556\n",
      "Epoch 46/1500\n",
      "22470/22470 [==============================] - 18s 801us/sample - loss: 19.7367 - rmse: 4.3012 - val_loss: 8.1869 - val_rmse: 2.4666\n",
      "Epoch 47/1500\n",
      "22470/22470 [==============================] - 18s 810us/sample - loss: 19.4647 - rmse: 4.2718 - val_loss: 8.0032 - val_rmse: 2.4327\n",
      "Epoch 48/1500\n",
      "22470/22470 [==============================] - 18s 807us/sample - loss: 19.0863 - rmse: 4.2312 - val_loss: 7.8065 - val_rmse: 2.4043\n",
      "Epoch 49/1500\n",
      "22470/22470 [==============================] - 18s 812us/sample - loss: 18.6580 - rmse: 4.1856 - val_loss: 7.5511 - val_rmse: 2.3573\n",
      "Epoch 50/1500\n",
      "22470/22470 [==============================] - 18s 800us/sample - loss: 18.3784 - rmse: 4.1569 - val_loss: 7.7116 - val_rmse: 2.3990\n",
      "Epoch 51/1500\n",
      "22470/22470 [==============================] - 18s 800us/sample - loss: 18.0433 - rmse: 4.1186 - val_loss: 7.5089 - val_rmse: 2.3667\n",
      "Epoch 52/1500\n",
      "22470/22470 [==============================] - 18s 806us/sample - loss: 17.7492 - rmse: 4.0863 - val_loss: 7.4452 - val_rmse: 2.3541\n",
      "Epoch 53/1500\n",
      "22470/22470 [==============================] - 18s 808us/sample - loss: 17.4795 - rmse: 4.0578 - val_loss: 7.1967 - val_rmse: 2.3045\n",
      "Epoch 54/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22470/22470 [==============================] - 17s 768us/sample - loss: 17.1731 - rmse: 4.0244 - val_loss: 7.0215 - val_rmse: 2.2760\n",
      "Epoch 55/1500\n",
      "22470/22470 [==============================] - 17s 757us/sample - loss: 16.8454 - rmse: 3.9834 - val_loss: 7.1382 - val_rmse: 2.3051\n",
      "Epoch 56/1500\n",
      "22470/22470 [==============================] - 17s 761us/sample - loss: 16.5211 - rmse: 3.9486 - val_loss: 6.7788 - val_rmse: 2.2358\n",
      "Epoch 57/1500\n",
      "22470/22470 [==============================] - 17s 757us/sample - loss: 16.3512 - rmse: 3.9267 - val_loss: 6.7984 - val_rmse: 2.2565\n",
      "Epoch 58/1500\n",
      "22470/22470 [==============================] - 17s 755us/sample - loss: 16.0284 - rmse: 3.8891 - val_loss: 6.7353 - val_rmse: 2.2388\n",
      "Epoch 59/1500\n",
      "22470/22470 [==============================] - 17s 759us/sample - loss: 15.7799 - rmse: 3.8600 - val_loss: 6.6204 - val_rmse: 2.2200\n",
      "Epoch 60/1500\n",
      "22470/22470 [==============================] - 17s 761us/sample - loss: 15.5185 - rmse: 3.8282 - val_loss: 6.5552 - val_rmse: 2.2158\n",
      "Epoch 61/1500\n",
      "22470/22470 [==============================] - 17s 760us/sample - loss: 15.2508 - rmse: 3.7980 - val_loss: 6.4618 - val_rmse: 2.2003\n",
      "Epoch 62/1500\n",
      "22470/22470 [==============================] - 17s 763us/sample - loss: 15.0058 - rmse: 3.7655 - val_loss: 6.2742 - val_rmse: 2.1612\n",
      "Epoch 63/1500\n",
      "22470/22470 [==============================] - 17s 759us/sample - loss: 14.7694 - rmse: 3.7399 - val_loss: 6.3262 - val_rmse: 2.1766\n",
      "Epoch 64/1500\n",
      "22470/22470 [==============================] - 17s 758us/sample - loss: 14.6020 - rmse: 3.7153 - val_loss: 6.3946 - val_rmse: 2.1954\n",
      "Epoch 65/1500\n",
      "22470/22470 [==============================] - 17s 765us/sample - loss: 14.3297 - rmse: 3.6829 - val_loss: 6.1436 - val_rmse: 2.1444\n",
      "Epoch 66/1500\n",
      "22470/22470 [==============================] - 17s 753us/sample - loss: 14.0839 - rmse: 3.6513 - val_loss: 6.1388 - val_rmse: 2.1571\n",
      "Epoch 67/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 13.8408 - rmse: 3.6217 - val_loss: 6.1490 - val_rmse: 2.1553\n",
      "Epoch 68/1500\n",
      "22470/22470 [==============================] - 17s 754us/sample - loss: 13.6811 - rmse: 3.5992 - val_loss: 5.9235 - val_rmse: 2.1100\n",
      "Epoch 69/1500\n",
      "22470/22470 [==============================] - 17s 754us/sample - loss: 13.3976 - rmse: 3.5629 - val_loss: 5.8556 - val_rmse: 2.0933\n",
      "Epoch 70/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 13.2536 - rmse: 3.5434 - val_loss: 5.8653 - val_rmse: 2.1018\n",
      "Epoch 71/1500\n",
      "22470/22470 [==============================] - 17s 753us/sample - loss: 13.0393 - rmse: 3.5158 - val_loss: 5.7271 - val_rmse: 2.0728\n",
      "Epoch 72/1500\n",
      "22470/22470 [==============================] - 17s 754us/sample - loss: 12.8110 - rmse: 3.4836 - val_loss: 5.6023 - val_rmse: 2.0532\n",
      "Epoch 73/1500\n",
      "22470/22470 [==============================] - 17s 754us/sample - loss: 12.6547 - rmse: 3.4652 - val_loss: 5.7199 - val_rmse: 2.0816\n",
      "Epoch 74/1500\n",
      "22470/22470 [==============================] - 17s 753us/sample - loss: 12.5247 - rmse: 3.4469 - val_loss: 5.5795 - val_rmse: 2.0467\n",
      "Epoch 75/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 12.3266 - rmse: 3.4173 - val_loss: 5.7147 - val_rmse: 2.0798\n",
      "Epoch 76/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 12.1921 - rmse: 3.3996 - val_loss: 5.5223 - val_rmse: 2.0458\n",
      "Epoch 77/1500\n",
      "22470/22470 [==============================] - 17s 753us/sample - loss: 11.9538 - rmse: 3.3675 - val_loss: 5.4752 - val_rmse: 2.0330\n",
      "Epoch 78/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 11.8249 - rmse: 3.3499 - val_loss: 5.5613 - val_rmse: 2.0678\n",
      "Epoch 79/1500\n",
      "22470/22470 [==============================] - 17s 753us/sample - loss: 11.5927 - rmse: 3.3155 - val_loss: 5.3523 - val_rmse: 2.0115\n",
      "Epoch 80/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 11.4980 - rmse: 3.3009 - val_loss: 5.5267 - val_rmse: 2.0490\n",
      "Epoch 81/1500\n",
      "22470/22470 [==============================] - 17s 746us/sample - loss: 11.2809 - rmse: 3.2713 - val_loss: 5.5486 - val_rmse: 2.0601\n",
      "Epoch 82/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 11.1821 - rmse: 3.2575 - val_loss: 5.2794 - val_rmse: 2.0124\n",
      "Epoch 83/1500\n",
      "22470/22470 [==============================] - 17s 755us/sample - loss: 11.0160 - rmse: 3.2314 - val_loss: 5.1847 - val_rmse: 1.9838\n",
      "Epoch 84/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 10.8766 - rmse: 3.2129 - val_loss: 5.2193 - val_rmse: 1.9885\n",
      "Epoch 85/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 10.7228 - rmse: 3.1888 - val_loss: 5.1697 - val_rmse: 1.9887\n",
      "Epoch 86/1500\n",
      "22470/22470 [==============================] - 17s 753us/sample - loss: 10.5733 - rmse: 3.1679 - val_loss: 5.0516 - val_rmse: 1.9572\n",
      "Epoch 87/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 10.4739 - rmse: 3.1512 - val_loss: 5.1720 - val_rmse: 1.9931\n",
      "Epoch 88/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 10.3339 - rmse: 3.1318 - val_loss: 5.0825 - val_rmse: 1.9645\n",
      "Epoch 89/1500\n",
      "22470/22470 [==============================] - 17s 758us/sample - loss: 10.2007 - rmse: 3.1122 - val_loss: 4.9811 - val_rmse: 1.9455\n",
      "Epoch 90/1500\n",
      "22470/22470 [==============================] - 17s 755us/sample - loss: 10.0690 - rmse: 3.0915 - val_loss: 4.8822 - val_rmse: 1.9247\n",
      "Epoch 91/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 9.9816 - rmse: 3.0763 - val_loss: 4.9654 - val_rmse: 1.9450\n",
      "Epoch 92/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 9.8332 - rmse: 3.0539 - val_loss: 4.9429 - val_rmse: 1.9398\n",
      "Epoch 93/1500\n",
      "22470/22470 [==============================] - 17s 753us/sample - loss: 9.6931 - rmse: 3.0319 - val_loss: 4.8284 - val_rmse: 1.9117\n",
      "Epoch 94/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 9.6340 - rmse: 3.0226 - val_loss: 4.8520 - val_rmse: 1.9173\n",
      "Epoch 95/1500\n",
      "22470/22470 [==============================] - 17s 759us/sample - loss: 9.4628 - rmse: 2.9946 - val_loss: 4.7211 - val_rmse: 1.8992\n",
      "Epoch 96/1500\n",
      "22470/22470 [==============================] - 17s 753us/sample - loss: 9.3835 - rmse: 2.9830 - val_loss: 4.8077 - val_rmse: 1.9176\n",
      "Epoch 97/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 9.2740 - rmse: 2.9653 - val_loss: 4.8958 - val_rmse: 1.9387\n",
      "Epoch 98/1500\n",
      "22470/22470 [==============================] - 17s 755us/sample - loss: 9.1426 - rmse: 2.9448 - val_loss: 4.6743 - val_rmse: 1.8881\n",
      "Epoch 99/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 9.0842 - rmse: 2.9363 - val_loss: 4.7222 - val_rmse: 1.8953\n",
      "Epoch 100/1500\n",
      "22470/22470 [==============================] - 17s 756us/sample - loss: 8.9678 - rmse: 2.9156 - val_loss: 4.6252 - val_rmse: 1.8805\n",
      "Epoch 101/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 8.8220 - rmse: 2.8922 - val_loss: 4.6314 - val_rmse: 1.8822\n",
      "Epoch 102/1500\n",
      "22470/22470 [==============================] - 17s 755us/sample - loss: 8.7724 - rmse: 2.8840 - val_loss: 4.6234 - val_rmse: 1.8767\n",
      "Epoch 103/1500\n",
      "22470/22470 [==============================] - 17s 755us/sample - loss: 8.6530 - rmse: 2.8652 - val_loss: 4.6968 - val_rmse: 1.9007\n",
      "Epoch 104/1500\n",
      "22470/22470 [==============================] - 17s 755us/sample - loss: 8.6227 - rmse: 2.8573 - val_loss: 4.5460 - val_rmse: 1.8608\n",
      "Epoch 105/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 8.4962 - rmse: 2.8380 - val_loss: 4.5658 - val_rmse: 1.8696\n",
      "Epoch 106/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 8.4128 - rmse: 2.8234 - val_loss: 4.5833 - val_rmse: 1.8723\n",
      "Epoch 107/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 8.3235 - rmse: 2.8082 - val_loss: 4.7154 - val_rmse: 1.8869\n",
      "Epoch 108/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 8.2536 - rmse: 2.7969 - val_loss: 4.5485 - val_rmse: 1.8675\n",
      "Epoch 109/1500\n",
      "22470/22470 [==============================] - 17s 755us/sample - loss: 8.1834 - rmse: 2.7844 - val_loss: 4.4177 - val_rmse: 1.8363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 110/1500\n",
      "22470/22470 [==============================] - 17s 754us/sample - loss: 8.0845 - rmse: 2.7667 - val_loss: 4.3754 - val_rmse: 1.8290\n",
      "Epoch 111/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 7.9742 - rmse: 2.7492 - val_loss: 4.4799 - val_rmse: 1.8634\n",
      "Epoch 112/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 7.9085 - rmse: 2.7370 - val_loss: 4.4723 - val_rmse: 1.8534\n",
      "Epoch 113/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 7.8547 - rmse: 2.7274 - val_loss: 4.2999 - val_rmse: 1.8125\n",
      "Epoch 114/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 7.7624 - rmse: 2.7107 - val_loss: 4.4131 - val_rmse: 1.8387\n",
      "Epoch 115/1500\n",
      "22470/22470 [==============================] - 17s 747us/sample - loss: 7.6703 - rmse: 2.6962 - val_loss: 4.4158 - val_rmse: 1.8388\n",
      "Epoch 116/1500\n",
      "22470/22470 [==============================] - 17s 746us/sample - loss: 7.5574 - rmse: 2.6747 - val_loss: 4.3884 - val_rmse: 1.8490\n",
      "Epoch 117/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 7.5598 - rmse: 2.6742 - val_loss: 4.3894 - val_rmse: 1.8330\n",
      "Epoch 118/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 7.4679 - rmse: 2.6586 - val_loss: 4.2537 - val_rmse: 1.8058\n",
      "Epoch 119/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 7.4341 - rmse: 2.6517 - val_loss: 4.3122 - val_rmse: 1.8097\n",
      "Epoch 120/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 7.3527 - rmse: 2.6380 - val_loss: 4.2874 - val_rmse: 1.8121\n",
      "Epoch 121/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 7.2943 - rmse: 2.6266 - val_loss: 4.3801 - val_rmse: 1.8443\n",
      "Epoch 122/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 7.2732 - rmse: 2.6228 - val_loss: 4.2551 - val_rmse: 1.8078\n",
      "Epoch 123/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 7.1262 - rmse: 2.5960 - val_loss: 4.2814 - val_rmse: 1.8204\n",
      "Epoch 124/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 7.1433 - rmse: 2.5971 - val_loss: 4.2071 - val_rmse: 1.7930\n",
      "Epoch 125/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 7.0025 - rmse: 2.5739 - val_loss: 4.2394 - val_rmse: 1.7984\n",
      "Epoch 126/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 6.9578 - rmse: 2.5642 - val_loss: 4.1948 - val_rmse: 1.7884\n",
      "Epoch 127/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 6.8982 - rmse: 2.5548 - val_loss: 4.1825 - val_rmse: 1.7898\n",
      "Epoch 128/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 6.8625 - rmse: 2.5465 - val_loss: 4.1727 - val_rmse: 1.7815\n",
      "Epoch 129/1500\n",
      "22470/22470 [==============================] - 17s 756us/sample - loss: 6.8084 - rmse: 2.5360 - val_loss: 4.1454 - val_rmse: 1.7773\n",
      "Epoch 130/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 6.7425 - rmse: 2.5245 - val_loss: 4.1892 - val_rmse: 1.7835\n",
      "Epoch 131/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 6.6893 - rmse: 2.5145 - val_loss: 4.1666 - val_rmse: 1.7802\n",
      "Epoch 132/1500\n",
      "22470/22470 [==============================] - 17s 759us/sample - loss: 6.6494 - rmse: 2.5061 - val_loss: 4.0820 - val_rmse: 1.7640\n",
      "Epoch 133/1500\n",
      "22470/22470 [==============================] - 17s 757us/sample - loss: 6.6065 - rmse: 2.4980 - val_loss: 4.0422 - val_rmse: 1.7573\n",
      "Epoch 134/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 6.5636 - rmse: 2.4881 - val_loss: 4.1014 - val_rmse: 1.7665\n",
      "Epoch 135/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 6.5036 - rmse: 2.4767 - val_loss: 4.0927 - val_rmse: 1.7679\n",
      "Epoch 136/1500\n",
      "22470/22470 [==============================] - 17s 756us/sample - loss: 6.4493 - rmse: 2.4666 - val_loss: 3.9862 - val_rmse: 1.7441\n",
      "Epoch 137/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 6.4189 - rmse: 2.4611 - val_loss: 4.0068 - val_rmse: 1.7489\n",
      "Epoch 138/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 6.3740 - rmse: 2.4518 - val_loss: 4.0251 - val_rmse: 1.7560\n",
      "Epoch 139/1500\n",
      "22470/22470 [==============================] - 17s 756us/sample - loss: 6.3313 - rmse: 2.4426 - val_loss: 3.9859 - val_rmse: 1.7415\n",
      "Epoch 140/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 6.2491 - rmse: 2.4270 - val_loss: 4.1383 - val_rmse: 1.7890\n",
      "Epoch 141/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 6.2160 - rmse: 2.4203 - val_loss: 4.1909 - val_rmse: 1.7860\n",
      "Epoch 142/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 6.1936 - rmse: 2.4156 - val_loss: 3.9978 - val_rmse: 1.7480\n",
      "Epoch 143/1500\n",
      "22470/22470 [==============================] - 17s 754us/sample - loss: 6.1347 - rmse: 2.4047 - val_loss: 3.9758 - val_rmse: 1.7418\n",
      "Epoch 144/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 6.0772 - rmse: 2.3938 - val_loss: 4.0926 - val_rmse: 1.7631\n",
      "Epoch 145/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 6.0485 - rmse: 2.3872 - val_loss: 4.0003 - val_rmse: 1.7424\n",
      "Epoch 146/1500\n",
      "22470/22470 [==============================] - 17s 755us/sample - loss: 6.0155 - rmse: 2.3797 - val_loss: 3.9259 - val_rmse: 1.7231\n",
      "Epoch 147/1500\n",
      "22470/22470 [==============================] - 17s 753us/sample - loss: 5.9916 - rmse: 2.3763 - val_loss: 3.9211 - val_rmse: 1.7282\n",
      "Epoch 148/1500\n",
      "22470/22470 [==============================] - 17s 755us/sample - loss: 5.9393 - rmse: 2.3649 - val_loss: 3.8863 - val_rmse: 1.7167\n",
      "Epoch 149/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 5.9027 - rmse: 2.3563 - val_loss: 3.9752 - val_rmse: 1.7380\n",
      "Epoch 150/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 5.8797 - rmse: 2.3519 - val_loss: 3.9575 - val_rmse: 1.7350\n",
      "Epoch 151/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 5.8162 - rmse: 2.3398 - val_loss: 4.0381 - val_rmse: 1.7537\n",
      "Epoch 152/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 5.8010 - rmse: 2.3365 - val_loss: 4.0669 - val_rmse: 1.7613\n",
      "Epoch 153/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 5.7880 - rmse: 2.3327 - val_loss: 4.0014 - val_rmse: 1.7432\n",
      "Epoch 154/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 5.7338 - rmse: 2.3228 - val_loss: 3.9523 - val_rmse: 1.7331\n",
      "Epoch 155/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 5.7022 - rmse: 2.3168 - val_loss: 3.9155 - val_rmse: 1.7304\n",
      "Epoch 156/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 5.6541 - rmse: 2.3059 - val_loss: 3.9678 - val_rmse: 1.7389\n",
      "Epoch 157/1500\n",
      "22470/22470 [==============================] - 17s 757us/sample - loss: 5.6279 - rmse: 2.2998 - val_loss: 3.8655 - val_rmse: 1.7147\n",
      "Epoch 158/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 5.6176 - rmse: 2.2964 - val_loss: 4.1705 - val_rmse: 1.7767\n",
      "Epoch 159/1500\n",
      "22470/22470 [==============================] - 17s 757us/sample - loss: 5.5829 - rmse: 2.2900 - val_loss: 3.8185 - val_rmse: 1.6985\n",
      "Epoch 160/1500\n",
      "22470/22470 [==============================] - 17s 759us/sample - loss: 5.5318 - rmse: 2.2797 - val_loss: 3.7344 - val_rmse: 1.6830\n",
      "Epoch 161/1500\n",
      "22470/22470 [==============================] - 17s 755us/sample - loss: 5.4977 - rmse: 2.2718 - val_loss: 3.7497 - val_rmse: 1.6874\n",
      "Epoch 162/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 5.5225 - rmse: 2.2743 - val_loss: 3.7934 - val_rmse: 1.6969\n",
      "Epoch 163/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 5.4402 - rmse: 2.2602 - val_loss: 3.8245 - val_rmse: 1.7113\n",
      "Epoch 164/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 5.4038 - rmse: 2.2529 - val_loss: 3.7885 - val_rmse: 1.6968\n",
      "Epoch 165/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 5.3894 - rmse: 2.2490 - val_loss: 3.8198 - val_rmse: 1.7011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 166/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 5.3415 - rmse: 2.2386 - val_loss: 3.7279 - val_rmse: 1.6785\n",
      "Epoch 167/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 5.3059 - rmse: 2.2321 - val_loss: 3.8494 - val_rmse: 1.7049\n",
      "Epoch 168/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 5.2903 - rmse: 2.2274 - val_loss: 3.8285 - val_rmse: 1.6974\n",
      "Epoch 169/1500\n",
      "22470/22470 [==============================] - 17s 746us/sample - loss: 5.2709 - rmse: 2.2241 - val_loss: 3.8160 - val_rmse: 1.6987\n",
      "Epoch 170/1500\n",
      "22470/22470 [==============================] - 17s 746us/sample - loss: 5.2488 - rmse: 2.2185 - val_loss: 3.9467 - val_rmse: 1.7229\n",
      "Epoch 171/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 5.2259 - rmse: 2.2112 - val_loss: 3.7426 - val_rmse: 1.6826\n",
      "Epoch 172/1500\n",
      "22470/22470 [==============================] - 17s 746us/sample - loss: 5.1766 - rmse: 2.2026 - val_loss: 3.7658 - val_rmse: 1.6882\n",
      "Epoch 173/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 5.1775 - rmse: 2.2016 - val_loss: 3.7526 - val_rmse: 1.6859\n",
      "Epoch 174/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 5.1573 - rmse: 2.1988 - val_loss: 3.7576 - val_rmse: 1.6860\n",
      "Epoch 175/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 5.1332 - rmse: 2.1927 - val_loss: 3.9702 - val_rmse: 1.7384\n",
      "Epoch 176/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 5.0861 - rmse: 2.1824 - val_loss: 3.7111 - val_rmse: 1.6766\n",
      "Epoch 177/1500\n",
      "22470/22470 [==============================] - 17s 747us/sample - loss: 5.0625 - rmse: 2.1777 - val_loss: 3.7617 - val_rmse: 1.6873\n",
      "Epoch 178/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 5.0571 - rmse: 2.1751 - val_loss: 3.8532 - val_rmse: 1.7064\n",
      "Epoch 179/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 4.9948 - rmse: 2.1625 - val_loss: 3.6037 - val_rmse: 1.6486\n",
      "Epoch 180/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 4.9605 - rmse: 2.1559 - val_loss: 3.7257 - val_rmse: 1.6824\n",
      "Epoch 181/1500\n",
      "22470/22470 [==============================] - 17s 747us/sample - loss: 4.9770 - rmse: 2.1577 - val_loss: 3.6627 - val_rmse: 1.6624\n",
      "Epoch 182/1500\n",
      "22470/22470 [==============================] - 17s 756us/sample - loss: 4.9485 - rmse: 2.1509 - val_loss: 3.5940 - val_rmse: 1.6422\n",
      "Epoch 183/1500\n",
      "22470/22470 [==============================] - 17s 754us/sample - loss: 4.9214 - rmse: 2.1452 - val_loss: 3.5510 - val_rmse: 1.6396\n",
      "Epoch 184/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 4.9128 - rmse: 2.1445 - val_loss: 3.5903 - val_rmse: 1.6465\n",
      "Epoch 185/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 4.8553 - rmse: 2.1300 - val_loss: 3.7189 - val_rmse: 1.6695\n",
      "Epoch 186/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 4.8610 - rmse: 2.1305 - val_loss: 3.6108 - val_rmse: 1.6492\n",
      "Epoch 187/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 4.8017 - rmse: 2.1189 - val_loss: 3.5835 - val_rmse: 1.6477\n",
      "Epoch 188/1500\n",
      "22470/22470 [==============================] - 17s 747us/sample - loss: 4.8120 - rmse: 2.1211 - val_loss: 3.6784 - val_rmse: 1.6707\n",
      "Epoch 189/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 4.7676 - rmse: 2.1105 - val_loss: 3.6483 - val_rmse: 1.6597\n",
      "Epoch 190/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 4.7955 - rmse: 2.1166 - val_loss: 3.7070 - val_rmse: 1.6877\n",
      "Epoch 191/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 4.7749 - rmse: 2.1119 - val_loss: 3.7592 - val_rmse: 1.6886\n",
      "Epoch 192/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 4.7343 - rmse: 2.1022 - val_loss: 3.7085 - val_rmse: 1.6791\n",
      "Epoch 193/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 4.7331 - rmse: 2.0999 - val_loss: 3.5757 - val_rmse: 1.6439\n",
      "Epoch 194/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 4.6839 - rmse: 2.0920 - val_loss: 3.6492 - val_rmse: 1.6545\n",
      "Epoch 195/1500\n",
      "22470/22470 [==============================] - 17s 754us/sample - loss: 4.6923 - rmse: 2.0915 - val_loss: 3.5866 - val_rmse: 1.6367\n",
      "Epoch 196/1500\n",
      "22470/22470 [==============================] - 17s 754us/sample - loss: 4.6371 - rmse: 2.0804 - val_loss: 3.6146 - val_rmse: 1.6513\n",
      "Epoch 197/1500\n",
      "22470/22470 [==============================] - 17s 757us/sample - loss: 4.6251 - rmse: 2.0767 - val_loss: 3.5816 - val_rmse: 1.6342\n",
      "Epoch 198/1500\n",
      "22470/22470 [==============================] - 17s 755us/sample - loss: 4.6058 - rmse: 2.0732 - val_loss: 3.5276 - val_rmse: 1.6337\n",
      "Epoch 199/1500\n",
      "22470/22470 [==============================] - 17s 757us/sample - loss: 4.5998 - rmse: 2.0719 - val_loss: 3.5256 - val_rmse: 1.6235\n",
      "Epoch 200/1500\n",
      "22470/22470 [==============================] - 17s 756us/sample - loss: 4.6002 - rmse: 2.0711 - val_loss: 3.4554 - val_rmse: 1.6118\n",
      "Epoch 201/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 4.5442 - rmse: 2.0596 - val_loss: 3.6178 - val_rmse: 1.6573\n",
      "Epoch 202/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 4.5292 - rmse: 2.0547 - val_loss: 3.5442 - val_rmse: 1.6309\n",
      "Epoch 203/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 4.5150 - rmse: 2.0513 - val_loss: 3.4879 - val_rmse: 1.6166\n",
      "Epoch 204/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 4.5001 - rmse: 2.0486 - val_loss: 3.5914 - val_rmse: 1.6461\n",
      "Epoch 205/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 4.5181 - rmse: 2.0506 - val_loss: 3.6220 - val_rmse: 1.6602\n",
      "Epoch 206/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 4.4906 - rmse: 2.0441 - val_loss: 3.5010 - val_rmse: 1.6183\n",
      "Epoch 207/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 4.4497 - rmse: 2.0358 - val_loss: 3.5715 - val_rmse: 1.6369\n",
      "Epoch 208/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 4.4312 - rmse: 2.0313 - val_loss: 3.7006 - val_rmse: 1.6609\n",
      "Epoch 209/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 4.4417 - rmse: 2.0319 - val_loss: 3.6255 - val_rmse: 1.6405\n",
      "Epoch 210/1500\n",
      "22470/22470 [==============================] - 17s 755us/sample - loss: 4.3906 - rmse: 2.0214 - val_loss: 3.4343 - val_rmse: 1.6035\n",
      "Epoch 211/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 4.4076 - rmse: 2.0239 - val_loss: 3.4391 - val_rmse: 1.6063\n",
      "Epoch 212/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 4.3540 - rmse: 2.0134 - val_loss: 3.5717 - val_rmse: 1.6332\n",
      "Epoch 213/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 4.3693 - rmse: 2.0158 - val_loss: 3.6357 - val_rmse: 1.6520\n",
      "Epoch 214/1500\n",
      "22470/22470 [==============================] - 17s 757us/sample - loss: 4.3736 - rmse: 2.0154 - val_loss: 3.4123 - val_rmse: 1.6015\n",
      "Epoch 215/1500\n",
      "22470/22470 [==============================] - 17s 759us/sample - loss: 4.3184 - rmse: 2.0033 - val_loss: 3.4159 - val_rmse: 1.6002\n",
      "Epoch 216/1500\n",
      "22470/22470 [==============================] - 17s 754us/sample - loss: 4.3004 - rmse: 1.9994 - val_loss: 3.4795 - val_rmse: 1.6112\n",
      "Epoch 217/1500\n",
      "22470/22470 [==============================] - 17s 757us/sample - loss: 4.2961 - rmse: 1.9989 - val_loss: 3.4030 - val_rmse: 1.5973\n",
      "Epoch 218/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 4.2907 - rmse: 1.9962 - val_loss: 3.4718 - val_rmse: 1.6131\n",
      "Epoch 219/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 4.2675 - rmse: 1.9909 - val_loss: 3.6733 - val_rmse: 1.6596\n",
      "Epoch 220/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 4.2648 - rmse: 1.9905 - val_loss: 3.4321 - val_rmse: 1.6050\n",
      "Epoch 221/1500\n",
      "22470/22470 [==============================] - 17s 753us/sample - loss: 4.2327 - rmse: 1.9820 - val_loss: 3.3909 - val_rmse: 1.5879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 222/1500\n",
      "22470/22470 [==============================] - 17s 747us/sample - loss: 4.2205 - rmse: 1.9795 - val_loss: 3.5261 - val_rmse: 1.6279\n",
      "Epoch 223/1500\n",
      "22470/22470 [==============================] - 17s 746us/sample - loss: 4.2265 - rmse: 1.9800 - val_loss: 3.3833 - val_rmse: 1.5888\n",
      "Epoch 224/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 4.1771 - rmse: 1.9704 - val_loss: 3.5599 - val_rmse: 1.6350\n",
      "Epoch 225/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 4.1874 - rmse: 1.9704 - val_loss: 3.3410 - val_rmse: 1.5805\n",
      "Epoch 226/1500\n",
      "22470/22470 [==============================] - 17s 746us/sample - loss: 4.1728 - rmse: 1.9680 - val_loss: 3.4170 - val_rmse: 1.5997\n",
      "Epoch 227/1500\n",
      "22470/22470 [==============================] - 17s 746us/sample - loss: 4.1326 - rmse: 1.9579 - val_loss: 3.4310 - val_rmse: 1.6004\n",
      "Epoch 228/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 4.1373 - rmse: 1.9582 - val_loss: 3.3751 - val_rmse: 1.5963\n",
      "Epoch 229/1500\n",
      "22470/22470 [==============================] - 17s 747us/sample - loss: 4.1331 - rmse: 1.9572 - val_loss: 3.3660 - val_rmse: 1.5864\n",
      "Epoch 230/1500\n",
      "22470/22470 [==============================] - 17s 753us/sample - loss: 4.1192 - rmse: 1.9540 - val_loss: 3.3319 - val_rmse: 1.5741\n",
      "Epoch 231/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 4.1026 - rmse: 1.9496 - val_loss: 3.4096 - val_rmse: 1.5930\n",
      "Epoch 232/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 4.0700 - rmse: 1.9432 - val_loss: 3.5102 - val_rmse: 1.6141\n",
      "Epoch 233/1500\n",
      "22470/22470 [==============================] - 17s 746us/sample - loss: 4.0523 - rmse: 1.9381 - val_loss: 3.4232 - val_rmse: 1.6036\n",
      "Epoch 234/1500\n",
      "22470/22470 [==============================] - 17s 747us/sample - loss: 4.0783 - rmse: 1.9444 - val_loss: 3.3972 - val_rmse: 1.5832\n",
      "Epoch 235/1500\n",
      "22470/22470 [==============================] - 17s 764us/sample - loss: 4.0405 - rmse: 1.9357 - val_loss: 3.2694 - val_rmse: 1.5610\n",
      "Epoch 236/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 4.0234 - rmse: 1.9304 - val_loss: 3.4865 - val_rmse: 1.6155\n",
      "Epoch 237/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 4.0212 - rmse: 1.9288 - val_loss: 3.3589 - val_rmse: 1.5810\n",
      "Epoch 238/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 4.0132 - rmse: 1.9278 - val_loss: 3.5703 - val_rmse: 1.6350\n",
      "Epoch 239/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 4.0158 - rmse: 1.9279 - val_loss: 3.4413 - val_rmse: 1.5988\n",
      "Epoch 240/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 3.9890 - rmse: 1.9203 - val_loss: 3.3713 - val_rmse: 1.5870\n",
      "Epoch 241/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 3.9860 - rmse: 1.9197 - val_loss: 3.4909 - val_rmse: 1.6112\n",
      "Epoch 242/1500\n",
      "22470/22470 [==============================] - 17s 747us/sample - loss: 3.9859 - rmse: 1.9205 - val_loss: 3.4292 - val_rmse: 1.5958\n",
      "Epoch 243/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 3.9355 - rmse: 1.9084 - val_loss: 3.3254 - val_rmse: 1.5764\n",
      "Epoch 244/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 3.9364 - rmse: 1.9079 - val_loss: 3.3033 - val_rmse: 1.5719\n",
      "Epoch 245/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 3.9139 - rmse: 1.9019 - val_loss: 3.3562 - val_rmse: 1.5829\n",
      "Epoch 246/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 3.9141 - rmse: 1.9021 - val_loss: 3.3596 - val_rmse: 1.5742\n",
      "Epoch 247/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 3.8967 - rmse: 1.8983 - val_loss: 3.5055 - val_rmse: 1.6080\n",
      "Epoch 248/1500\n",
      "22470/22470 [==============================] - 17s 755us/sample - loss: 3.8960 - rmse: 1.8969 - val_loss: 3.2451 - val_rmse: 1.5496\n",
      "Epoch 249/1500\n",
      "22470/22470 [==============================] - 17s 753us/sample - loss: 3.8631 - rmse: 1.8898 - val_loss: 3.3288 - val_rmse: 1.5676\n",
      "Epoch 250/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 3.8825 - rmse: 1.8934 - val_loss: 3.3649 - val_rmse: 1.5762\n",
      "Epoch 251/1500\n",
      "22470/22470 [==============================] - 17s 755us/sample - loss: 3.8543 - rmse: 1.8882 - val_loss: 3.3366 - val_rmse: 1.5699\n",
      "Epoch 252/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 3.8438 - rmse: 1.8831 - val_loss: 3.2473 - val_rmse: 1.5523\n",
      "Epoch 253/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 3.8272 - rmse: 1.8800 - val_loss: 3.2450 - val_rmse: 1.5522\n",
      "Epoch 254/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 3.7955 - rmse: 1.8731 - val_loss: 3.2581 - val_rmse: 1.5499\n",
      "Epoch 255/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 3.8102 - rmse: 1.8747 - val_loss: 3.2939 - val_rmse: 1.5694\n",
      "Epoch 256/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 3.7819 - rmse: 1.8706 - val_loss: 3.3077 - val_rmse: 1.5629\n",
      "Epoch 257/1500\n",
      "22470/22470 [==============================] - 17s 753us/sample - loss: 3.7842 - rmse: 1.8683 - val_loss: 3.2371 - val_rmse: 1.5476\n",
      "Epoch 258/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 3.7772 - rmse: 1.8670 - val_loss: 3.3249 - val_rmse: 1.5758\n",
      "Epoch 259/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 3.7567 - rmse: 1.8629 - val_loss: 3.3505 - val_rmse: 1.5682\n",
      "Epoch 260/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 3.7363 - rmse: 1.8567 - val_loss: 3.3160 - val_rmse: 1.5609\n",
      "Epoch 261/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 3.7310 - rmse: 1.8561 - val_loss: 3.3420 - val_rmse: 1.5772\n",
      "Epoch 262/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 3.7256 - rmse: 1.8545 - val_loss: 3.2928 - val_rmse: 1.5597\n",
      "Epoch 263/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 3.6999 - rmse: 1.8484 - val_loss: 3.4575 - val_rmse: 1.5957\n",
      "Epoch 264/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 3.7121 - rmse: 1.8495 - val_loss: 3.4052 - val_rmse: 1.5753\n",
      "Epoch 265/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 3.7139 - rmse: 1.8508 - val_loss: 3.3206 - val_rmse: 1.5632\n",
      "Epoch 266/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 3.7061 - rmse: 1.8475 - val_loss: 3.2621 - val_rmse: 1.5488\n",
      "Epoch 267/1500\n",
      "22470/22470 [==============================] - 17s 762us/sample - loss: 3.6799 - rmse: 1.8427 - val_loss: 3.2319 - val_rmse: 1.5436\n",
      "Epoch 268/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 3.6523 - rmse: 1.8358 - val_loss: 3.2526 - val_rmse: 1.5449\n",
      "Epoch 269/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 3.6569 - rmse: 1.8351 - val_loss: 3.2491 - val_rmse: 1.5517\n",
      "Epoch 270/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 3.6674 - rmse: 1.8377 - val_loss: 3.3420 - val_rmse: 1.5667\n",
      "Epoch 271/1500\n",
      "22470/22470 [==============================] - 17s 756us/sample - loss: 3.6250 - rmse: 1.8275 - val_loss: 3.1493 - val_rmse: 1.5246\n",
      "Epoch 272/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 3.6458 - rmse: 1.8326 - val_loss: 3.4703 - val_rmse: 1.6043\n",
      "Epoch 273/1500\n",
      "22470/22470 [==============================] - 17s 754us/sample - loss: 3.6400 - rmse: 1.8309 - val_loss: 3.1251 - val_rmse: 1.5187\n",
      "Epoch 274/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 3.6015 - rmse: 1.8208 - val_loss: 3.1984 - val_rmse: 1.5330\n",
      "Epoch 275/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 3.6202 - rmse: 1.8254 - val_loss: 3.2118 - val_rmse: 1.5429\n",
      "Epoch 276/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 3.6173 - rmse: 1.8237 - val_loss: 3.2783 - val_rmse: 1.5549\n",
      "Epoch 277/1500\n",
      "22470/22470 [==============================] - 17s 753us/sample - loss: 3.5823 - rmse: 1.8162 - val_loss: 3.2511 - val_rmse: 1.5427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 278/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 3.5792 - rmse: 1.8157 - val_loss: 3.2387 - val_rmse: 1.5405\n",
      "Epoch 279/1500\n",
      "22470/22470 [==============================] - 17s 746us/sample - loss: 3.5782 - rmse: 1.8144 - val_loss: 3.1974 - val_rmse: 1.5345\n",
      "Epoch 280/1500\n",
      "22470/22470 [==============================] - 17s 753us/sample - loss: 3.5423 - rmse: 1.8061 - val_loss: 3.1140 - val_rmse: 1.5140\n",
      "Epoch 281/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 3.5571 - rmse: 1.8082 - val_loss: 3.2371 - val_rmse: 1.5354\n",
      "Epoch 282/1500\n",
      "22470/22470 [==============================] - 17s 746us/sample - loss: 3.5644 - rmse: 1.8106 - val_loss: 3.1382 - val_rmse: 1.5147\n",
      "Epoch 283/1500\n",
      "22470/22470 [==============================] - 17s 746us/sample - loss: 3.5312 - rmse: 1.8029 - val_loss: 3.2438 - val_rmse: 1.5411\n",
      "Epoch 284/1500\n",
      "22470/22470 [==============================] - 17s 746us/sample - loss: 3.5280 - rmse: 1.8005 - val_loss: 3.1745 - val_rmse: 1.5261\n",
      "Epoch 285/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 3.5288 - rmse: 1.8008 - val_loss: 3.2558 - val_rmse: 1.5485\n",
      "Epoch 286/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 3.5051 - rmse: 1.7952 - val_loss: 3.1846 - val_rmse: 1.5255\n",
      "Epoch 287/1500\n",
      "22470/22470 [==============================] - 17s 753us/sample - loss: 3.4969 - rmse: 1.7931 - val_loss: 3.3061 - val_rmse: 1.5572\n",
      "Epoch 288/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 3.5029 - rmse: 1.7939 - val_loss: 3.1542 - val_rmse: 1.5237\n",
      "Epoch 289/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 3.4866 - rmse: 1.7889 - val_loss: 3.1235 - val_rmse: 1.5185\n",
      "Epoch 290/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 3.4706 - rmse: 1.7858 - val_loss: 3.1083 - val_rmse: 1.5103\n",
      "Epoch 291/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 3.4667 - rmse: 1.7837 - val_loss: 3.2003 - val_rmse: 1.5277\n",
      "Epoch 292/1500\n",
      "22470/22470 [==============================] - 17s 747us/sample - loss: 3.4678 - rmse: 1.7838 - val_loss: 3.1404 - val_rmse: 1.5106\n",
      "Epoch 293/1500\n",
      "22470/22470 [==============================] - 17s 747us/sample - loss: 3.4656 - rmse: 1.7836 - val_loss: 3.1743 - val_rmse: 1.5161\n",
      "Epoch 294/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 3.4519 - rmse: 1.7809 - val_loss: 3.2473 - val_rmse: 1.5538\n",
      "Epoch 295/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 3.4418 - rmse: 1.7778 - val_loss: 3.1580 - val_rmse: 1.5169\n",
      "Epoch 296/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 3.3859 - rmse: 1.7653 - val_loss: 3.1225 - val_rmse: 1.5136\n",
      "Epoch 297/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 3.4035 - rmse: 1.7669 - val_loss: 3.1300 - val_rmse: 1.5202\n",
      "Epoch 298/1500\n",
      "22470/22470 [==============================] - 17s 756us/sample - loss: 3.4084 - rmse: 1.7697 - val_loss: 3.1021 - val_rmse: 1.5064\n",
      "Epoch 299/1500\n",
      "22470/22470 [==============================] - 17s 754us/sample - loss: 3.3723 - rmse: 1.7594 - val_loss: 3.1144 - val_rmse: 1.5036\n",
      "Epoch 300/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 3.4018 - rmse: 1.7670 - val_loss: 3.1491 - val_rmse: 1.5094\n",
      "Epoch 301/1500\n",
      "22470/22470 [==============================] - 17s 747us/sample - loss: 3.4005 - rmse: 1.7658 - val_loss: 3.1444 - val_rmse: 1.5092\n",
      "Epoch 302/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 3.3958 - rmse: 1.7646 - val_loss: 3.1357 - val_rmse: 1.5170\n",
      "Epoch 303/1500\n",
      "22470/22470 [==============================] - 17s 753us/sample - loss: 3.3697 - rmse: 1.7578 - val_loss: 3.0994 - val_rmse: 1.4976\n",
      "Epoch 304/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 3.3421 - rmse: 1.7509 - val_loss: 3.1156 - val_rmse: 1.5160\n",
      "Epoch 305/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 3.3522 - rmse: 1.7529 - val_loss: 3.2508 - val_rmse: 1.5372\n",
      "Epoch 306/1500\n",
      "22470/22470 [==============================] - 17s 754us/sample - loss: 3.3566 - rmse: 1.7539 - val_loss: 2.9946 - val_rmse: 1.4755\n",
      "Epoch 307/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 3.3461 - rmse: 1.7521 - val_loss: 3.0469 - val_rmse: 1.4928\n",
      "Epoch 308/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 3.3471 - rmse: 1.7500 - val_loss: 3.0812 - val_rmse: 1.5010\n",
      "Epoch 309/1500\n",
      "22470/22470 [==============================] - 17s 747us/sample - loss: 3.3142 - rmse: 1.7436 - val_loss: 3.1633 - val_rmse: 1.5163\n",
      "Epoch 310/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 3.3123 - rmse: 1.7428 - val_loss: 3.1213 - val_rmse: 1.5054\n",
      "Epoch 311/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 3.3183 - rmse: 1.7447 - val_loss: 3.1379 - val_rmse: 1.5073\n",
      "Epoch 312/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 3.3168 - rmse: 1.7431 - val_loss: 3.1258 - val_rmse: 1.5053\n",
      "Epoch 313/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 3.2999 - rmse: 1.7391 - val_loss: 3.0460 - val_rmse: 1.4887\n",
      "Epoch 314/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 3.2757 - rmse: 1.7331 - val_loss: 3.0879 - val_rmse: 1.5008\n",
      "Epoch 315/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 3.3074 - rmse: 1.7398 - val_loss: 3.0528 - val_rmse: 1.4890\n",
      "Epoch 316/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 3.2642 - rmse: 1.7298 - val_loss: 3.0242 - val_rmse: 1.4841\n",
      "Epoch 317/1500\n",
      "22470/22470 [==============================] - 17s 757us/sample - loss: 3.2618 - rmse: 1.7275 - val_loss: 2.9957 - val_rmse: 1.4711\n",
      "Epoch 318/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 3.2730 - rmse: 1.7310 - val_loss: 3.0700 - val_rmse: 1.4966\n",
      "Epoch 319/1500\n",
      "22470/22470 [==============================] - 17s 755us/sample - loss: 3.2624 - rmse: 1.7274 - val_loss: 2.9761 - val_rmse: 1.4692\n",
      "Epoch 320/1500\n",
      "22470/22470 [==============================] - 17s 754us/sample - loss: 3.2533 - rmse: 1.7252 - val_loss: 3.0185 - val_rmse: 1.4808\n",
      "Epoch 321/1500\n",
      "22470/22470 [==============================] - 17s 753us/sample - loss: 3.2652 - rmse: 1.7280 - val_loss: 3.0094 - val_rmse: 1.4777\n",
      "Epoch 322/1500\n",
      "22470/22470 [==============================] - 17s 759us/sample - loss: 3.2478 - rmse: 1.7233 - val_loss: 3.0385 - val_rmse: 1.4855\n",
      "Epoch 323/1500\n",
      "22470/22470 [==============================] - 17s 762us/sample - loss: 3.2450 - rmse: 1.7213 - val_loss: 2.9525 - val_rmse: 1.4598\n",
      "Epoch 324/1500\n",
      "22470/22470 [==============================] - 17s 753us/sample - loss: 3.2083 - rmse: 1.7135 - val_loss: 3.0016 - val_rmse: 1.4762\n",
      "Epoch 325/1500\n",
      "22470/22470 [==============================] - 17s 754us/sample - loss: 3.2116 - rmse: 1.7142 - val_loss: 2.9594 - val_rmse: 1.4610\n",
      "Epoch 326/1500\n",
      "22470/22470 [==============================] - 17s 755us/sample - loss: 3.1995 - rmse: 1.7106 - val_loss: 2.9974 - val_rmse: 1.4701\n",
      "Epoch 327/1500\n",
      "22470/22470 [==============================] - 17s 753us/sample - loss: 3.2113 - rmse: 1.7123 - val_loss: 3.0418 - val_rmse: 1.4819\n",
      "Epoch 328/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 3.1825 - rmse: 1.7060 - val_loss: 3.0592 - val_rmse: 1.4876\n",
      "Epoch 329/1500\n",
      "22470/22470 [==============================] - 17s 753us/sample - loss: 3.2119 - rmse: 1.7135 - val_loss: 3.0084 - val_rmse: 1.4723\n",
      "Epoch 330/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 3.2054 - rmse: 1.7098 - val_loss: 3.0405 - val_rmse: 1.4827\n",
      "Epoch 331/1500\n",
      "22470/22470 [==============================] - 17s 755us/sample - loss: 3.1672 - rmse: 1.7017 - val_loss: 3.1891 - val_rmse: 1.5276\n",
      "Epoch 332/1500\n",
      "22470/22470 [==============================] - 17s 753us/sample - loss: 3.1729 - rmse: 1.7023 - val_loss: 3.0579 - val_rmse: 1.4835\n",
      "Epoch 333/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 3.1770 - rmse: 1.7034 - val_loss: 3.0071 - val_rmse: 1.4760\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 334/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 3.1581 - rmse: 1.6990 - val_loss: 3.0957 - val_rmse: 1.5017\n",
      "Epoch 335/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 3.1611 - rmse: 1.7000 - val_loss: 3.0096 - val_rmse: 1.4689\n",
      "Epoch 336/1500\n",
      "22470/22470 [==============================] - 17s 747us/sample - loss: 3.1451 - rmse: 1.6955 - val_loss: 2.9750 - val_rmse: 1.4607\n",
      "Epoch 337/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 3.1396 - rmse: 1.6934 - val_loss: 3.0193 - val_rmse: 1.4749\n",
      "Epoch 338/1500\n",
      "22470/22470 [==============================] - 17s 756us/sample - loss: 3.1348 - rmse: 1.6920 - val_loss: 2.9373 - val_rmse: 1.4504\n",
      "Epoch 339/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 3.1530 - rmse: 1.6955 - val_loss: 3.0344 - val_rmse: 1.4747\n",
      "Epoch 340/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 3.1095 - rmse: 1.6848 - val_loss: 3.0984 - val_rmse: 1.4957\n",
      "Epoch 341/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 3.1270 - rmse: 1.6902 - val_loss: 3.0260 - val_rmse: 1.4706\n",
      "Epoch 342/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 3.1182 - rmse: 1.6880 - val_loss: 2.9998 - val_rmse: 1.4689\n",
      "Epoch 343/1500\n",
      "22470/22470 [==============================] - 17s 747us/sample - loss: 3.0883 - rmse: 1.6797 - val_loss: 3.0132 - val_rmse: 1.4721\n",
      "Epoch 344/1500\n",
      "22470/22470 [==============================] - 17s 747us/sample - loss: 3.1011 - rmse: 1.6827 - val_loss: 2.9620 - val_rmse: 1.4564\n",
      "Epoch 345/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 3.0967 - rmse: 1.6811 - val_loss: 2.9402 - val_rmse: 1.4506\n",
      "Epoch 346/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 3.0742 - rmse: 1.6746 - val_loss: 3.0146 - val_rmse: 1.4657\n",
      "Epoch 347/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 3.1032 - rmse: 1.6822 - val_loss: 2.9828 - val_rmse: 1.4565\n",
      "Epoch 348/1500\n",
      "22470/22470 [==============================] - 17s 747us/sample - loss: 3.0858 - rmse: 1.6767 - val_loss: 2.9675 - val_rmse: 1.4520\n",
      "Epoch 349/1500\n",
      "22470/22470 [==============================] - 17s 747us/sample - loss: 3.0591 - rmse: 1.6700 - val_loss: 2.9519 - val_rmse: 1.4517\n",
      "Epoch 350/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 3.0494 - rmse: 1.6679 - val_loss: 2.9636 - val_rmse: 1.4536\n",
      "Epoch 351/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 3.0364 - rmse: 1.6650 - val_loss: 2.9764 - val_rmse: 1.4524\n",
      "Epoch 352/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 3.0736 - rmse: 1.6738 - val_loss: 2.9868 - val_rmse: 1.4667\n",
      "Epoch 353/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 3.0605 - rmse: 1.6700 - val_loss: 3.0223 - val_rmse: 1.4681\n",
      "Epoch 354/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 3.0356 - rmse: 1.6629 - val_loss: 3.0014 - val_rmse: 1.4560\n",
      "Epoch 355/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 3.0458 - rmse: 1.6667 - val_loss: 2.9664 - val_rmse: 1.4519\n",
      "Epoch 356/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 3.0257 - rmse: 1.6608 - val_loss: 3.0131 - val_rmse: 1.4547\n",
      "Epoch 357/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 3.0358 - rmse: 1.6615 - val_loss: 2.9917 - val_rmse: 1.4539\n",
      "Epoch 358/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 3.0241 - rmse: 1.6603 - val_loss: 2.9855 - val_rmse: 1.4544\n",
      "Epoch 359/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 3.0188 - rmse: 1.6592 - val_loss: 2.9824 - val_rmse: 1.4559\n",
      "Epoch 360/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 3.0058 - rmse: 1.6550 - val_loss: 3.0251 - val_rmse: 1.4618\n",
      "Epoch 361/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 3.0137 - rmse: 1.6570 - val_loss: 3.2300 - val_rmse: 1.5254\n",
      "Epoch 362/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 3.0018 - rmse: 1.6529 - val_loss: 2.9837 - val_rmse: 1.4581\n",
      "Epoch 363/1500\n",
      "22470/22470 [==============================] - 17s 753us/sample - loss: 2.9970 - rmse: 1.6512 - val_loss: 2.9810 - val_rmse: 1.4494\n",
      "Epoch 364/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 2.9930 - rmse: 1.6493 - val_loss: 3.0277 - val_rmse: 1.4707\n",
      "Epoch 365/1500\n",
      "22470/22470 [==============================] - 17s 756us/sample - loss: 2.9849 - rmse: 1.6477 - val_loss: 2.9125 - val_rmse: 1.4372\n",
      "Epoch 366/1500\n",
      "22470/22470 [==============================] - 17s 758us/sample - loss: 2.9781 - rmse: 1.6464 - val_loss: 2.8826 - val_rmse: 1.4348\n",
      "Epoch 367/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 2.9684 - rmse: 1.6440 - val_loss: 2.9293 - val_rmse: 1.4458\n",
      "Epoch 368/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 2.9663 - rmse: 1.6430 - val_loss: 2.9487 - val_rmse: 1.4442\n",
      "Epoch 369/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 2.9708 - rmse: 1.6446 - val_loss: 3.0070 - val_rmse: 1.4585\n",
      "Epoch 370/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 2.9479 - rmse: 1.6375 - val_loss: 2.9680 - val_rmse: 1.4472\n",
      "Epoch 371/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 2.9486 - rmse: 1.6379 - val_loss: 2.9132 - val_rmse: 1.4395\n",
      "Epoch 372/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 2.9376 - rmse: 1.6343 - val_loss: 2.9681 - val_rmse: 1.4533\n",
      "Epoch 373/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 2.9602 - rmse: 1.6387 - val_loss: 3.0139 - val_rmse: 1.4678\n",
      "Epoch 374/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 2.9221 - rmse: 1.6301 - val_loss: 2.9101 - val_rmse: 1.4381\n",
      "Epoch 375/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 2.9218 - rmse: 1.6292 - val_loss: 2.9943 - val_rmse: 1.4565\n",
      "Epoch 376/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 2.9260 - rmse: 1.6310 - val_loss: 2.9380 - val_rmse: 1.4387\n",
      "Epoch 377/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 2.9339 - rmse: 1.6322 - val_loss: 2.9311 - val_rmse: 1.4375\n",
      "Epoch 378/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 2.9169 - rmse: 1.6291 - val_loss: 2.9681 - val_rmse: 1.4443\n",
      "Epoch 379/1500\n",
      "22470/22470 [==============================] - 17s 755us/sample - loss: 2.8978 - rmse: 1.6243 - val_loss: 2.8579 - val_rmse: 1.4253\n",
      "Epoch 380/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 2.9050 - rmse: 1.6249 - val_loss: 2.9256 - val_rmse: 1.4358\n",
      "Epoch 381/1500\n",
      "22470/22470 [==============================] - 17s 754us/sample - loss: 2.9224 - rmse: 1.6281 - val_loss: 2.8305 - val_rmse: 1.4134\n",
      "Epoch 382/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 2.9032 - rmse: 1.6246 - val_loss: 2.8943 - val_rmse: 1.4272\n",
      "Epoch 383/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 2.8999 - rmse: 1.6231 - val_loss: 2.8424 - val_rmse: 1.4191\n",
      "Epoch 384/1500\n",
      "22470/22470 [==============================] - 17s 753us/sample - loss: 2.9026 - rmse: 1.6234 - val_loss: 2.8504 - val_rmse: 1.4188\n",
      "Epoch 385/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 2.8915 - rmse: 1.6214 - val_loss: 2.8737 - val_rmse: 1.4235\n",
      "Epoch 386/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 2.8678 - rmse: 1.6151 - val_loss: 2.8864 - val_rmse: 1.4312\n",
      "Epoch 387/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 2.8909 - rmse: 1.6194 - val_loss: 2.9621 - val_rmse: 1.4509\n",
      "Epoch 388/1500\n",
      "22470/22470 [==============================] - 17s 753us/sample - loss: 2.8666 - rmse: 1.6135 - val_loss: 2.9840 - val_rmse: 1.4522\n",
      "Epoch 389/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 2.8780 - rmse: 1.6157 - val_loss: 2.9258 - val_rmse: 1.4361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 390/1500\n",
      "22470/22470 [==============================] - 17s 747us/sample - loss: 2.8714 - rmse: 1.6133 - val_loss: 2.8997 - val_rmse: 1.4288\n",
      "Epoch 391/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 2.8740 - rmse: 1.6151 - val_loss: 2.8085 - val_rmse: 1.4094\n",
      "Epoch 392/1500\n",
      "22470/22470 [==============================] - 17s 756us/sample - loss: 2.8617 - rmse: 1.6121 - val_loss: 2.8751 - val_rmse: 1.4208\n",
      "Epoch 393/1500\n",
      "22470/22470 [==============================] - 17s 747us/sample - loss: 2.8378 - rmse: 1.6043 - val_loss: 2.9037 - val_rmse: 1.4402\n",
      "Epoch 394/1500\n",
      "22470/22470 [==============================] - 17s 747us/sample - loss: 2.8271 - rmse: 1.6021 - val_loss: 2.9155 - val_rmse: 1.4379\n",
      "Epoch 395/1500\n",
      "22470/22470 [==============================] - 17s 747us/sample - loss: 2.8336 - rmse: 1.6038 - val_loss: 2.8544 - val_rmse: 1.4189\n",
      "Epoch 396/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 2.8217 - rmse: 1.6004 - val_loss: 2.8584 - val_rmse: 1.4175\n",
      "Epoch 397/1500\n",
      "22470/22470 [==============================] - 17s 747us/sample - loss: 2.8264 - rmse: 1.6009 - val_loss: 2.9273 - val_rmse: 1.4454\n",
      "Epoch 398/1500\n",
      "22470/22470 [==============================] - 17s 745us/sample - loss: 2.8300 - rmse: 1.6030 - val_loss: 2.9628 - val_rmse: 1.4565\n",
      "Epoch 399/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 2.8340 - rmse: 1.6031 - val_loss: 2.8512 - val_rmse: 1.4150\n",
      "Epoch 400/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 2.8173 - rmse: 1.5990 - val_loss: 2.8240 - val_rmse: 1.4152\n",
      "Epoch 401/1500\n",
      "22470/22470 [==============================] - 17s 756us/sample - loss: 2.7999 - rmse: 1.5933 - val_loss: 2.9124 - val_rmse: 1.4343\n",
      "Epoch 402/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 2.8078 - rmse: 1.5956 - val_loss: 2.8310 - val_rmse: 1.4080\n",
      "Epoch 403/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 2.8149 - rmse: 1.5979 - val_loss: 2.8943 - val_rmse: 1.4284\n",
      "Epoch 404/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 2.7943 - rmse: 1.5924 - val_loss: 2.9460 - val_rmse: 1.4371\n",
      "Epoch 405/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 2.8017 - rmse: 1.5935 - val_loss: 2.8645 - val_rmse: 1.4211\n",
      "Epoch 406/1500\n",
      "22470/22470 [==============================] - 17s 747us/sample - loss: 2.8003 - rmse: 1.5924 - val_loss: 2.9729 - val_rmse: 1.4523\n",
      "Epoch 407/1500\n",
      "22470/22470 [==============================] - 17s 755us/sample - loss: 2.7959 - rmse: 1.5914 - val_loss: 2.7959 - val_rmse: 1.4029\n",
      "Epoch 408/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 2.7823 - rmse: 1.5870 - val_loss: 2.8711 - val_rmse: 1.4164\n",
      "Epoch 409/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 2.7914 - rmse: 1.5889 - val_loss: 2.9247 - val_rmse: 1.4248\n",
      "Epoch 410/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 2.7786 - rmse: 1.5864 - val_loss: 2.8269 - val_rmse: 1.4090\n",
      "Epoch 411/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 2.7348 - rmse: 1.5748 - val_loss: 2.9560 - val_rmse: 1.4318\n",
      "Epoch 412/1500\n",
      "22470/22470 [==============================] - 17s 753us/sample - loss: 2.7432 - rmse: 1.5767 - val_loss: 2.7431 - val_rmse: 1.3883\n",
      "Epoch 413/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 2.7600 - rmse: 1.5823 - val_loss: 2.7740 - val_rmse: 1.3983\n",
      "Epoch 414/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 2.7627 - rmse: 1.5814 - val_loss: 2.8579 - val_rmse: 1.4245\n",
      "Epoch 415/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 2.7618 - rmse: 1.5805 - val_loss: 2.8103 - val_rmse: 1.4066\n",
      "Epoch 416/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 2.7593 - rmse: 1.5810 - val_loss: 2.8558 - val_rmse: 1.4212\n",
      "Epoch 417/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 2.7336 - rmse: 1.5732 - val_loss: 2.7655 - val_rmse: 1.3978\n",
      "Epoch 418/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 2.7477 - rmse: 1.5771 - val_loss: 2.8507 - val_rmse: 1.4125\n",
      "Epoch 419/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 2.7406 - rmse: 1.5748 - val_loss: 2.7691 - val_rmse: 1.3927\n",
      "Epoch 420/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 2.7263 - rmse: 1.5700 - val_loss: 2.8477 - val_rmse: 1.4141\n",
      "Epoch 421/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 2.7218 - rmse: 1.5697 - val_loss: 2.7591 - val_rmse: 1.3908\n",
      "Epoch 422/1500\n",
      "22470/22470 [==============================] - 17s 753us/sample - loss: 2.7204 - rmse: 1.5683 - val_loss: 2.9252 - val_rmse: 1.4338\n",
      "Epoch 423/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 2.7130 - rmse: 1.5657 - val_loss: 2.7744 - val_rmse: 1.3927\n",
      "Epoch 424/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 2.7112 - rmse: 1.5663 - val_loss: 2.8738 - val_rmse: 1.4222\n",
      "Epoch 425/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 2.7204 - rmse: 1.5674 - val_loss: 2.7791 - val_rmse: 1.3964\n",
      "Epoch 426/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 2.7177 - rmse: 1.5676 - val_loss: 2.8179 - val_rmse: 1.4003\n",
      "Epoch 427/1500\n",
      "22470/22470 [==============================] - 17s 756us/sample - loss: 2.7019 - rmse: 1.5624 - val_loss: 2.7339 - val_rmse: 1.3787\n",
      "Epoch 428/1500\n",
      "22470/22470 [==============================] - 17s 753us/sample - loss: 2.7100 - rmse: 1.5636 - val_loss: 2.7749 - val_rmse: 1.3867\n",
      "Epoch 429/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 2.6886 - rmse: 1.5598 - val_loss: 2.7896 - val_rmse: 1.3947\n",
      "Epoch 430/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 2.6871 - rmse: 1.5597 - val_loss: 2.8963 - val_rmse: 1.4310\n",
      "Epoch 431/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 2.6985 - rmse: 1.5611 - val_loss: 2.7314 - val_rmse: 1.3841\n",
      "Epoch 432/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 2.6810 - rmse: 1.5561 - val_loss: 2.8229 - val_rmse: 1.4056\n",
      "Epoch 433/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 2.6903 - rmse: 1.5591 - val_loss: 2.7886 - val_rmse: 1.3853\n",
      "Epoch 434/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 2.6785 - rmse: 1.5554 - val_loss: 2.7759 - val_rmse: 1.3856\n",
      "Epoch 435/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 2.6860 - rmse: 1.5568 - val_loss: 2.7520 - val_rmse: 1.3814\n",
      "Epoch 436/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 2.6790 - rmse: 1.5549 - val_loss: 2.7876 - val_rmse: 1.3885\n",
      "Epoch 437/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 2.6669 - rmse: 1.5512 - val_loss: 2.8956 - val_rmse: 1.4075\n",
      "Epoch 438/1500\n",
      "22470/22470 [==============================] - 17s 759us/sample - loss: 2.6602 - rmse: 1.5504 - val_loss: 2.9052 - val_rmse: 1.4221\n",
      "Epoch 439/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 2.6615 - rmse: 1.5513 - val_loss: 2.8988 - val_rmse: 1.4083\n",
      "Epoch 440/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 2.6555 - rmse: 1.5479 - val_loss: 2.8652 - val_rmse: 1.4085\n",
      "Epoch 441/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 2.6554 - rmse: 1.5497 - val_loss: 2.8197 - val_rmse: 1.3904\n",
      "Epoch 442/1500\n",
      "22470/22470 [==============================] - 17s 753us/sample - loss: 2.6391 - rmse: 1.5441 - val_loss: 2.7710 - val_rmse: 1.3794\n",
      "Epoch 443/1500\n",
      "22470/22470 [==============================] - 17s 758us/sample - loss: 2.6519 - rmse: 1.5484 - val_loss: 2.7076 - val_rmse: 1.3692\n",
      "Epoch 444/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 2.6506 - rmse: 1.5458 - val_loss: 2.7728 - val_rmse: 1.3833\n",
      "Epoch 445/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 2.6545 - rmse: 1.5482 - val_loss: 2.8609 - val_rmse: 1.4040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 446/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 2.6388 - rmse: 1.5423 - val_loss: 2.7864 - val_rmse: 1.3845\n",
      "Epoch 447/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 2.6340 - rmse: 1.5420 - val_loss: 2.7217 - val_rmse: 1.3695\n",
      "Epoch 448/1500\n",
      "22470/22470 [==============================] - 18s 821us/sample - loss: 2.6233 - rmse: 1.5387 - val_loss: 2.7609 - val_rmse: 1.3821\n",
      "Epoch 449/1500\n",
      "22470/22470 [==============================] - 18s 789us/sample - loss: 2.6150 - rmse: 1.5373 - val_loss: 2.7238 - val_rmse: 1.3779\n",
      "Epoch 450/1500\n",
      "22470/22470 [==============================] - 18s 780us/sample - loss: 2.6149 - rmse: 1.5357 - val_loss: 2.7947 - val_rmse: 1.3879\n",
      "Epoch 451/1500\n",
      "22470/22470 [==============================] - 18s 779us/sample - loss: 2.6239 - rmse: 1.5384 - val_loss: 2.8714 - val_rmse: 1.4075\n",
      "Epoch 452/1500\n",
      "22470/22470 [==============================] - 18s 782us/sample - loss: 2.6018 - rmse: 1.5327 - val_loss: 2.8628 - val_rmse: 1.4089\n",
      "Epoch 453/1500\n",
      "22470/22470 [==============================] - 17s 776us/sample - loss: 2.6427 - rmse: 1.5431 - val_loss: 2.7325 - val_rmse: 1.3727\n",
      "Epoch 454/1500\n",
      "22470/22470 [==============================] - 18s 781us/sample - loss: 2.5871 - rmse: 1.5282 - val_loss: 2.7415 - val_rmse: 1.3805\n",
      "Epoch 455/1500\n",
      "22470/22470 [==============================] - 18s 784us/sample - loss: 2.5986 - rmse: 1.5309 - val_loss: 2.6635 - val_rmse: 1.3607\n",
      "Epoch 456/1500\n",
      "22470/22470 [==============================] - 18s 783us/sample - loss: 2.5944 - rmse: 1.5303 - val_loss: 2.7830 - val_rmse: 1.3791\n",
      "Epoch 457/1500\n",
      "22470/22470 [==============================] - 18s 781us/sample - loss: 2.6093 - rmse: 1.5340 - val_loss: 2.7222 - val_rmse: 1.3720\n",
      "Epoch 458/1500\n",
      "22470/22470 [==============================] - 18s 784us/sample - loss: 2.5948 - rmse: 1.5293 - val_loss: 2.7277 - val_rmse: 1.3695\n",
      "Epoch 459/1500\n",
      "22470/22470 [==============================] - 18s 782us/sample - loss: 2.5804 - rmse: 1.5267 - val_loss: 2.8248 - val_rmse: 1.4033\n",
      "Epoch 460/1500\n",
      "22470/22470 [==============================] - 18s 780us/sample - loss: 2.5852 - rmse: 1.5265 - val_loss: 2.8193 - val_rmse: 1.3911\n",
      "Epoch 461/1500\n",
      "22470/22470 [==============================] - 17s 777us/sample - loss: 2.5789 - rmse: 1.5247 - val_loss: 2.7734 - val_rmse: 1.3812\n",
      "Epoch 462/1500\n",
      "22470/22470 [==============================] - 18s 783us/sample - loss: 2.5791 - rmse: 1.5239 - val_loss: 2.6994 - val_rmse: 1.3646\n",
      "Epoch 463/1500\n",
      "22470/22470 [==============================] - 18s 788us/sample - loss: 2.5775 - rmse: 1.5243 - val_loss: 2.6784 - val_rmse: 1.3562\n",
      "Epoch 464/1500\n",
      "22470/22470 [==============================] - 18s 789us/sample - loss: 2.5633 - rmse: 1.5202 - val_loss: 2.6749 - val_rmse: 1.3557\n",
      "Epoch 465/1500\n",
      "22470/22470 [==============================] - 18s 786us/sample - loss: 2.5652 - rmse: 1.5197 - val_loss: 2.7649 - val_rmse: 1.3708\n",
      "Epoch 466/1500\n",
      "22470/22470 [==============================] - 18s 816us/sample - loss: 2.5570 - rmse: 1.5181 - val_loss: 2.7221 - val_rmse: 1.3678\n",
      "Epoch 467/1500\n",
      "22470/22470 [==============================] - 17s 773us/sample - loss: 2.5320 - rmse: 1.5118 - val_loss: 2.7365 - val_rmse: 1.3761\n",
      "Epoch 468/1500\n",
      "22470/22470 [==============================] - 17s 770us/sample - loss: 2.5642 - rmse: 1.5212 - val_loss: 2.7061 - val_rmse: 1.3656\n",
      "Epoch 469/1500\n",
      "22470/22470 [==============================] - 17s 771us/sample - loss: 2.5643 - rmse: 1.5198 - val_loss: 2.7243 - val_rmse: 1.3668\n",
      "Epoch 470/1500\n",
      "22470/22470 [==============================] - 17s 772us/sample - loss: 2.5539 - rmse: 1.5164 - val_loss: 2.8099 - val_rmse: 1.3865\n",
      "Epoch 471/1500\n",
      "22470/22470 [==============================] - 17s 771us/sample - loss: 2.5546 - rmse: 1.5161 - val_loss: 2.7403 - val_rmse: 1.3737\n",
      "Epoch 472/1500\n",
      "22470/22470 [==============================] - 17s 778us/sample - loss: 2.5675 - rmse: 1.5201 - val_loss: 2.6547 - val_rmse: 1.3547\n",
      "Epoch 473/1500\n",
      "22470/22470 [==============================] - 17s 771us/sample - loss: 2.5364 - rmse: 1.5117 - val_loss: 2.7000 - val_rmse: 1.3567\n",
      "Epoch 474/1500\n",
      "22470/22470 [==============================] - 17s 772us/sample - loss: 2.5429 - rmse: 1.5128 - val_loss: 2.7108 - val_rmse: 1.3649\n",
      "Epoch 475/1500\n",
      "22470/22470 [==============================] - 17s 775us/sample - loss: 2.5585 - rmse: 1.5172 - val_loss: 2.6696 - val_rmse: 1.3508\n",
      "Epoch 476/1500\n",
      "22470/22470 [==============================] - 17s 772us/sample - loss: 2.5256 - rmse: 1.5070 - val_loss: 2.6960 - val_rmse: 1.3626\n",
      "Epoch 477/1500\n",
      "22470/22470 [==============================] - 17s 774us/sample - loss: 2.5308 - rmse: 1.5087 - val_loss: 2.6535 - val_rmse: 1.3497\n",
      "Epoch 478/1500\n",
      "22470/22470 [==============================] - 17s 772us/sample - loss: 2.5220 - rmse: 1.5067 - val_loss: 2.6709 - val_rmse: 1.3612\n",
      "Epoch 479/1500\n",
      "22470/22470 [==============================] - 17s 774us/sample - loss: 2.5183 - rmse: 1.5054 - val_loss: 2.6935 - val_rmse: 1.3571\n",
      "Epoch 480/1500\n",
      "22470/22470 [==============================] - 17s 770us/sample - loss: 2.5342 - rmse: 1.5096 - val_loss: 2.7626 - val_rmse: 1.3843\n",
      "Epoch 481/1500\n",
      "22470/22470 [==============================] - 17s 771us/sample - loss: 2.5048 - rmse: 1.5013 - val_loss: 2.7358 - val_rmse: 1.3756\n",
      "Epoch 482/1500\n",
      "22470/22470 [==============================] - 17s 771us/sample - loss: 2.5226 - rmse: 1.5058 - val_loss: 2.7966 - val_rmse: 1.3913\n",
      "Epoch 483/1500\n",
      "22470/22470 [==============================] - 17s 773us/sample - loss: 2.4968 - rmse: 1.4988 - val_loss: 2.6473 - val_rmse: 1.3513\n",
      "Epoch 484/1500\n",
      "22470/22470 [==============================] - 17s 770us/sample - loss: 2.4916 - rmse: 1.4979 - val_loss: 2.6896 - val_rmse: 1.3520\n",
      "Epoch 485/1500\n",
      "22470/22470 [==============================] - 17s 771us/sample - loss: 2.5241 - rmse: 1.5055 - val_loss: 2.7843 - val_rmse: 1.3758\n",
      "Epoch 486/1500\n",
      "22470/22470 [==============================] - 17s 771us/sample - loss: 2.4880 - rmse: 1.4962 - val_loss: 2.7165 - val_rmse: 1.3617\n",
      "Epoch 487/1500\n",
      "22470/22470 [==============================] - 17s 771us/sample - loss: 2.4996 - rmse: 1.4992 - val_loss: 2.7290 - val_rmse: 1.3680\n",
      "Epoch 488/1500\n",
      "22470/22470 [==============================] - 17s 770us/sample - loss: 2.5043 - rmse: 1.5005 - val_loss: 2.7054 - val_rmse: 1.3542\n",
      "Epoch 489/1500\n",
      "22470/22470 [==============================] - 17s 775us/sample - loss: 2.4688 - rmse: 1.4902 - val_loss: 2.6823 - val_rmse: 1.3500\n",
      "Epoch 490/1500\n",
      "22470/22470 [==============================] - 17s 777us/sample - loss: 2.4873 - rmse: 1.4955 - val_loss: 2.5907 - val_rmse: 1.3321\n",
      "Epoch 491/1500\n",
      "22470/22470 [==============================] - 17s 772us/sample - loss: 2.4979 - rmse: 1.4983 - val_loss: 2.7195 - val_rmse: 1.3629\n",
      "Epoch 492/1500\n",
      "22470/22470 [==============================] - 17s 770us/sample - loss: 2.4885 - rmse: 1.4962 - val_loss: 2.6299 - val_rmse: 1.3419\n",
      "Epoch 493/1500\n",
      "22470/22470 [==============================] - 17s 770us/sample - loss: 2.4643 - rmse: 1.4881 - val_loss: 2.7561 - val_rmse: 1.3726\n",
      "Epoch 494/1500\n",
      "22470/22470 [==============================] - 17s 770us/sample - loss: 2.4794 - rmse: 1.4919 - val_loss: 2.7382 - val_rmse: 1.3623\n",
      "Epoch 495/1500\n",
      "22470/22470 [==============================] - 17s 768us/sample - loss: 2.4650 - rmse: 1.4881 - val_loss: 2.7391 - val_rmse: 1.3697\n",
      "Epoch 496/1500\n",
      "22470/22470 [==============================] - 17s 769us/sample - loss: 2.4658 - rmse: 1.4884 - val_loss: 2.6862 - val_rmse: 1.3541\n",
      "Epoch 497/1500\n",
      "22470/22470 [==============================] - 17s 774us/sample - loss: 2.4626 - rmse: 1.4873 - val_loss: 2.7705 - val_rmse: 1.3630\n",
      "Epoch 498/1500\n",
      "22470/22470 [==============================] - 17s 772us/sample - loss: 2.4748 - rmse: 1.4918 - val_loss: 2.7316 - val_rmse: 1.3612\n",
      "Epoch 499/1500\n",
      "22470/22470 [==============================] - 17s 772us/sample - loss: 2.4594 - rmse: 1.4867 - val_loss: 2.7094 - val_rmse: 1.3525\n",
      "Epoch 500/1500\n",
      "22470/22470 [==============================] - 17s 771us/sample - loss: 2.4576 - rmse: 1.4845 - val_loss: 2.7081 - val_rmse: 1.3559\n",
      "Epoch 501/1500\n",
      "22470/22470 [==============================] - 17s 770us/sample - loss: 2.4655 - rmse: 1.4867 - val_loss: 2.6874 - val_rmse: 1.3517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 502/1500\n",
      "22470/22470 [==============================] - 17s 747us/sample - loss: 2.4554 - rmse: 1.4846 - val_loss: 2.7306 - val_rmse: 1.3604\n",
      "Epoch 503/1500\n",
      "22470/22470 [==============================] - 17s 747us/sample - loss: 2.4600 - rmse: 1.4840 - val_loss: 2.7068 - val_rmse: 1.3646\n",
      "Epoch 504/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 2.4662 - rmse: 1.4860 - val_loss: 2.7106 - val_rmse: 1.3498\n",
      "Epoch 505/1500\n",
      "22470/22470 [==============================] - 17s 747us/sample - loss: 2.4327 - rmse: 1.4784 - val_loss: 2.6617 - val_rmse: 1.3411\n",
      "Epoch 506/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 2.4159 - rmse: 1.4739 - val_loss: 2.7598 - val_rmse: 1.3663\n",
      "Epoch 507/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 2.4464 - rmse: 1.4807 - val_loss: 2.7641 - val_rmse: 1.3645\n",
      "Epoch 508/1500\n",
      "22470/22470 [==============================] - 17s 747us/sample - loss: 2.4081 - rmse: 1.4712 - val_loss: 2.6818 - val_rmse: 1.3451\n",
      "Epoch 509/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 2.4370 - rmse: 1.4781 - val_loss: 2.6770 - val_rmse: 1.3524\n",
      "Epoch 510/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 2.4181 - rmse: 1.4737 - val_loss: 2.6763 - val_rmse: 1.3475\n",
      "Epoch 511/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 2.4558 - rmse: 1.4835 - val_loss: 2.6527 - val_rmse: 1.3498\n",
      "Epoch 512/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 2.4190 - rmse: 1.4735 - val_loss: 2.6751 - val_rmse: 1.3464\n",
      "Epoch 513/1500\n",
      "22470/22470 [==============================] - 17s 747us/sample - loss: 2.4236 - rmse: 1.4745 - val_loss: 2.7134 - val_rmse: 1.3534\n",
      "Epoch 514/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 2.4315 - rmse: 1.4764 - val_loss: 2.6816 - val_rmse: 1.3529\n",
      "Epoch 515/1500\n",
      "22470/22470 [==============================] - 17s 753us/sample - loss: 2.4180 - rmse: 1.4718 - val_loss: 2.6096 - val_rmse: 1.3291\n",
      "Epoch 516/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 2.4389 - rmse: 1.4780 - val_loss: 2.6494 - val_rmse: 1.3427\n",
      "Epoch 517/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 2.4171 - rmse: 1.4727 - val_loss: 2.6219 - val_rmse: 1.3295\n",
      "Epoch 518/1500\n",
      "22470/22470 [==============================] - 17s 756us/sample - loss: 2.4059 - rmse: 1.4691 - val_loss: 2.6076 - val_rmse: 1.3286\n",
      "Epoch 519/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 2.4172 - rmse: 1.4717 - val_loss: 2.6743 - val_rmse: 1.3667\n",
      "Epoch 520/1500\n",
      "22470/22470 [==============================] - 17s 754us/sample - loss: 2.4262 - rmse: 1.4746 - val_loss: 2.5827 - val_rmse: 1.3187\n",
      "Epoch 521/1500\n",
      "22470/22470 [==============================] - 17s 753us/sample - loss: 2.3912 - rmse: 1.4643 - val_loss: 2.6983 - val_rmse: 1.3462\n",
      "Epoch 522/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 2.4015 - rmse: 1.4679 - val_loss: 2.7862 - val_rmse: 1.3631\n",
      "Epoch 523/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 2.3818 - rmse: 1.4627 - val_loss: 2.6795 - val_rmse: 1.3500\n",
      "Epoch 524/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 2.3921 - rmse: 1.4643 - val_loss: 2.6411 - val_rmse: 1.3401\n",
      "Epoch 525/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 2.3779 - rmse: 1.4604 - val_loss: 2.6011 - val_rmse: 1.3257\n",
      "Epoch 526/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 2.3835 - rmse: 1.4623 - val_loss: 2.5995 - val_rmse: 1.3247\n",
      "Epoch 527/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 2.3912 - rmse: 1.4634 - val_loss: 2.5906 - val_rmse: 1.3262\n",
      "Epoch 528/1500\n",
      "22470/22470 [==============================] - 17s 758us/sample - loss: 2.4049 - rmse: 1.4670 - val_loss: 2.5648 - val_rmse: 1.3173\n",
      "Epoch 529/1500\n",
      "22470/22470 [==============================] - 17s 753us/sample - loss: 2.3801 - rmse: 1.4593 - val_loss: 2.5976 - val_rmse: 1.3253\n",
      "Epoch 530/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 2.3854 - rmse: 1.4621 - val_loss: 2.6061 - val_rmse: 1.3243\n",
      "Epoch 531/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 2.3692 - rmse: 1.4579 - val_loss: 2.6680 - val_rmse: 1.3443\n",
      "Epoch 532/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 2.3783 - rmse: 1.4583 - val_loss: 2.6307 - val_rmse: 1.3376\n",
      "Epoch 533/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 2.3931 - rmse: 1.4639 - val_loss: 2.5956 - val_rmse: 1.3271\n",
      "Epoch 534/1500\n",
      "22470/22470 [==============================] - 17s 753us/sample - loss: 2.3882 - rmse: 1.4621 - val_loss: 2.6610 - val_rmse: 1.3353\n",
      "Epoch 535/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 2.3775 - rmse: 1.4570 - val_loss: 2.6199 - val_rmse: 1.3330\n",
      "Epoch 536/1500\n",
      "22470/22470 [==============================] - 17s 753us/sample - loss: 2.3712 - rmse: 1.4572 - val_loss: 2.5556 - val_rmse: 1.3110\n",
      "Epoch 537/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 2.3796 - rmse: 1.4583 - val_loss: 2.5917 - val_rmse: 1.3191\n",
      "Epoch 538/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 2.3437 - rmse: 1.4483 - val_loss: 2.6899 - val_rmse: 1.3460\n",
      "Epoch 539/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 2.3406 - rmse: 1.4481 - val_loss: 2.6752 - val_rmse: 1.3449\n",
      "Epoch 540/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 2.3504 - rmse: 1.4502 - val_loss: 2.6240 - val_rmse: 1.3276\n",
      "Epoch 541/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 2.3478 - rmse: 1.4496 - val_loss: 2.6319 - val_rmse: 1.3308\n",
      "Epoch 542/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 2.3425 - rmse: 1.4486 - val_loss: 2.6314 - val_rmse: 1.3296\n",
      "Epoch 543/1500\n",
      "22470/22470 [==============================] - 17s 754us/sample - loss: 2.3497 - rmse: 1.4505 - val_loss: 2.6103 - val_rmse: 1.3229\n",
      "Epoch 544/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 2.3425 - rmse: 1.4482 - val_loss: 2.6601 - val_rmse: 1.3345\n",
      "Epoch 545/1500\n",
      "22470/22470 [==============================] - 17s 755us/sample - loss: 2.3508 - rmse: 1.4499 - val_loss: 2.5345 - val_rmse: 1.3072\n",
      "Epoch 546/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 2.3350 - rmse: 1.4446 - val_loss: 2.5830 - val_rmse: 1.3206\n",
      "Epoch 547/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 2.3228 - rmse: 1.4417 - val_loss: 2.7439 - val_rmse: 1.3633\n",
      "Epoch 548/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 2.3246 - rmse: 1.4426 - val_loss: 2.5533 - val_rmse: 1.3106\n",
      "Epoch 549/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 2.3105 - rmse: 1.4373 - val_loss: 2.5777 - val_rmse: 1.3133\n",
      "Epoch 550/1500\n",
      "22470/22470 [==============================] - 17s 753us/sample - loss: 2.3111 - rmse: 1.4384 - val_loss: 2.5469 - val_rmse: 1.3104\n",
      "Epoch 551/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 2.3059 - rmse: 1.4366 - val_loss: 2.5565 - val_rmse: 1.3118\n",
      "Epoch 552/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 2.3428 - rmse: 1.4461 - val_loss: 2.6003 - val_rmse: 1.3219\n",
      "Epoch 553/1500\n",
      "22470/22470 [==============================] - 17s 754us/sample - loss: 2.3191 - rmse: 1.4386 - val_loss: 2.5158 - val_rmse: 1.3010\n",
      "Epoch 554/1500\n",
      "22470/22470 [==============================] - 17s 756us/sample - loss: 2.3372 - rmse: 1.4454 - val_loss: 2.5899 - val_rmse: 1.3231\n",
      "Epoch 555/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 2.3362 - rmse: 1.4449 - val_loss: 2.5325 - val_rmse: 1.3079\n",
      "Epoch 556/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 2.3045 - rmse: 1.4359 - val_loss: 2.5640 - val_rmse: 1.3152\n",
      "Epoch 557/1500\n",
      "22470/22470 [==============================] - 17s 757us/sample - loss: 2.3143 - rmse: 1.4382 - val_loss: 2.5233 - val_rmse: 1.2992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 558/1500\n",
      "22470/22470 [==============================] - 17s 747us/sample - loss: 2.3211 - rmse: 1.4412 - val_loss: 2.5534 - val_rmse: 1.3055\n",
      "Epoch 559/1500\n",
      "22470/22470 [==============================] - 17s 746us/sample - loss: 2.3075 - rmse: 1.4366 - val_loss: 2.5729 - val_rmse: 1.3125\n",
      "Epoch 560/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 2.3213 - rmse: 1.4408 - val_loss: 2.4937 - val_rmse: 1.2966\n",
      "Epoch 561/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 2.2941 - rmse: 1.4328 - val_loss: 2.5436 - val_rmse: 1.3101\n",
      "Epoch 562/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 2.2690 - rmse: 1.4254 - val_loss: 2.5283 - val_rmse: 1.3028\n",
      "Epoch 563/1500\n",
      "22470/22470 [==============================] - 17s 747us/sample - loss: 2.2950 - rmse: 1.4327 - val_loss: 2.6930 - val_rmse: 1.3572\n",
      "Epoch 564/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 2.2814 - rmse: 1.4276 - val_loss: 2.5564 - val_rmse: 1.3176\n",
      "Epoch 565/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 2.2926 - rmse: 1.4319 - val_loss: 2.5629 - val_rmse: 1.3061\n",
      "Epoch 566/1500\n",
      "22470/22470 [==============================] - 17s 747us/sample - loss: 2.2806 - rmse: 1.4274 - val_loss: 2.5736 - val_rmse: 1.3097\n",
      "Epoch 567/1500\n",
      "22470/22470 [==============================] - 17s 746us/sample - loss: 2.2681 - rmse: 1.4244 - val_loss: 2.5830 - val_rmse: 1.3147\n",
      "Epoch 568/1500\n",
      "22470/22470 [==============================] - 17s 747us/sample - loss: 2.2782 - rmse: 1.4264 - val_loss: 2.5346 - val_rmse: 1.3045\n",
      "Epoch 569/1500\n",
      "22470/22470 [==============================] - 17s 747us/sample - loss: 2.2699 - rmse: 1.4232 - val_loss: 2.5377 - val_rmse: 1.3060\n",
      "Epoch 570/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 2.2777 - rmse: 1.4260 - val_loss: 2.5494 - val_rmse: 1.3053\n",
      "Epoch 571/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 2.2791 - rmse: 1.4289 - val_loss: 2.5566 - val_rmse: 1.3052\n",
      "Epoch 572/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 2.2544 - rmse: 1.4204 - val_loss: 2.5695 - val_rmse: 1.3051\n",
      "Epoch 573/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 2.2588 - rmse: 1.4204 - val_loss: 2.5563 - val_rmse: 1.3028\n",
      "Epoch 574/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 2.2616 - rmse: 1.4217 - val_loss: 2.5565 - val_rmse: 1.3073\n",
      "Epoch 575/1500\n",
      "22470/22470 [==============================] - 17s 755us/sample - loss: 2.2537 - rmse: 1.4189 - val_loss: 2.5093 - val_rmse: 1.2956\n",
      "Epoch 576/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 2.2740 - rmse: 1.4255 - val_loss: 2.5927 - val_rmse: 1.3204\n",
      "Epoch 577/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 2.2517 - rmse: 1.4181 - val_loss: 2.5334 - val_rmse: 1.3101\n",
      "Epoch 578/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 2.2685 - rmse: 1.4230 - val_loss: 2.5734 - val_rmse: 1.3087\n",
      "Epoch 579/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 2.2502 - rmse: 1.4179 - val_loss: 2.6585 - val_rmse: 1.3270\n",
      "Epoch 580/1500\n",
      "22470/22470 [==============================] - 17s 753us/sample - loss: 2.2745 - rmse: 1.4237 - val_loss: 2.5356 - val_rmse: 1.2953\n",
      "Epoch 581/1500\n",
      "22470/22470 [==============================] - 17s 753us/sample - loss: 2.2590 - rmse: 1.4193 - val_loss: 2.5815 - val_rmse: 1.3092\n",
      "Epoch 582/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 2.2509 - rmse: 1.4176 - val_loss: 2.6022 - val_rmse: 1.3167\n",
      "Epoch 583/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 2.2500 - rmse: 1.4174 - val_loss: 2.5691 - val_rmse: 1.3031\n",
      "Epoch 584/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 2.2325 - rmse: 1.4129 - val_loss: 2.5543 - val_rmse: 1.2966\n",
      "Epoch 585/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 2.2526 - rmse: 1.4166 - val_loss: 2.6266 - val_rmse: 1.3121\n",
      "Epoch 586/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 2.2245 - rmse: 1.4088 - val_loss: 2.5853 - val_rmse: 1.3122\n",
      "Epoch 587/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 2.2391 - rmse: 1.4127 - val_loss: 2.5365 - val_rmse: 1.3027\n",
      "Epoch 588/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 2.2136 - rmse: 1.4057 - val_loss: 2.5365 - val_rmse: 1.2975\n",
      "Epoch 589/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 2.2273 - rmse: 1.4110 - val_loss: 2.5848 - val_rmse: 1.3092\n",
      "Epoch 590/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 2.2224 - rmse: 1.4084 - val_loss: 2.5263 - val_rmse: 1.2974\n",
      "Epoch 591/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 2.2326 - rmse: 1.4111 - val_loss: 2.5297 - val_rmse: 1.3048\n",
      "Epoch 592/1500\n",
      "22470/22470 [==============================] - 17s 755us/sample - loss: 2.2340 - rmse: 1.4120 - val_loss: 2.4700 - val_rmse: 1.2817\n",
      "Epoch 593/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 2.2297 - rmse: 1.4109 - val_loss: 2.6235 - val_rmse: 1.3135\n",
      "Epoch 594/1500\n",
      "22470/22470 [==============================] - 17s 753us/sample - loss: 2.2133 - rmse: 1.4056 - val_loss: 2.5663 - val_rmse: 1.3033\n",
      "Epoch 595/1500\n",
      "22470/22470 [==============================] - 17s 747us/sample - loss: 2.2331 - rmse: 1.4097 - val_loss: 2.5390 - val_rmse: 1.2991\n",
      "Epoch 596/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 2.2341 - rmse: 1.4101 - val_loss: 2.5740 - val_rmse: 1.3114\n",
      "Epoch 597/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 2.2091 - rmse: 1.4045 - val_loss: 2.5010 - val_rmse: 1.2864\n",
      "Epoch 598/1500\n",
      "22470/22470 [==============================] - 17s 753us/sample - loss: 2.2301 - rmse: 1.4094 - val_loss: 2.5642 - val_rmse: 1.2974\n",
      "Epoch 599/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 2.2197 - rmse: 1.4063 - val_loss: 2.5360 - val_rmse: 1.2902\n",
      "Epoch 600/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 2.2149 - rmse: 1.4049 - val_loss: 2.5681 - val_rmse: 1.2966\n",
      "Epoch 601/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 2.2090 - rmse: 1.4034 - val_loss: 2.4778 - val_rmse: 1.2819\n",
      "Epoch 602/1500\n",
      "22470/22470 [==============================] - 17s 757us/sample - loss: 2.2094 - rmse: 1.4020 - val_loss: 2.4751 - val_rmse: 1.2796\n",
      "Epoch 603/1500\n",
      "22470/22470 [==============================] - 17s 754us/sample - loss: 2.2064 - rmse: 1.4010 - val_loss: 2.5310 - val_rmse: 1.2916\n",
      "Epoch 604/1500\n",
      "22470/22470 [==============================] - 17s 754us/sample - loss: 2.2156 - rmse: 1.4049 - val_loss: 2.5274 - val_rmse: 1.2891\n",
      "Epoch 605/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 2.2147 - rmse: 1.4036 - val_loss: 2.5960 - val_rmse: 1.3300\n",
      "Epoch 606/1500\n",
      "22470/22470 [==============================] - 17s 754us/sample - loss: 2.1828 - rmse: 1.3956 - val_loss: 2.5479 - val_rmse: 1.3018\n",
      "Epoch 607/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 2.2018 - rmse: 1.3995 - val_loss: 2.5036 - val_rmse: 1.2891\n",
      "Epoch 608/1500\n",
      "22470/22470 [==============================] - 17s 756us/sample - loss: 2.2030 - rmse: 1.4002 - val_loss: 2.4605 - val_rmse: 1.2723\n",
      "Epoch 609/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 2.1946 - rmse: 1.3979 - val_loss: 2.5484 - val_rmse: 1.2961\n",
      "Epoch 610/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 2.1951 - rmse: 1.3975 - val_loss: 2.6210 - val_rmse: 1.3216\n",
      "Epoch 611/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 2.1834 - rmse: 1.3954 - val_loss: 2.5493 - val_rmse: 1.2937\n",
      "Epoch 612/1500\n",
      "22470/22470 [==============================] - 17s 755us/sample - loss: 2.1806 - rmse: 1.3944 - val_loss: 2.4102 - val_rmse: 1.2706\n",
      "Epoch 613/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 2.1747 - rmse: 1.3926 - val_loss: 2.4548 - val_rmse: 1.2716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 614/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 2.1959 - rmse: 1.3979 - val_loss: 2.5262 - val_rmse: 1.2985\n",
      "Epoch 615/1500\n",
      "22470/22470 [==============================] - 17s 746us/sample - loss: 2.1841 - rmse: 1.3947 - val_loss: 2.5728 - val_rmse: 1.3049\n",
      "Epoch 616/1500\n",
      "22470/22470 [==============================] - 17s 746us/sample - loss: 2.1929 - rmse: 1.3959 - val_loss: 2.5308 - val_rmse: 1.2883\n",
      "Epoch 617/1500\n",
      "22470/22470 [==============================] - 17s 746us/sample - loss: 2.1816 - rmse: 1.3931 - val_loss: 2.4921 - val_rmse: 1.2772\n",
      "Epoch 618/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 2.1671 - rmse: 1.3885 - val_loss: 2.4929 - val_rmse: 1.2805\n",
      "Epoch 619/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 2.1636 - rmse: 1.3881 - val_loss: 2.4692 - val_rmse: 1.2752\n",
      "Epoch 620/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 2.1821 - rmse: 1.3934 - val_loss: 2.5847 - val_rmse: 1.3074\n",
      "Epoch 621/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 2.1601 - rmse: 1.3874 - val_loss: 2.4870 - val_rmse: 1.2821\n",
      "Epoch 622/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 2.1736 - rmse: 1.3902 - val_loss: 2.5032 - val_rmse: 1.2840\n",
      "Epoch 623/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 2.1579 - rmse: 1.3855 - val_loss: 2.4622 - val_rmse: 1.2725\n",
      "Epoch 624/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 2.1304 - rmse: 1.3779 - val_loss: 2.6109 - val_rmse: 1.3053\n",
      "Epoch 625/1500\n",
      "22470/22470 [==============================] - 17s 755us/sample - loss: 2.1640 - rmse: 1.3884 - val_loss: 2.4368 - val_rmse: 1.2646\n",
      "Epoch 626/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 2.1706 - rmse: 1.3885 - val_loss: 2.5096 - val_rmse: 1.2816\n",
      "Epoch 627/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 2.1608 - rmse: 1.3866 - val_loss: 2.5275 - val_rmse: 1.2867\n",
      "Epoch 628/1500\n",
      "22470/22470 [==============================] - 17s 756us/sample - loss: 2.1382 - rmse: 1.3793 - val_loss: 2.3883 - val_rmse: 1.2554\n",
      "Epoch 629/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 2.1684 - rmse: 1.3885 - val_loss: 2.5258 - val_rmse: 1.2918\n",
      "Epoch 630/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 2.1439 - rmse: 1.3818 - val_loss: 2.4771 - val_rmse: 1.2737\n",
      "Epoch 631/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 2.1371 - rmse: 1.3796 - val_loss: 2.5225 - val_rmse: 1.2863\n",
      "Epoch 632/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 2.1671 - rmse: 1.3878 - val_loss: 2.5165 - val_rmse: 1.2844\n",
      "Epoch 633/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 2.1377 - rmse: 1.3789 - val_loss: 2.4681 - val_rmse: 1.2750\n",
      "Epoch 634/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 2.1691 - rmse: 1.3874 - val_loss: 2.4973 - val_rmse: 1.2804\n",
      "Epoch 635/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 2.1460 - rmse: 1.3810 - val_loss: 2.4462 - val_rmse: 1.2680\n",
      "Epoch 636/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 2.1523 - rmse: 1.3840 - val_loss: 2.4829 - val_rmse: 1.2744\n",
      "Epoch 637/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 2.1547 - rmse: 1.3837 - val_loss: 2.4752 - val_rmse: 1.2744\n",
      "Epoch 638/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 2.1270 - rmse: 1.3758 - val_loss: 2.4266 - val_rmse: 1.2692\n",
      "Epoch 639/1500\n",
      "22470/22470 [==============================] - 17s 754us/sample - loss: 2.1323 - rmse: 1.3762 - val_loss: 2.4479 - val_rmse: 1.2679\n",
      "Epoch 640/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 2.1354 - rmse: 1.3783 - val_loss: 2.4421 - val_rmse: 1.2671\n",
      "Epoch 641/1500\n",
      "22470/22470 [==============================] - 17s 753us/sample - loss: 2.1354 - rmse: 1.3772 - val_loss: 2.4640 - val_rmse: 1.2732\n",
      "Epoch 642/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 2.1250 - rmse: 1.3740 - val_loss: 2.4412 - val_rmse: 1.2666\n",
      "Epoch 643/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 2.1370 - rmse: 1.3771 - val_loss: 2.4936 - val_rmse: 1.2780\n",
      "Epoch 644/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 2.1342 - rmse: 1.3767 - val_loss: 2.6146 - val_rmse: 1.2965\n",
      "Epoch 645/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 2.1132 - rmse: 1.3711 - val_loss: 2.4529 - val_rmse: 1.2681\n",
      "Epoch 646/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 2.1211 - rmse: 1.3729 - val_loss: 2.5263 - val_rmse: 1.2863\n",
      "Epoch 647/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 2.1210 - rmse: 1.3725 - val_loss: 2.4283 - val_rmse: 1.2650\n",
      "Epoch 648/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 2.1241 - rmse: 1.3723 - val_loss: 2.4217 - val_rmse: 1.2584\n",
      "Epoch 649/1500\n",
      "22470/22470 [==============================] - 17s 757us/sample - loss: 2.1109 - rmse: 1.3698 - val_loss: 2.3999 - val_rmse: 1.2558\n",
      "Epoch 650/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 2.1177 - rmse: 1.3712 - val_loss: 2.5923 - val_rmse: 1.2976\n",
      "Epoch 651/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 2.1161 - rmse: 1.3708 - val_loss: 2.5171 - val_rmse: 1.2831\n",
      "Epoch 652/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 2.1077 - rmse: 1.3673 - val_loss: 2.5144 - val_rmse: 1.2822\n",
      "Epoch 653/1500\n",
      "22470/22470 [==============================] - 17s 755us/sample - loss: 2.1200 - rmse: 1.3716 - val_loss: 2.4665 - val_rmse: 1.2774\n",
      "Epoch 654/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 2.1195 - rmse: 1.3715 - val_loss: 2.4051 - val_rmse: 1.2607\n",
      "Epoch 655/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 2.1096 - rmse: 1.3697 - val_loss: 2.4786 - val_rmse: 1.2735\n",
      "Epoch 656/1500\n",
      "22470/22470 [==============================] - 17s 760us/sample - loss: 2.1045 - rmse: 1.3680 - val_loss: 2.4856 - val_rmse: 1.2785\n",
      "Epoch 657/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 2.1132 - rmse: 1.3688 - val_loss: 2.4706 - val_rmse: 1.2606\n",
      "Epoch 658/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 2.0960 - rmse: 1.3639 - val_loss: 2.5029 - val_rmse: 1.2729\n",
      "Epoch 659/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 2.0839 - rmse: 1.3609 - val_loss: 2.4890 - val_rmse: 1.2678\n",
      "Epoch 660/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 2.0999 - rmse: 1.3657 - val_loss: 2.5270 - val_rmse: 1.2878\n",
      "Epoch 661/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 2.0965 - rmse: 1.3639 - val_loss: 2.5761 - val_rmse: 1.2954\n",
      "Epoch 662/1500\n",
      "22470/22470 [==============================] - 17s 755us/sample - loss: 2.0782 - rmse: 1.3569 - val_loss: 2.4177 - val_rmse: 1.2540\n",
      "Epoch 663/1500\n",
      "22470/22470 [==============================] - 17s 755us/sample - loss: 2.0971 - rmse: 1.3632 - val_loss: 2.4482 - val_rmse: 1.2602\n",
      "Epoch 664/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 2.0961 - rmse: 1.3632 - val_loss: 2.5670 - val_rmse: 1.2883\n",
      "Epoch 665/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 2.0936 - rmse: 1.3618 - val_loss: 2.4656 - val_rmse: 1.2679\n",
      "Epoch 666/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 2.0763 - rmse: 1.3567 - val_loss: 2.4825 - val_rmse: 1.2659\n",
      "Epoch 667/1500\n",
      "22470/22470 [==============================] - 17s 755us/sample - loss: 2.0755 - rmse: 1.3570 - val_loss: 2.3966 - val_rmse: 1.2457\n",
      "Epoch 668/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 2.0887 - rmse: 1.3612 - val_loss: 2.4306 - val_rmse: 1.2648\n",
      "Epoch 669/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 2.0743 - rmse: 1.3579 - val_loss: 2.5741 - val_rmse: 1.3033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 670/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 2.0751 - rmse: 1.3566 - val_loss: 2.4748 - val_rmse: 1.2669\n",
      "Epoch 671/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 2.0650 - rmse: 1.3548 - val_loss: 2.4310 - val_rmse: 1.2551\n",
      "Epoch 672/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 2.0854 - rmse: 1.3578 - val_loss: 2.4204 - val_rmse: 1.2534\n",
      "Epoch 673/1500\n",
      "22470/22470 [==============================] - 17s 746us/sample - loss: 2.0680 - rmse: 1.3540 - val_loss: 2.3989 - val_rmse: 1.2497\n",
      "Epoch 674/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 2.0650 - rmse: 1.3539 - val_loss: 2.4365 - val_rmse: 1.2602\n",
      "Epoch 675/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 2.0593 - rmse: 1.3511 - val_loss: 2.3635 - val_rmse: 1.2465\n",
      "Epoch 676/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 2.0529 - rmse: 1.3493 - val_loss: 2.5192 - val_rmse: 1.2874\n",
      "Epoch 677/1500\n",
      "22470/22470 [==============================] - 17s 747us/sample - loss: 2.0649 - rmse: 1.3533 - val_loss: 2.3742 - val_rmse: 1.2503\n",
      "Epoch 678/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 2.0814 - rmse: 1.3585 - val_loss: 2.4105 - val_rmse: 1.2484\n",
      "Epoch 679/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 2.0659 - rmse: 1.3539 - val_loss: 2.4607 - val_rmse: 1.2573\n",
      "Epoch 680/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 2.0777 - rmse: 1.3562 - val_loss: 2.4119 - val_rmse: 1.2517\n",
      "Epoch 681/1500\n",
      "22470/22470 [==============================] - 17s 746us/sample - loss: 2.0727 - rmse: 1.3551 - val_loss: 2.4285 - val_rmse: 1.2527\n",
      "Epoch 682/1500\n",
      "22470/22470 [==============================] - 17s 747us/sample - loss: 2.0644 - rmse: 1.3530 - val_loss: 2.4313 - val_rmse: 1.2557\n",
      "Epoch 683/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 2.0727 - rmse: 1.3543 - val_loss: 2.4309 - val_rmse: 1.2587\n",
      "Epoch 684/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 2.0719 - rmse: 1.3535 - val_loss: 2.4666 - val_rmse: 1.2647\n",
      "Epoch 685/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 2.0473 - rmse: 1.3475 - val_loss: 2.4549 - val_rmse: 1.2638\n",
      "Epoch 686/1500\n",
      "22470/22470 [==============================] - 17s 747us/sample - loss: 2.0471 - rmse: 1.3461 - val_loss: 2.4389 - val_rmse: 1.2602\n",
      "Epoch 687/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 2.0543 - rmse: 1.3491 - val_loss: 2.4227 - val_rmse: 1.2517\n",
      "Epoch 688/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 2.0523 - rmse: 1.3484 - val_loss: 2.4975 - val_rmse: 1.2658\n",
      "Epoch 689/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 2.0436 - rmse: 1.3446 - val_loss: 2.4302 - val_rmse: 1.2528\n",
      "Epoch 690/1500\n",
      "22470/22470 [==============================] - 17s 757us/sample - loss: 2.0213 - rmse: 1.3393 - val_loss: 2.3940 - val_rmse: 1.2425\n",
      "Epoch 691/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 2.0400 - rmse: 1.3448 - val_loss: 2.4068 - val_rmse: 1.2486\n",
      "Epoch 692/1500\n",
      "22470/22470 [==============================] - 17s 753us/sample - loss: 2.0457 - rmse: 1.3453 - val_loss: 2.4555 - val_rmse: 1.2556\n",
      "Epoch 693/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 2.0416 - rmse: 1.3443 - val_loss: 2.4639 - val_rmse: 1.2627\n",
      "Epoch 694/1500\n",
      "22470/22470 [==============================] - 17s 754us/sample - loss: 2.0515 - rmse: 1.3468 - val_loss: 2.3825 - val_rmse: 1.2383\n",
      "Epoch 695/1500\n",
      "22470/22470 [==============================] - 17s 753us/sample - loss: 2.0390 - rmse: 1.3444 - val_loss: 2.4947 - val_rmse: 1.2671\n",
      "Epoch 696/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 2.0255 - rmse: 1.3389 - val_loss: 2.4327 - val_rmse: 1.2484\n",
      "Epoch 697/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 2.0204 - rmse: 1.3373 - val_loss: 2.4291 - val_rmse: 1.2527\n",
      "Epoch 698/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 2.0432 - rmse: 1.3450 - val_loss: 2.5411 - val_rmse: 1.2765\n",
      "Epoch 699/1500\n",
      "22470/22470 [==============================] - 17s 753us/sample - loss: 2.0123 - rmse: 1.3354 - val_loss: 2.5454 - val_rmse: 1.2757\n",
      "Epoch 700/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 2.0390 - rmse: 1.3428 - val_loss: 2.4377 - val_rmse: 1.2563\n",
      "Epoch 701/1500\n",
      "22470/22470 [==============================] - 17s 755us/sample - loss: 2.0193 - rmse: 1.3375 - val_loss: 2.3600 - val_rmse: 1.2320\n",
      "Epoch 702/1500\n",
      "22470/22470 [==============================] - 17s 756us/sample - loss: 2.0397 - rmse: 1.3426 - val_loss: 2.3664 - val_rmse: 1.2353\n",
      "Epoch 703/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 2.0244 - rmse: 1.3394 - val_loss: 2.3940 - val_rmse: 1.2537\n",
      "Epoch 704/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 2.0398 - rmse: 1.3432 - val_loss: 2.3979 - val_rmse: 1.2392\n",
      "Epoch 705/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 2.0174 - rmse: 1.3372 - val_loss: 2.4729 - val_rmse: 1.2557\n",
      "Epoch 706/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 2.0330 - rmse: 1.3418 - val_loss: 2.3676 - val_rmse: 1.2335\n",
      "Epoch 707/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 2.0141 - rmse: 1.3341 - val_loss: 2.4951 - val_rmse: 1.2589\n",
      "Epoch 708/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 2.0357 - rmse: 1.3405 - val_loss: 2.4731 - val_rmse: 1.2555\n",
      "Epoch 709/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 2.0088 - rmse: 1.3345 - val_loss: 2.3869 - val_rmse: 1.2400\n",
      "Epoch 710/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 2.0143 - rmse: 1.3352 - val_loss: 2.4515 - val_rmse: 1.2493\n",
      "Epoch 711/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 2.0022 - rmse: 1.3316 - val_loss: 2.4757 - val_rmse: 1.2525\n",
      "Epoch 712/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 2.0109 - rmse: 1.3350 - val_loss: 2.5287 - val_rmse: 1.2751\n",
      "Epoch 713/1500\n",
      "22470/22470 [==============================] - 17s 754us/sample - loss: 2.0209 - rmse: 1.3360 - val_loss: 2.3698 - val_rmse: 1.2417\n",
      "Epoch 714/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 1.9901 - rmse: 1.3275 - val_loss: 2.4921 - val_rmse: 1.2581\n",
      "Epoch 715/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 2.0215 - rmse: 1.3368 - val_loss: 2.4686 - val_rmse: 1.2569\n",
      "Epoch 716/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 2.0240 - rmse: 1.3380 - val_loss: 2.4266 - val_rmse: 1.2402\n",
      "Epoch 717/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 1.9941 - rmse: 1.3286 - val_loss: 2.3932 - val_rmse: 1.2489\n",
      "Epoch 718/1500\n",
      "22470/22470 [==============================] - 17s 753us/sample - loss: 2.0008 - rmse: 1.3296 - val_loss: 2.3668 - val_rmse: 1.2363\n",
      "Epoch 719/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 2.0299 - rmse: 1.3392 - val_loss: 2.4031 - val_rmse: 1.2394\n",
      "Epoch 720/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 1.9991 - rmse: 1.3293 - val_loss: 2.5085 - val_rmse: 1.2617\n",
      "Epoch 721/1500\n",
      "22470/22470 [==============================] - 17s 754us/sample - loss: 1.9968 - rmse: 1.3301 - val_loss: 2.3325 - val_rmse: 1.2244\n",
      "Epoch 722/1500\n",
      "22470/22470 [==============================] - 17s 759us/sample - loss: 1.9973 - rmse: 1.3287 - val_loss: 2.2891 - val_rmse: 1.2199\n",
      "Epoch 723/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 1.9911 - rmse: 1.3267 - val_loss: 2.5298 - val_rmse: 1.2811\n",
      "Epoch 724/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 2.0059 - rmse: 1.3328 - val_loss: 2.4872 - val_rmse: 1.2552\n",
      "Epoch 725/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 1.9996 - rmse: 1.3291 - val_loss: 2.5496 - val_rmse: 1.2747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 726/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 1.9915 - rmse: 1.3267 - val_loss: 2.4493 - val_rmse: 1.2570\n",
      "Epoch 727/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 1.9935 - rmse: 1.3260 - val_loss: 2.4688 - val_rmse: 1.2535\n",
      "Epoch 728/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 1.9985 - rmse: 1.3287 - val_loss: 2.3390 - val_rmse: 1.2288\n",
      "Epoch 729/1500\n",
      "22470/22470 [==============================] - 17s 746us/sample - loss: 1.9676 - rmse: 1.3187 - val_loss: 2.3321 - val_rmse: 1.2251\n",
      "Epoch 730/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 1.9843 - rmse: 1.3248 - val_loss: 2.4946 - val_rmse: 1.2581\n",
      "Epoch 731/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 1.9881 - rmse: 1.3242 - val_loss: 2.4151 - val_rmse: 1.2402\n",
      "Epoch 732/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 1.9753 - rmse: 1.3215 - val_loss: 2.3679 - val_rmse: 1.2296\n",
      "Epoch 733/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 1.9863 - rmse: 1.3260 - val_loss: 2.3495 - val_rmse: 1.2280\n",
      "Epoch 734/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 1.9730 - rmse: 1.3193 - val_loss: 2.4759 - val_rmse: 1.2557\n",
      "Epoch 735/1500\n",
      "22470/22470 [==============================] - 17s 747us/sample - loss: 1.9876 - rmse: 1.3224 - val_loss: 2.4869 - val_rmse: 1.2606\n",
      "Epoch 736/1500\n",
      "22470/22470 [==============================] - 17s 747us/sample - loss: 1.9781 - rmse: 1.3234 - val_loss: 2.4534 - val_rmse: 1.2534\n",
      "Epoch 737/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 1.9913 - rmse: 1.3267 - val_loss: 2.4260 - val_rmse: 1.2504\n",
      "Epoch 738/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 1.9812 - rmse: 1.3229 - val_loss: 2.3789 - val_rmse: 1.2332\n",
      "Epoch 739/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 1.9761 - rmse: 1.3210 - val_loss: 2.4760 - val_rmse: 1.2523\n",
      "Epoch 740/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 1.9902 - rmse: 1.3240 - val_loss: 2.3480 - val_rmse: 1.2204\n",
      "Epoch 741/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 2.0044 - rmse: 1.3287 - val_loss: 2.3917 - val_rmse: 1.2310\n",
      "Epoch 742/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 1.9945 - rmse: 1.3268 - val_loss: 2.4358 - val_rmse: 1.2404\n",
      "Epoch 743/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 1.9654 - rmse: 1.3171 - val_loss: 2.4027 - val_rmse: 1.2344\n",
      "Epoch 744/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 1.9568 - rmse: 1.3147 - val_loss: 2.4023 - val_rmse: 1.2467\n",
      "Epoch 745/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 1.9601 - rmse: 1.3166 - val_loss: 2.3868 - val_rmse: 1.2445\n",
      "Epoch 746/1500\n",
      "22470/22470 [==============================] - 17s 755us/sample - loss: 1.9582 - rmse: 1.3147 - val_loss: 2.4581 - val_rmse: 1.2502\n",
      "Epoch 747/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 1.9481 - rmse: 1.3141 - val_loss: 2.4222 - val_rmse: 1.2427\n",
      "Epoch 748/1500\n",
      "22470/22470 [==============================] - 17s 753us/sample - loss: 1.9473 - rmse: 1.3127 - val_loss: 2.4107 - val_rmse: 1.2361\n",
      "Epoch 749/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 1.9412 - rmse: 1.3095 - val_loss: 2.3833 - val_rmse: 1.2337\n",
      "Epoch 750/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 1.9587 - rmse: 1.3135 - val_loss: 2.4460 - val_rmse: 1.2608\n",
      "Epoch 751/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 1.9673 - rmse: 1.3170 - val_loss: 2.3787 - val_rmse: 1.2308\n",
      "Epoch 752/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 1.9579 - rmse: 1.3144 - val_loss: 2.3749 - val_rmse: 1.2331\n",
      "Epoch 753/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 1.9678 - rmse: 1.3162 - val_loss: 2.4078 - val_rmse: 1.2395\n",
      "Epoch 754/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 1.9429 - rmse: 1.3098 - val_loss: 2.4299 - val_rmse: 1.2434\n",
      "Epoch 755/1500\n",
      "22470/22470 [==============================] - 17s 757us/sample - loss: 1.9350 - rmse: 1.3070 - val_loss: 2.3159 - val_rmse: 1.2137\n",
      "Epoch 756/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 1.9348 - rmse: 1.3054 - val_loss: 2.4219 - val_rmse: 1.2356\n",
      "Epoch 757/1500\n",
      "22470/22470 [==============================] - 17s 753us/sample - loss: 1.9504 - rmse: 1.3122 - val_loss: 2.3727 - val_rmse: 1.2246\n",
      "Epoch 758/1500\n",
      "22470/22470 [==============================] - 17s 753us/sample - loss: 1.9608 - rmse: 1.3161 - val_loss: 2.3594 - val_rmse: 1.2252\n",
      "Epoch 759/1500\n",
      "22470/22470 [==============================] - 17s 753us/sample - loss: 1.9557 - rmse: 1.3133 - val_loss: 2.6102 - val_rmse: 1.2874\n",
      "Epoch 760/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 1.9416 - rmse: 1.3096 - val_loss: 2.4080 - val_rmse: 1.2367\n",
      "Epoch 761/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 1.9519 - rmse: 1.3118 - val_loss: 2.3808 - val_rmse: 1.2387\n",
      "Epoch 762/1500\n",
      "22470/22470 [==============================] - 17s 755us/sample - loss: 1.9536 - rmse: 1.3130 - val_loss: 2.3046 - val_rmse: 1.2112\n",
      "Epoch 763/1500\n",
      "22470/22470 [==============================] - 17s 763us/sample - loss: 1.9446 - rmse: 1.3085 - val_loss: 2.3911 - val_rmse: 1.2451\n",
      "Epoch 764/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 1.9285 - rmse: 1.3049 - val_loss: 2.3542 - val_rmse: 1.2235\n",
      "Epoch 765/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 1.9242 - rmse: 1.3028 - val_loss: 2.3093 - val_rmse: 1.2119\n",
      "Epoch 766/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 1.9445 - rmse: 1.3082 - val_loss: 2.4020 - val_rmse: 1.2319\n",
      "Epoch 767/1500\n",
      "22470/22470 [==============================] - 17s 754us/sample - loss: 1.9368 - rmse: 1.3059 - val_loss: 2.3725 - val_rmse: 1.2260\n",
      "Epoch 768/1500\n",
      "22470/22470 [==============================] - 18s 807us/sample - loss: 1.9426 - rmse: 1.3098 - val_loss: 2.3558 - val_rmse: 1.2235\n",
      "Epoch 769/1500\n",
      "22470/22470 [==============================] - 19s 835us/sample - loss: 1.9421 - rmse: 1.3071 - val_loss: 2.3245 - val_rmse: 1.2130\n",
      "Epoch 770/1500\n",
      "22470/22470 [==============================] - 18s 805us/sample - loss: 1.9239 - rmse: 1.3024 - val_loss: 2.3561 - val_rmse: 1.2180\n",
      "Epoch 771/1500\n",
      "22470/22470 [==============================] - 18s 805us/sample - loss: 1.9227 - rmse: 1.3025 - val_loss: 2.3901 - val_rmse: 1.2282\n",
      "Epoch 772/1500\n",
      "22470/22470 [==============================] - 18s 803us/sample - loss: 1.9396 - rmse: 1.3064 - val_loss: 2.3459 - val_rmse: 1.2185\n",
      "Epoch 773/1500\n",
      "22470/22470 [==============================] - 18s 803us/sample - loss: 1.9205 - rmse: 1.3016 - val_loss: 2.3692 - val_rmse: 1.2223\n",
      "Epoch 774/1500\n",
      "22470/22470 [==============================] - 18s 803us/sample - loss: 1.9357 - rmse: 1.3065 - val_loss: 2.3883 - val_rmse: 1.2352\n",
      "Epoch 775/1500\n",
      "22470/22470 [==============================] - 18s 805us/sample - loss: 1.9208 - rmse: 1.3005 - val_loss: 2.3355 - val_rmse: 1.2171\n",
      "Epoch 776/1500\n",
      "22470/22470 [==============================] - 18s 803us/sample - loss: 1.9235 - rmse: 1.3018 - val_loss: 2.4149 - val_rmse: 1.2282\n",
      "Epoch 777/1500\n",
      "22470/22470 [==============================] - 19s 854us/sample - loss: 1.9196 - rmse: 1.3013 - val_loss: 2.3544 - val_rmse: 1.2144\n",
      "Epoch 778/1500\n",
      "22470/22470 [==============================] - 18s 822us/sample - loss: 1.9038 - rmse: 1.2953 - val_loss: 2.3607 - val_rmse: 1.2160\n",
      "Epoch 779/1500\n",
      "22470/22470 [==============================] - 19s 853us/sample - loss: 1.9206 - rmse: 1.3002 - val_loss: 2.3359 - val_rmse: 1.2083\n",
      "Epoch 780/1500\n",
      "22470/22470 [==============================] - 19s 835us/sample - loss: 1.9218 - rmse: 1.3008 - val_loss: 2.3954 - val_rmse: 1.2254\n",
      "Epoch 781/1500\n",
      "22470/22470 [==============================] - 18s 803us/sample - loss: 1.9355 - rmse: 1.3058 - val_loss: 2.3921 - val_rmse: 1.2255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 782/1500\n",
      "22470/22470 [==============================] - 17s 764us/sample - loss: 1.9113 - rmse: 1.2987 - val_loss: 2.3092 - val_rmse: 1.2058\n",
      "Epoch 783/1500\n",
      "22470/22470 [==============================] - 17s 759us/sample - loss: 1.9075 - rmse: 1.2960 - val_loss: 2.4257 - val_rmse: 1.2353\n",
      "Epoch 784/1500\n",
      "22470/22470 [==============================] - 17s 759us/sample - loss: 1.9236 - rmse: 1.3011 - val_loss: 2.3991 - val_rmse: 1.2329\n",
      "Epoch 785/1500\n",
      "22470/22470 [==============================] - 17s 758us/sample - loss: 1.9177 - rmse: 1.2998 - val_loss: 2.3278 - val_rmse: 1.2091\n",
      "Epoch 786/1500\n",
      "22470/22470 [==============================] - 17s 758us/sample - loss: 1.9091 - rmse: 1.2950 - val_loss: 2.4369 - val_rmse: 1.2339\n",
      "Epoch 787/1500\n",
      "22470/22470 [==============================] - 17s 756us/sample - loss: 1.9187 - rmse: 1.2999 - val_loss: 2.4313 - val_rmse: 1.2385\n",
      "Epoch 788/1500\n",
      "22470/22470 [==============================] - 17s 758us/sample - loss: 1.9098 - rmse: 1.2965 - val_loss: 2.3317 - val_rmse: 1.2115\n",
      "Epoch 789/1500\n",
      "22470/22470 [==============================] - 17s 757us/sample - loss: 1.9155 - rmse: 1.2993 - val_loss: 2.3785 - val_rmse: 1.2155\n",
      "Epoch 790/1500\n",
      "22470/22470 [==============================] - 18s 815us/sample - loss: 1.8976 - rmse: 1.2933 - val_loss: 2.3309 - val_rmse: 1.2118\n",
      "Epoch 791/1500\n",
      "22470/22470 [==============================] - 18s 819us/sample - loss: 1.9190 - rmse: 1.2975 - val_loss: 2.3224 - val_rmse: 1.2088\n",
      "Epoch 792/1500\n",
      "22470/22470 [==============================] - 18s 809us/sample - loss: 1.9094 - rmse: 1.2966 - val_loss: 2.3147 - val_rmse: 1.2049\n",
      "Epoch 793/1500\n",
      "22470/22470 [==============================] - 18s 799us/sample - loss: 1.9040 - rmse: 1.2957 - val_loss: 2.3841 - val_rmse: 1.2269\n",
      "Epoch 794/1500\n",
      "22470/22470 [==============================] - 18s 801us/sample - loss: 1.9016 - rmse: 1.2933 - val_loss: 2.3808 - val_rmse: 1.2215\n",
      "Epoch 795/1500\n",
      "22470/22470 [==============================] - 18s 799us/sample - loss: 1.8944 - rmse: 1.2927 - val_loss: 2.3171 - val_rmse: 1.2078\n",
      "Epoch 796/1500\n",
      "22470/22470 [==============================] - 18s 801us/sample - loss: 1.9125 - rmse: 1.2970 - val_loss: 2.3219 - val_rmse: 1.2107\n",
      "Epoch 797/1500\n",
      "22470/22470 [==============================] - 18s 804us/sample - loss: 1.8926 - rmse: 1.2913 - val_loss: 2.3121 - val_rmse: 1.2069\n",
      "Epoch 798/1500\n",
      "22470/22470 [==============================] - 18s 805us/sample - loss: 1.8879 - rmse: 1.2900 - val_loss: 2.2701 - val_rmse: 1.1992\n",
      "Epoch 799/1500\n",
      "22470/22470 [==============================] - 19s 847us/sample - loss: 1.8916 - rmse: 1.2900 - val_loss: 2.3029 - val_rmse: 1.2052\n",
      "Epoch 800/1500\n",
      "22470/22470 [==============================] - 19s 831us/sample - loss: 1.9048 - rmse: 1.2952 - val_loss: 2.3682 - val_rmse: 1.2215\n",
      "Epoch 801/1500\n",
      "22470/22470 [==============================] - 18s 804us/sample - loss: 1.8909 - rmse: 1.2903 - val_loss: 2.3382 - val_rmse: 1.2037\n",
      "Epoch 802/1500\n",
      "22470/22470 [==============================] - 18s 803us/sample - loss: 1.8966 - rmse: 1.2913 - val_loss: 2.3764 - val_rmse: 1.2227\n",
      "Epoch 803/1500\n",
      "22470/22470 [==============================] - 18s 802us/sample - loss: 1.9178 - rmse: 1.2966 - val_loss: 2.3701 - val_rmse: 1.2191\n",
      "Epoch 804/1500\n",
      "22470/22470 [==============================] - 18s 805us/sample - loss: 1.8852 - rmse: 1.2893 - val_loss: 2.3306 - val_rmse: 1.2122\n",
      "Epoch 805/1500\n",
      "22470/22470 [==============================] - 18s 802us/sample - loss: 1.8950 - rmse: 1.2906 - val_loss: 2.4748 - val_rmse: 1.2326\n",
      "Epoch 806/1500\n",
      "22470/22470 [==============================] - 18s 804us/sample - loss: 1.8756 - rmse: 1.2851 - val_loss: 2.2875 - val_rmse: 1.2006\n",
      "Epoch 807/1500\n",
      "22470/22470 [==============================] - 18s 802us/sample - loss: 1.8758 - rmse: 1.2862 - val_loss: 2.2764 - val_rmse: 1.2011\n",
      "Epoch 808/1500\n",
      "22470/22470 [==============================] - 18s 800us/sample - loss: 1.9001 - rmse: 1.2931 - val_loss: 2.6648 - val_rmse: 1.2924\n",
      "Epoch 809/1500\n",
      "22470/22470 [==============================] - 18s 804us/sample - loss: 1.8893 - rmse: 1.2888 - val_loss: 2.4381 - val_rmse: 1.2296\n",
      "Epoch 810/1500\n",
      "22470/22470 [==============================] - 18s 805us/sample - loss: 1.9094 - rmse: 1.2945 - val_loss: 2.3210 - val_rmse: 1.2031\n",
      "Epoch 811/1500\n",
      "22470/22470 [==============================] - 18s 803us/sample - loss: 1.8843 - rmse: 1.2886 - val_loss: 2.3983 - val_rmse: 1.2332\n",
      "Epoch 812/1500\n",
      "22470/22470 [==============================] - 18s 817us/sample - loss: 1.8940 - rmse: 1.2900 - val_loss: 2.3015 - val_rmse: 1.2000\n",
      "Epoch 813/1500\n",
      "22470/22470 [==============================] - 19s 854us/sample - loss: 1.9029 - rmse: 1.2925 - val_loss: 2.4786 - val_rmse: 1.2402\n",
      "Epoch 814/1500\n",
      "22470/22470 [==============================] - 18s 801us/sample - loss: 1.8837 - rmse: 1.2863 - val_loss: 2.3842 - val_rmse: 1.2149\n",
      "Epoch 815/1500\n",
      "22470/22470 [==============================] - 18s 803us/sample - loss: 1.8749 - rmse: 1.2833 - val_loss: 2.3928 - val_rmse: 1.2234\n",
      "Epoch 816/1500\n",
      "22470/22470 [==============================] - 18s 808us/sample - loss: 1.8675 - rmse: 1.2821 - val_loss: 2.3120 - val_rmse: 1.2008\n",
      "Epoch 817/1500\n",
      "22470/22470 [==============================] - 18s 804us/sample - loss: 1.8904 - rmse: 1.2885 - val_loss: 2.3645 - val_rmse: 1.2203\n",
      "Epoch 818/1500\n",
      "22470/22470 [==============================] - 18s 805us/sample - loss: 1.8742 - rmse: 1.2847 - val_loss: 2.5200 - val_rmse: 1.2513\n",
      "Epoch 819/1500\n",
      "22470/22470 [==============================] - 18s 802us/sample - loss: 1.8616 - rmse: 1.2810 - val_loss: 2.4027 - val_rmse: 1.2231\n",
      "Epoch 820/1500\n",
      "22470/22470 [==============================] - 18s 801us/sample - loss: 1.8734 - rmse: 1.2832 - val_loss: 2.2951 - val_rmse: 1.1993\n",
      "Epoch 821/1500\n",
      "22470/22470 [==============================] - 18s 807us/sample - loss: 1.8658 - rmse: 1.2811 - val_loss: 2.3481 - val_rmse: 1.2115\n",
      "Epoch 822/1500\n",
      "22470/22470 [==============================] - 18s 802us/sample - loss: 1.8737 - rmse: 1.2826 - val_loss: 2.2632 - val_rmse: 1.2033\n",
      "Epoch 823/1500\n",
      "22470/22470 [==============================] - 18s 805us/sample - loss: 1.8736 - rmse: 1.2827 - val_loss: 2.4426 - val_rmse: 1.2501\n",
      "Epoch 824/1500\n",
      "22470/22470 [==============================] - 18s 803us/sample - loss: 1.8755 - rmse: 1.2832 - val_loss: 2.2976 - val_rmse: 1.2014\n",
      "Epoch 825/1500\n",
      "22470/22470 [==============================] - 18s 802us/sample - loss: 1.8600 - rmse: 1.2795 - val_loss: 2.5071 - val_rmse: 1.2596\n",
      "Epoch 826/1500\n",
      "22470/22470 [==============================] - 19s 857us/sample - loss: 1.8577 - rmse: 1.2769 - val_loss: 2.3077 - val_rmse: 1.2002\n",
      "Epoch 827/1500\n",
      "22470/22470 [==============================] - 19s 833us/sample - loss: 1.8722 - rmse: 1.2828 - val_loss: 2.2947 - val_rmse: 1.1970\n",
      "Epoch 828/1500\n",
      "22470/22470 [==============================] - 18s 806us/sample - loss: 1.8684 - rmse: 1.2804 - val_loss: 2.3403 - val_rmse: 1.2157\n",
      "Epoch 829/1500\n",
      "22470/22470 [==============================] - 18s 803us/sample - loss: 1.8576 - rmse: 1.2775 - val_loss: 2.3108 - val_rmse: 1.1987\n",
      "Epoch 830/1500\n",
      "22470/22470 [==============================] - 18s 809us/sample - loss: 1.8590 - rmse: 1.2779 - val_loss: 2.4727 - val_rmse: 1.2352\n",
      "Epoch 831/1500\n",
      "22470/22470 [==============================] - 18s 805us/sample - loss: 1.8550 - rmse: 1.2767 - val_loss: 2.4221 - val_rmse: 1.2231\n",
      "Epoch 832/1500\n",
      "22470/22470 [==============================] - 18s 803us/sample - loss: 1.8672 - rmse: 1.2800 - val_loss: 2.3706 - val_rmse: 1.2135\n",
      "Epoch 833/1500\n",
      "22470/22470 [==============================] - 18s 803us/sample - loss: 1.8675 - rmse: 1.2796 - val_loss: 2.2993 - val_rmse: 1.2072\n",
      "Epoch 834/1500\n",
      "22470/22470 [==============================] - 18s 805us/sample - loss: 1.8651 - rmse: 1.2782 - val_loss: 2.3825 - val_rmse: 1.2204\n",
      "Epoch 835/1500\n",
      "22470/22470 [==============================] - 18s 805us/sample - loss: 1.8561 - rmse: 1.2764 - val_loss: 2.4416 - val_rmse: 1.2326\n",
      "Epoch 836/1500\n",
      "22470/22470 [==============================] - 18s 804us/sample - loss: 1.8569 - rmse: 1.2766 - val_loss: 2.3597 - val_rmse: 1.2116\n",
      "Epoch 837/1500\n",
      "22470/22470 [==============================] - 18s 810us/sample - loss: 1.8734 - rmse: 1.2816 - val_loss: 2.2790 - val_rmse: 1.1969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 838/1500\n",
      "22470/22470 [==============================] - 17s 765us/sample - loss: 1.8568 - rmse: 1.2761 - val_loss: 2.2737 - val_rmse: 1.1920\n",
      "Epoch 839/1500\n",
      "22470/22470 [==============================] - 17s 759us/sample - loss: 1.8756 - rmse: 1.2827 - val_loss: 2.4117 - val_rmse: 1.2443\n",
      "Epoch 840/1500\n",
      "22470/22470 [==============================] - 17s 762us/sample - loss: 1.8539 - rmse: 1.2757 - val_loss: 2.2109 - val_rmse: 1.1746\n",
      "Epoch 841/1500\n",
      "22470/22470 [==============================] - 17s 759us/sample - loss: 1.8600 - rmse: 1.2773 - val_loss: 2.3495 - val_rmse: 1.2111\n",
      "Epoch 842/1500\n",
      "22470/22470 [==============================] - 17s 764us/sample - loss: 1.8528 - rmse: 1.2758 - val_loss: 2.4114 - val_rmse: 1.2290\n",
      "Epoch 843/1500\n",
      "22470/22470 [==============================] - 17s 770us/sample - loss: 1.8644 - rmse: 1.2774 - val_loss: 2.3182 - val_rmse: 1.2059\n",
      "Epoch 844/1500\n",
      "22470/22470 [==============================] - 18s 794us/sample - loss: 1.8404 - rmse: 1.2716 - val_loss: 2.3076 - val_rmse: 1.2099\n",
      "Epoch 845/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 1.8393 - rmse: 1.2715 - val_loss: 2.3087 - val_rmse: 1.2018\n",
      "Epoch 846/1500\n",
      "22470/22470 [==============================] - 17s 758us/sample - loss: 1.8478 - rmse: 1.2732 - val_loss: 2.3768 - val_rmse: 1.2136\n",
      "Epoch 847/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 1.8399 - rmse: 1.2711 - val_loss: 2.2772 - val_rmse: 1.1928\n",
      "Epoch 848/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 1.8518 - rmse: 1.2736 - val_loss: 2.4072 - val_rmse: 1.2250\n",
      "Epoch 849/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 1.8409 - rmse: 1.2710 - val_loss: 2.3231 - val_rmse: 1.2010\n",
      "Epoch 850/1500\n",
      "22470/22470 [==============================] - 17s 753us/sample - loss: 1.8647 - rmse: 1.2785 - val_loss: 2.3068 - val_rmse: 1.2065\n",
      "Epoch 851/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 1.8362 - rmse: 1.2698 - val_loss: 2.2798 - val_rmse: 1.2055\n",
      "Epoch 852/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 1.8337 - rmse: 1.2683 - val_loss: 2.3205 - val_rmse: 1.2051\n",
      "Epoch 853/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 1.8278 - rmse: 1.2674 - val_loss: 2.3196 - val_rmse: 1.2000\n",
      "Epoch 854/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 1.8215 - rmse: 1.2648 - val_loss: 2.3264 - val_rmse: 1.2003\n",
      "Epoch 855/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 1.8094 - rmse: 1.2609 - val_loss: 2.4121 - val_rmse: 1.2210\n",
      "Epoch 856/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 1.8321 - rmse: 1.2686 - val_loss: 2.3389 - val_rmse: 1.2034\n",
      "Epoch 857/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 1.8172 - rmse: 1.2651 - val_loss: 2.2795 - val_rmse: 1.1936\n",
      "Epoch 858/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 1.8224 - rmse: 1.2648 - val_loss: 2.3907 - val_rmse: 1.2037\n",
      "Epoch 859/1500\n",
      "22470/22470 [==============================] - 17s 747us/sample - loss: 1.8386 - rmse: 1.2699 - val_loss: 2.3523 - val_rmse: 1.2012\n",
      "Epoch 860/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 1.8473 - rmse: 1.2726 - val_loss: 2.3317 - val_rmse: 1.1972\n",
      "Epoch 861/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 1.8296 - rmse: 1.2685 - val_loss: 2.3263 - val_rmse: 1.2038\n",
      "Epoch 862/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 1.8316 - rmse: 1.2676 - val_loss: 2.3175 - val_rmse: 1.1877\n",
      "Epoch 863/1500\n",
      "22470/22470 [==============================] - 17s 753us/sample - loss: 1.8221 - rmse: 1.2662 - val_loss: 2.3593 - val_rmse: 1.2045\n",
      "Epoch 864/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 1.8343 - rmse: 1.2669 - val_loss: 2.3428 - val_rmse: 1.2051\n",
      "Epoch 865/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 1.8370 - rmse: 1.2679 - val_loss: 2.3372 - val_rmse: 1.1992\n",
      "Epoch 866/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 1.8228 - rmse: 1.2644 - val_loss: 2.4095 - val_rmse: 1.2117\n",
      "Epoch 867/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 1.8118 - rmse: 1.2622 - val_loss: 2.3496 - val_rmse: 1.2130\n",
      "Epoch 868/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 1.8242 - rmse: 1.2633 - val_loss: 2.3592 - val_rmse: 1.2024\n",
      "Epoch 869/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 1.8309 - rmse: 1.2672 - val_loss: 2.4102 - val_rmse: 1.2143\n",
      "Epoch 870/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 1.8183 - rmse: 1.2617 - val_loss: 2.3530 - val_rmse: 1.2007\n",
      "Epoch 871/1500\n",
      "22470/22470 [==============================] - 17s 753us/sample - loss: 1.8187 - rmse: 1.2630 - val_loss: 2.2738 - val_rmse: 1.1867\n",
      "Epoch 872/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 1.8082 - rmse: 1.2590 - val_loss: 2.3179 - val_rmse: 1.1948\n",
      "Epoch 873/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 1.8143 - rmse: 1.2611 - val_loss: 2.4406 - val_rmse: 1.2182\n",
      "Epoch 874/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 1.8229 - rmse: 1.2639 - val_loss: 2.2562 - val_rmse: 1.1812\n",
      "Epoch 875/1500\n",
      "22470/22470 [==============================] - 17s 756us/sample - loss: 1.8067 - rmse: 1.2585 - val_loss: 2.2148 - val_rmse: 1.1683\n",
      "Epoch 876/1500\n",
      "22470/22470 [==============================] - 17s 753us/sample - loss: 1.8235 - rmse: 1.2647 - val_loss: 2.3138 - val_rmse: 1.1866\n",
      "Epoch 877/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 1.8217 - rmse: 1.2632 - val_loss: 2.4116 - val_rmse: 1.2071\n",
      "Epoch 878/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 1.8019 - rmse: 1.2588 - val_loss: 2.4100 - val_rmse: 1.2148\n",
      "Epoch 879/1500\n",
      "22470/22470 [==============================] - 17s 754us/sample - loss: 1.8147 - rmse: 1.2614 - val_loss: 2.3835 - val_rmse: 1.2034\n",
      "Epoch 880/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 1.8152 - rmse: 1.2600 - val_loss: 2.4807 - val_rmse: 1.2284\n",
      "Epoch 881/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 1.8235 - rmse: 1.2642 - val_loss: 2.3363 - val_rmse: 1.1978\n",
      "Epoch 882/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 1.8137 - rmse: 1.2599 - val_loss: 2.2856 - val_rmse: 1.1882\n",
      "Epoch 883/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 1.8048 - rmse: 1.2575 - val_loss: 2.3932 - val_rmse: 1.2093\n",
      "Epoch 884/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 1.8044 - rmse: 1.2589 - val_loss: 2.2406 - val_rmse: 1.1764\n",
      "Epoch 885/1500\n",
      "22470/22470 [==============================] - 17s 753us/sample - loss: 1.8015 - rmse: 1.2580 - val_loss: 2.2677 - val_rmse: 1.1786\n",
      "Epoch 886/1500\n",
      "22470/22470 [==============================] - 17s 754us/sample - loss: 1.8084 - rmse: 1.2577 - val_loss: 2.4380 - val_rmse: 1.2211\n",
      "Epoch 887/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 1.8043 - rmse: 1.2587 - val_loss: 2.4011 - val_rmse: 1.2094\n",
      "Epoch 888/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 1.7893 - rmse: 1.2533 - val_loss: 2.3626 - val_rmse: 1.2030\n",
      "Epoch 889/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 1.8044 - rmse: 1.2563 - val_loss: 2.3229 - val_rmse: 1.1921\n",
      "Epoch 890/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 1.8187 - rmse: 1.2627 - val_loss: 2.3219 - val_rmse: 1.1996\n",
      "Epoch 891/1500\n",
      "22470/22470 [==============================] - 17s 754us/sample - loss: 1.7866 - rmse: 1.2512 - val_loss: 2.3344 - val_rmse: 1.1951\n",
      "Epoch 892/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 1.8022 - rmse: 1.2556 - val_loss: 2.3763 - val_rmse: 1.2036\n",
      "Epoch 893/1500\n",
      "22470/22470 [==============================] - 17s 755us/sample - loss: 1.7934 - rmse: 1.2535 - val_loss: 2.3389 - val_rmse: 1.1958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 894/1500\n",
      "22470/22470 [==============================] - 17s 747us/sample - loss: 1.8115 - rmse: 1.2594 - val_loss: 2.2556 - val_rmse: 1.1825\n",
      "Epoch 895/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 1.7855 - rmse: 1.2508 - val_loss: 2.3567 - val_rmse: 1.1969\n",
      "Epoch 896/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 1.7737 - rmse: 1.2471 - val_loss: 2.3291 - val_rmse: 1.1957\n",
      "Epoch 897/1500\n",
      "22470/22470 [==============================] - 17s 747us/sample - loss: 1.7935 - rmse: 1.2528 - val_loss: 2.2918 - val_rmse: 1.1881\n",
      "Epoch 898/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 1.7890 - rmse: 1.2516 - val_loss: 2.4002 - val_rmse: 1.2057\n",
      "Epoch 899/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 1.8008 - rmse: 1.2549 - val_loss: 2.2604 - val_rmse: 1.1893\n",
      "Epoch 900/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 1.7884 - rmse: 1.2515 - val_loss: 2.4719 - val_rmse: 1.2424\n",
      "Epoch 901/1500\n",
      "22470/22470 [==============================] - 17s 747us/sample - loss: 1.8106 - rmse: 1.2570 - val_loss: 2.3041 - val_rmse: 1.1898\n",
      "Epoch 902/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 1.7903 - rmse: 1.2518 - val_loss: 2.2952 - val_rmse: 1.1980\n",
      "Epoch 903/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 1.8004 - rmse: 1.2552 - val_loss: 2.3455 - val_rmse: 1.1977\n",
      "Epoch 904/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 1.7904 - rmse: 1.2510 - val_loss: 2.2806 - val_rmse: 1.1812\n",
      "Epoch 905/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 1.7893 - rmse: 1.2515 - val_loss: 2.2071 - val_rmse: 1.1689\n",
      "Epoch 906/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 1.7814 - rmse: 1.2490 - val_loss: 2.8683 - val_rmse: 1.3319\n",
      "Epoch 907/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 1.7763 - rmse: 1.2475 - val_loss: 2.2696 - val_rmse: 1.1789\n",
      "Epoch 908/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 1.7905 - rmse: 1.2518 - val_loss: 2.2849 - val_rmse: 1.1878\n",
      "Epoch 909/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 1.7804 - rmse: 1.2482 - val_loss: 2.2883 - val_rmse: 1.1946\n",
      "Epoch 910/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 1.7953 - rmse: 1.2531 - val_loss: 2.3030 - val_rmse: 1.1862\n",
      "Epoch 911/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 1.7677 - rmse: 1.2440 - val_loss: 2.2717 - val_rmse: 1.1735\n",
      "Epoch 912/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 1.7680 - rmse: 1.2453 - val_loss: 2.5405 - val_rmse: 1.2419\n",
      "Epoch 913/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 1.7882 - rmse: 1.2503 - val_loss: 2.4650 - val_rmse: 1.2239\n",
      "Epoch 914/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 1.7621 - rmse: 1.2433 - val_loss: 2.3790 - val_rmse: 1.2007\n",
      "Epoch 915/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 1.7917 - rmse: 1.2505 - val_loss: 2.2919 - val_rmse: 1.1800\n",
      "Epoch 916/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 1.7791 - rmse: 1.2481 - val_loss: 2.4124 - val_rmse: 1.2104\n",
      "Epoch 917/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 1.7773 - rmse: 1.2473 - val_loss: 2.2859 - val_rmse: 1.1769\n",
      "Epoch 918/1500\n",
      "22470/22470 [==============================] - 17s 754us/sample - loss: 1.7719 - rmse: 1.2440 - val_loss: 2.2463 - val_rmse: 1.1745\n",
      "Epoch 919/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 1.7722 - rmse: 1.2455 - val_loss: 2.2997 - val_rmse: 1.1847\n",
      "Epoch 920/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 1.7835 - rmse: 1.2501 - val_loss: 2.2497 - val_rmse: 1.1771\n",
      "Epoch 921/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 1.7669 - rmse: 1.2436 - val_loss: 2.2667 - val_rmse: 1.1808\n",
      "Epoch 922/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 1.7694 - rmse: 1.2445 - val_loss: 2.3397 - val_rmse: 1.1941\n",
      "Epoch 923/1500\n",
      "22470/22470 [==============================] - 17s 748us/sample - loss: 1.7769 - rmse: 1.2472 - val_loss: 2.4490 - val_rmse: 1.2204\n",
      "Epoch 924/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 1.7728 - rmse: 1.2455 - val_loss: 2.4351 - val_rmse: 1.2124\n",
      "Epoch 925/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 1.7681 - rmse: 1.2435 - val_loss: 2.3153 - val_rmse: 1.1882\n",
      "Epoch 926/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 1.7686 - rmse: 1.2438 - val_loss: 2.2508 - val_rmse: 1.1750\n",
      "Epoch 927/1500\n",
      "22470/22470 [==============================] - 17s 751us/sample - loss: 1.7572 - rmse: 1.2402 - val_loss: 2.2406 - val_rmse: 1.1729\n",
      "Epoch 928/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 1.7720 - rmse: 1.2440 - val_loss: 2.2828 - val_rmse: 1.1878\n",
      "Epoch 929/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 1.7599 - rmse: 1.2417 - val_loss: 2.2888 - val_rmse: 1.1813\n",
      "Epoch 930/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 1.7432 - rmse: 1.2355 - val_loss: 2.3013 - val_rmse: 1.1880\n",
      "Epoch 931/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 1.7581 - rmse: 1.2396 - val_loss: 2.3731 - val_rmse: 1.1998\n",
      "Epoch 932/1500\n",
      "22470/22470 [==============================] - 17s 749us/sample - loss: 1.7838 - rmse: 1.2470 - val_loss: 2.3367 - val_rmse: 1.1929\n",
      "Epoch 933/1500\n",
      "22470/22470 [==============================] - 17s 750us/sample - loss: 1.7795 - rmse: 1.2471 - val_loss: 2.2709 - val_rmse: 1.1814\n",
      "Epoch 934/1500\n",
      "22470/22470 [==============================] - 17s 757us/sample - loss: 1.7736 - rmse: 1.2435 - val_loss: 2.2356 - val_rmse: 1.1678\n",
      "Epoch 935/1500\n",
      "22470/22470 [==============================] - 17s 755us/sample - loss: 1.7628 - rmse: 1.2408 - val_loss: 2.2581 - val_rmse: 1.1785\n",
      "Epoch 936/1500\n",
      "22470/22470 [==============================] - 17s 752us/sample - loss: 1.7658 - rmse: 1.2423 - val_loss: 2.3258 - val_rmse: 1.1916\n",
      "Epoch 937/1500\n",
      "22470/22470 [==============================] - 17s 753us/sample - loss: 1.7617 - rmse: 1.2384 - val_loss: 2.1979 - val_rmse: 1.1708\n",
      "Epoch 938/1500\n",
      "22470/22470 [==============================] - 17s 760us/sample - loss: 1.7510 - rmse: 1.2371 - val_loss: 2.4423 - val_rmse: 1.2157\n",
      "Epoch 939/1500\n",
      "22470/22470 [==============================] - 20s 899us/sample - loss: 1.7465 - rmse: 1.2362 - val_loss: 2.2584 - val_rmse: 1.1794\n",
      "Epoch 940/1500\n",
      "22470/22470 [==============================] - 19s 840us/sample - loss: 1.7604 - rmse: 1.2399 - val_loss: 2.2237 - val_rmse: 1.1656\n",
      "Epoch 941/1500\n",
      "22470/22470 [==============================] - 17s 771us/sample - loss: 1.7463 - rmse: 1.2356 - val_loss: 2.1667 - val_rmse: 1.1587\n",
      "Epoch 942/1500\n",
      "22470/22470 [==============================] - 16s 719us/sample - loss: 1.7538 - rmse: 1.2382 - val_loss: 2.2643 - val_rmse: 1.1746\n",
      "Epoch 943/1500\n",
      "22470/22470 [==============================] - 16s 713us/sample - loss: 1.7472 - rmse: 1.2355 - val_loss: 2.3232 - val_rmse: 1.1903\n",
      "Epoch 944/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.7609 - rmse: 1.2404 - val_loss: 2.3034 - val_rmse: 1.1829\n",
      "Epoch 945/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.7495 - rmse: 1.2359 - val_loss: 2.2264 - val_rmse: 1.1722\n",
      "Epoch 946/1500\n",
      "22470/22470 [==============================] - 16s 713us/sample - loss: 1.7654 - rmse: 1.2425 - val_loss: 2.3014 - val_rmse: 1.1852\n",
      "Epoch 947/1500\n",
      "22470/22470 [==============================] - 16s 713us/sample - loss: 1.7441 - rmse: 1.2346 - val_loss: 2.2888 - val_rmse: 1.1829\n",
      "Epoch 948/1500\n",
      "22470/22470 [==============================] - 16s 719us/sample - loss: 1.7594 - rmse: 1.2377 - val_loss: 2.2386 - val_rmse: 1.1736\n",
      "Epoch 949/1500\n",
      "22470/22470 [==============================] - 16s 718us/sample - loss: 1.7439 - rmse: 1.2348 - val_loss: 2.3483 - val_rmse: 1.1994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 950/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.7389 - rmse: 1.2334 - val_loss: 2.2313 - val_rmse: 1.1701\n",
      "Epoch 951/1500\n",
      "22470/22470 [==============================] - 16s 713us/sample - loss: 1.7515 - rmse: 1.2345 - val_loss: 2.3407 - val_rmse: 1.1929\n",
      "Epoch 952/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.7478 - rmse: 1.2347 - val_loss: 2.1965 - val_rmse: 1.1646\n",
      "Epoch 953/1500\n",
      "22470/22470 [==============================] - 16s 713us/sample - loss: 1.7609 - rmse: 1.2397 - val_loss: 2.2128 - val_rmse: 1.1686\n",
      "Epoch 954/1500\n",
      "22470/22470 [==============================] - 16s 713us/sample - loss: 1.7467 - rmse: 1.2355 - val_loss: 2.2796 - val_rmse: 1.1742\n",
      "Epoch 955/1500\n",
      "22470/22470 [==============================] - 16s 712us/sample - loss: 1.7433 - rmse: 1.2342 - val_loss: 2.2713 - val_rmse: 1.1746\n",
      "Epoch 956/1500\n",
      "22470/22470 [==============================] - 16s 713us/sample - loss: 1.7297 - rmse: 1.2298 - val_loss: 2.3906 - val_rmse: 1.2009\n",
      "Epoch 957/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.7374 - rmse: 1.2333 - val_loss: 2.3224 - val_rmse: 1.1884\n",
      "Epoch 958/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.7430 - rmse: 1.2342 - val_loss: 2.2780 - val_rmse: 1.1728\n",
      "Epoch 959/1500\n",
      "22470/22470 [==============================] - 16s 713us/sample - loss: 1.7432 - rmse: 1.2336 - val_loss: 2.2531 - val_rmse: 1.1739\n",
      "Epoch 960/1500\n",
      "22470/22470 [==============================] - 17s 762us/sample - loss: 1.7356 - rmse: 1.2306 - val_loss: 2.2622 - val_rmse: 1.1700\n",
      "Epoch 961/1500\n",
      "22470/22470 [==============================] - 16s 719us/sample - loss: 1.7265 - rmse: 1.2289 - val_loss: 2.3635 - val_rmse: 1.2024\n",
      "Epoch 962/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.7329 - rmse: 1.2290 - val_loss: 2.3238 - val_rmse: 1.1837\n",
      "Epoch 963/1500\n",
      "22470/22470 [==============================] - 16s 718us/sample - loss: 1.7272 - rmse: 1.2276 - val_loss: 2.3404 - val_rmse: 1.1980\n",
      "Epoch 964/1500\n",
      "22470/22470 [==============================] - 16s 731us/sample - loss: 1.7432 - rmse: 1.2331 - val_loss: 2.3395 - val_rmse: 1.1892\n",
      "Epoch 965/1500\n",
      "22470/22470 [==============================] - 16s 723us/sample - loss: 1.7263 - rmse: 1.2267 - val_loss: 2.1767 - val_rmse: 1.1555\n",
      "Epoch 966/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.7452 - rmse: 1.2328 - val_loss: 2.2094 - val_rmse: 1.1625\n",
      "Epoch 967/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.7353 - rmse: 1.2306 - val_loss: 2.2865 - val_rmse: 1.1725\n",
      "Epoch 968/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.7391 - rmse: 1.2319 - val_loss: 2.4233 - val_rmse: 1.2200\n",
      "Epoch 969/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.7356 - rmse: 1.2317 - val_loss: 2.3645 - val_rmse: 1.1929\n",
      "Epoch 970/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.7422 - rmse: 1.2330 - val_loss: 2.2508 - val_rmse: 1.1764\n",
      "Epoch 971/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.7269 - rmse: 1.2264 - val_loss: 2.2619 - val_rmse: 1.1735\n",
      "Epoch 972/1500\n",
      "22470/22470 [==============================] - 16s 718us/sample - loss: 1.7556 - rmse: 1.2368 - val_loss: 2.2094 - val_rmse: 1.1641\n",
      "Epoch 973/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.7163 - rmse: 1.2227 - val_loss: 2.3235 - val_rmse: 1.1828\n",
      "Epoch 974/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.7243 - rmse: 1.2262 - val_loss: 2.2333 - val_rmse: 1.1657\n",
      "Epoch 975/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.7443 - rmse: 1.2323 - val_loss: 2.2310 - val_rmse: 1.1670\n",
      "Epoch 976/1500\n",
      "22470/22470 [==============================] - 16s 718us/sample - loss: 1.7274 - rmse: 1.2277 - val_loss: 2.4222 - val_rmse: 1.2096\n",
      "Epoch 977/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.7296 - rmse: 1.2264 - val_loss: 2.4304 - val_rmse: 1.2053\n",
      "Epoch 978/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.7220 - rmse: 1.2265 - val_loss: 2.3888 - val_rmse: 1.1984\n",
      "Epoch 979/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.7158 - rmse: 1.2239 - val_loss: 2.2851 - val_rmse: 1.1815\n",
      "Epoch 980/1500\n",
      "22470/22470 [==============================] - 16s 720us/sample - loss: 1.7131 - rmse: 1.2229 - val_loss: 2.4364 - val_rmse: 1.2166\n",
      "Epoch 981/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.7167 - rmse: 1.2241 - val_loss: 2.3435 - val_rmse: 1.1931\n",
      "Epoch 982/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.7303 - rmse: 1.2290 - val_loss: 2.2870 - val_rmse: 1.1719\n",
      "Epoch 983/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.7163 - rmse: 1.2232 - val_loss: 2.2349 - val_rmse: 1.1713\n",
      "Epoch 984/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.7135 - rmse: 1.2231 - val_loss: 2.2241 - val_rmse: 1.1667\n",
      "Epoch 985/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.7172 - rmse: 1.2235 - val_loss: 2.2953 - val_rmse: 1.1767\n",
      "Epoch 986/1500\n",
      "22470/22470 [==============================] - 16s 721us/sample - loss: 1.7033 - rmse: 1.2190 - val_loss: 2.3431 - val_rmse: 1.1912\n",
      "Epoch 987/1500\n",
      "22470/22470 [==============================] - 16s 718us/sample - loss: 1.7369 - rmse: 1.2301 - val_loss: 2.3181 - val_rmse: 1.1781\n",
      "Epoch 988/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.7261 - rmse: 1.2253 - val_loss: 2.3532 - val_rmse: 1.1866\n",
      "Epoch 989/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.6957 - rmse: 1.2161 - val_loss: 2.2763 - val_rmse: 1.1801\n",
      "Epoch 990/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.7212 - rmse: 1.2242 - val_loss: 2.3896 - val_rmse: 1.2008\n",
      "Epoch 991/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.7137 - rmse: 1.2225 - val_loss: 2.2556 - val_rmse: 1.1729\n",
      "Epoch 992/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.7079 - rmse: 1.2195 - val_loss: 2.3545 - val_rmse: 1.1868\n",
      "Epoch 993/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.7215 - rmse: 1.2242 - val_loss: 2.2819 - val_rmse: 1.1753\n",
      "Epoch 994/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.7063 - rmse: 1.2197 - val_loss: 2.4059 - val_rmse: 1.2026\n",
      "Epoch 995/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.7148 - rmse: 1.2224 - val_loss: 2.4912 - val_rmse: 1.2219\n",
      "Epoch 996/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.7280 - rmse: 1.2270 - val_loss: 2.2766 - val_rmse: 1.1712\n",
      "Epoch 997/1500\n",
      "22470/22470 [==============================] - 16s 721us/sample - loss: 1.7326 - rmse: 1.2269 - val_loss: 2.1619 - val_rmse: 1.1472\n",
      "Epoch 998/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.7152 - rmse: 1.2229 - val_loss: 2.2355 - val_rmse: 1.1596\n",
      "Epoch 999/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.7027 - rmse: 1.2183 - val_loss: 2.2728 - val_rmse: 1.1709\n",
      "Epoch 1000/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.7251 - rmse: 1.2259 - val_loss: 2.2831 - val_rmse: 1.1677\n",
      "Epoch 1001/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.7054 - rmse: 1.2198 - val_loss: 2.3944 - val_rmse: 1.1938\n",
      "Epoch 1002/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.6984 - rmse: 1.2167 - val_loss: 2.1953 - val_rmse: 1.1708\n",
      "Epoch 1003/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.7237 - rmse: 1.2243 - val_loss: 2.3552 - val_rmse: 1.1900\n",
      "Epoch 1004/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.6910 - rmse: 1.2143 - val_loss: 2.3140 - val_rmse: 1.1808\n",
      "Epoch 1005/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.7035 - rmse: 1.2179 - val_loss: 2.2505 - val_rmse: 1.1668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1006/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.7140 - rmse: 1.2210 - val_loss: 2.3164 - val_rmse: 1.1837\n",
      "Epoch 1007/1500\n",
      "22470/22470 [==============================] - 16s 713us/sample - loss: 1.7084 - rmse: 1.2203 - val_loss: 2.1908 - val_rmse: 1.1659\n",
      "Epoch 1008/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.6944 - rmse: 1.2158 - val_loss: 2.2566 - val_rmse: 1.1616\n",
      "Epoch 1009/1500\n",
      "22470/22470 [==============================] - 16s 712us/sample - loss: 1.6997 - rmse: 1.2170 - val_loss: 2.1931 - val_rmse: 1.1529\n",
      "Epoch 1010/1500\n",
      "22470/22470 [==============================] - 16s 721us/sample - loss: 1.7067 - rmse: 1.2186 - val_loss: 2.1681 - val_rmse: 1.1490\n",
      "Epoch 1011/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.7205 - rmse: 1.2228 - val_loss: 2.1876 - val_rmse: 1.1556\n",
      "Epoch 1012/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.7158 - rmse: 1.2214 - val_loss: 2.3994 - val_rmse: 1.2151\n",
      "Epoch 1013/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.7003 - rmse: 1.2168 - val_loss: 2.2464 - val_rmse: 1.1642\n",
      "Epoch 1014/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.7073 - rmse: 1.2188 - val_loss: 2.1873 - val_rmse: 1.1541\n",
      "Epoch 1015/1500\n",
      "22470/22470 [==============================] - 16s 713us/sample - loss: 1.7055 - rmse: 1.2184 - val_loss: 2.2044 - val_rmse: 1.1563\n",
      "Epoch 1016/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.7147 - rmse: 1.2203 - val_loss: 2.2631 - val_rmse: 1.1734\n",
      "Epoch 1017/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.6893 - rmse: 1.2141 - val_loss: 2.2061 - val_rmse: 1.1525\n",
      "Epoch 1018/1500\n",
      "22470/22470 [==============================] - 16s 720us/sample - loss: 1.6969 - rmse: 1.2157 - val_loss: 2.1500 - val_rmse: 1.1515\n",
      "Epoch 1019/1500\n",
      "22470/22470 [==============================] - 16s 718us/sample - loss: 1.6982 - rmse: 1.2171 - val_loss: 2.1153 - val_rmse: 1.1353\n",
      "Epoch 1020/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.7009 - rmse: 1.2166 - val_loss: 2.1413 - val_rmse: 1.1458\n",
      "Epoch 1021/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.6855 - rmse: 1.2132 - val_loss: 2.2601 - val_rmse: 1.1715\n",
      "Epoch 1022/1500\n",
      "22470/22470 [==============================] - 16s 713us/sample - loss: 1.7128 - rmse: 1.2209 - val_loss: 2.2418 - val_rmse: 1.1663\n",
      "Epoch 1023/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.6950 - rmse: 1.2158 - val_loss: 2.1797 - val_rmse: 1.1508\n",
      "Epoch 1024/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.7051 - rmse: 1.2191 - val_loss: 2.1159 - val_rmse: 1.1432\n",
      "Epoch 1025/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.6919 - rmse: 1.2135 - val_loss: 2.2102 - val_rmse: 1.1610\n",
      "Epoch 1026/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.6912 - rmse: 1.2136 - val_loss: 2.4382 - val_rmse: 1.2128\n",
      "Epoch 1027/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.6958 - rmse: 1.2154 - val_loss: 2.2388 - val_rmse: 1.1648\n",
      "Epoch 1028/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.6870 - rmse: 1.2110 - val_loss: 2.2239 - val_rmse: 1.1680\n",
      "Epoch 1029/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.6907 - rmse: 1.2139 - val_loss: 2.2031 - val_rmse: 1.1568\n",
      "Epoch 1030/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.6902 - rmse: 1.2144 - val_loss: 2.1949 - val_rmse: 1.1494\n",
      "Epoch 1031/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.6936 - rmse: 1.2145 - val_loss: 2.1965 - val_rmse: 1.1631\n",
      "Epoch 1032/1500\n",
      "22470/22470 [==============================] - 16s 718us/sample - loss: 1.6882 - rmse: 1.2127 - val_loss: 2.4279 - val_rmse: 1.2019\n",
      "Epoch 1033/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.6835 - rmse: 1.2098 - val_loss: 2.2061 - val_rmse: 1.1539\n",
      "Epoch 1034/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.6853 - rmse: 1.2105 - val_loss: 2.1631 - val_rmse: 1.1493\n",
      "Epoch 1035/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.6702 - rmse: 1.2080 - val_loss: 2.1852 - val_rmse: 1.1495\n",
      "Epoch 1036/1500\n",
      "22470/22470 [==============================] - 16s 718us/sample - loss: 1.6777 - rmse: 1.2080 - val_loss: 2.1883 - val_rmse: 1.1442\n",
      "Epoch 1037/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.6977 - rmse: 1.2138 - val_loss: 2.1708 - val_rmse: 1.1525\n",
      "Epoch 1038/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.6804 - rmse: 1.2105 - val_loss: 2.2893 - val_rmse: 1.1826\n",
      "Epoch 1039/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.6936 - rmse: 1.2134 - val_loss: 2.2011 - val_rmse: 1.1588\n",
      "Epoch 1040/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.6822 - rmse: 1.2105 - val_loss: 2.2544 - val_rmse: 1.1614\n",
      "Epoch 1041/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.6825 - rmse: 1.2095 - val_loss: 2.2367 - val_rmse: 1.1560\n",
      "Epoch 1042/1500\n",
      "22470/22470 [==============================] - 16s 718us/sample - loss: 1.6799 - rmse: 1.2088 - val_loss: 2.3835 - val_rmse: 1.1993\n",
      "Epoch 1043/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.6918 - rmse: 1.2117 - val_loss: 2.3516 - val_rmse: 1.1828\n",
      "Epoch 1044/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.6965 - rmse: 1.2144 - val_loss: 2.3357 - val_rmse: 1.1829\n",
      "Epoch 1045/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.6650 - rmse: 1.2044 - val_loss: 2.2412 - val_rmse: 1.1568\n",
      "Epoch 1046/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.6878 - rmse: 1.2119 - val_loss: 2.1717 - val_rmse: 1.1462\n",
      "Epoch 1047/1500\n",
      "22470/22470 [==============================] - 16s 720us/sample - loss: 1.6833 - rmse: 1.2093 - val_loss: 2.2515 - val_rmse: 1.1629\n",
      "Epoch 1048/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.6718 - rmse: 1.2069 - val_loss: 2.4130 - val_rmse: 1.1964\n",
      "Epoch 1049/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.6828 - rmse: 1.2104 - val_loss: 2.1464 - val_rmse: 1.1462\n",
      "Epoch 1050/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.6614 - rmse: 1.2043 - val_loss: 2.3030 - val_rmse: 1.1781\n",
      "Epoch 1051/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.6695 - rmse: 1.2049 - val_loss: 2.2378 - val_rmse: 1.1575\n",
      "Epoch 1052/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.6829 - rmse: 1.2089 - val_loss: 2.2205 - val_rmse: 1.1570\n",
      "Epoch 1053/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.6792 - rmse: 1.2085 - val_loss: 2.2444 - val_rmse: 1.1561\n",
      "Epoch 1054/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.6627 - rmse: 1.2022 - val_loss: 2.4315 - val_rmse: 1.2123\n",
      "Epoch 1055/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.6688 - rmse: 1.2045 - val_loss: 2.2681 - val_rmse: 1.1640\n",
      "Epoch 1056/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.6634 - rmse: 1.2031 - val_loss: 2.2000 - val_rmse: 1.1473\n",
      "Epoch 1057/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.6784 - rmse: 1.2084 - val_loss: 2.2670 - val_rmse: 1.1643\n",
      "Epoch 1058/1500\n",
      "22470/22470 [==============================] - 16s 719us/sample - loss: 1.6657 - rmse: 1.2039 - val_loss: 2.2134 - val_rmse: 1.1545\n",
      "Epoch 1059/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.6571 - rmse: 1.2005 - val_loss: 2.2945 - val_rmse: 1.1760\n",
      "Epoch 1060/1500\n",
      "22470/22470 [==============================] - 16s 720us/sample - loss: 1.6586 - rmse: 1.2023 - val_loss: 2.2632 - val_rmse: 1.1644\n",
      "Epoch 1061/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22470/22470 [==============================] - 16s 713us/sample - loss: 1.6656 - rmse: 1.2039 - val_loss: 2.2114 - val_rmse: 1.1528\n",
      "Epoch 1062/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.6596 - rmse: 1.2020 - val_loss: 2.2666 - val_rmse: 1.1681\n",
      "Epoch 1063/1500\n",
      "22470/22470 [==============================] - 16s 718us/sample - loss: 1.6616 - rmse: 1.2022 - val_loss: 2.1012 - val_rmse: 1.1246\n",
      "Epoch 1064/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.6642 - rmse: 1.2033 - val_loss: 2.2979 - val_rmse: 1.1708\n",
      "Epoch 1065/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.6762 - rmse: 1.2067 - val_loss: 2.2764 - val_rmse: 1.1636\n",
      "Epoch 1066/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.6536 - rmse: 1.2007 - val_loss: 2.1908 - val_rmse: 1.1436\n",
      "Epoch 1067/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.6508 - rmse: 1.1989 - val_loss: 2.1767 - val_rmse: 1.1455\n",
      "Epoch 1068/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.6622 - rmse: 1.2016 - val_loss: 2.2441 - val_rmse: 1.1568\n",
      "Epoch 1069/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.6625 - rmse: 1.2027 - val_loss: 2.3814 - val_rmse: 1.1915\n",
      "Epoch 1070/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.6674 - rmse: 1.2053 - val_loss: 2.1204 - val_rmse: 1.1401\n",
      "Epoch 1071/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.6709 - rmse: 1.2055 - val_loss: 2.1690 - val_rmse: 1.1397\n",
      "Epoch 1072/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.6724 - rmse: 1.2052 - val_loss: 2.2139 - val_rmse: 1.1524\n",
      "Epoch 1073/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.6555 - rmse: 1.2008 - val_loss: 2.2218 - val_rmse: 1.1510\n",
      "Epoch 1074/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.6628 - rmse: 1.2024 - val_loss: 2.1076 - val_rmse: 1.1456\n",
      "Epoch 1075/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.6618 - rmse: 1.2013 - val_loss: 2.1389 - val_rmse: 1.1358\n",
      "Epoch 1076/1500\n",
      "22470/22470 [==============================] - 16s 724us/sample - loss: 1.6491 - rmse: 1.1967 - val_loss: 2.3477 - val_rmse: 1.1814\n",
      "Epoch 1077/1500\n",
      "22470/22470 [==============================] - 16s 719us/sample - loss: 1.6629 - rmse: 1.2021 - val_loss: 2.2767 - val_rmse: 1.1690\n",
      "Epoch 1078/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.6570 - rmse: 1.1994 - val_loss: 2.2708 - val_rmse: 1.1628\n",
      "Epoch 1079/1500\n",
      "22470/22470 [==============================] - 16s 713us/sample - loss: 1.6361 - rmse: 1.1931 - val_loss: 2.2238 - val_rmse: 1.1518\n",
      "Epoch 1080/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.6527 - rmse: 1.1975 - val_loss: 2.4510 - val_rmse: 1.2324\n",
      "Epoch 1081/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.6421 - rmse: 1.1961 - val_loss: 2.2207 - val_rmse: 1.1488\n",
      "Epoch 1082/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.6604 - rmse: 1.2009 - val_loss: 2.2446 - val_rmse: 1.1684\n",
      "Epoch 1083/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.6599 - rmse: 1.2010 - val_loss: 2.1482 - val_rmse: 1.1422\n",
      "Epoch 1084/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.6414 - rmse: 1.1952 - val_loss: 2.1655 - val_rmse: 1.1406\n",
      "Epoch 1085/1500\n",
      "22470/22470 [==============================] - 16s 718us/sample - loss: 1.6548 - rmse: 1.1984 - val_loss: 2.1528 - val_rmse: 1.1489\n",
      "Epoch 1086/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.6470 - rmse: 1.1971 - val_loss: 2.1693 - val_rmse: 1.1429\n",
      "Epoch 1087/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.6425 - rmse: 1.1960 - val_loss: 2.2503 - val_rmse: 1.1541\n",
      "Epoch 1088/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.6613 - rmse: 1.1999 - val_loss: 2.1749 - val_rmse: 1.1411\n",
      "Epoch 1089/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.6571 - rmse: 1.1997 - val_loss: 2.1634 - val_rmse: 1.1387\n",
      "Epoch 1090/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.6428 - rmse: 1.1953 - val_loss: 2.2242 - val_rmse: 1.1542\n",
      "Epoch 1091/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.6458 - rmse: 1.1963 - val_loss: 2.1113 - val_rmse: 1.1363\n",
      "Epoch 1092/1500\n",
      "22470/22470 [==============================] - 16s 720us/sample - loss: 1.6473 - rmse: 1.1959 - val_loss: 2.4740 - val_rmse: 1.2082\n",
      "Epoch 1093/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.6425 - rmse: 1.1946 - val_loss: 2.1964 - val_rmse: 1.1482\n",
      "Epoch 1094/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.6600 - rmse: 1.1989 - val_loss: 2.1906 - val_rmse: 1.1457\n",
      "Epoch 1095/1500\n",
      "22470/22470 [==============================] - 16s 713us/sample - loss: 1.6555 - rmse: 1.2002 - val_loss: 2.3658 - val_rmse: 1.1830\n",
      "Epoch 1096/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.6380 - rmse: 1.1923 - val_loss: 2.3035 - val_rmse: 1.1828\n",
      "Epoch 1097/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.6359 - rmse: 1.1924 - val_loss: 2.1679 - val_rmse: 1.1430\n",
      "Epoch 1098/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.6359 - rmse: 1.1919 - val_loss: 2.1648 - val_rmse: 1.1405\n",
      "Epoch 1099/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.6339 - rmse: 1.1925 - val_loss: 2.2047 - val_rmse: 1.1475\n",
      "Epoch 1100/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.6524 - rmse: 1.1975 - val_loss: 2.2164 - val_rmse: 1.1503\n",
      "Epoch 1101/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.6506 - rmse: 1.1974 - val_loss: 2.1831 - val_rmse: 1.1513\n",
      "Epoch 1102/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.6329 - rmse: 1.1910 - val_loss: 2.1375 - val_rmse: 1.1349\n",
      "Epoch 1103/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.6268 - rmse: 1.1878 - val_loss: 2.2896 - val_rmse: 1.1620\n",
      "Epoch 1104/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.6239 - rmse: 1.1881 - val_loss: 2.1972 - val_rmse: 1.1452\n",
      "Epoch 1105/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.6370 - rmse: 1.1912 - val_loss: 2.2497 - val_rmse: 1.1586\n",
      "Epoch 1106/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.6375 - rmse: 1.1926 - val_loss: 2.1868 - val_rmse: 1.1469\n",
      "Epoch 1107/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.6270 - rmse: 1.1896 - val_loss: 2.1050 - val_rmse: 1.1299\n",
      "Epoch 1108/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.6510 - rmse: 1.1959 - val_loss: 2.2130 - val_rmse: 1.1436\n",
      "Epoch 1109/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.6216 - rmse: 1.1880 - val_loss: 2.2153 - val_rmse: 1.1455\n",
      "Epoch 1110/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.6561 - rmse: 1.1991 - val_loss: 2.2071 - val_rmse: 1.1425\n",
      "Epoch 1111/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.6440 - rmse: 1.1951 - val_loss: 2.3822 - val_rmse: 1.1872\n",
      "Epoch 1112/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.6531 - rmse: 1.1974 - val_loss: 2.3035 - val_rmse: 1.1601\n",
      "Epoch 1113/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.6215 - rmse: 1.1860 - val_loss: 2.3791 - val_rmse: 1.1808\n",
      "Epoch 1114/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.6290 - rmse: 1.1881 - val_loss: 2.1904 - val_rmse: 1.1478\n",
      "Epoch 1115/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.6315 - rmse: 1.1892 - val_loss: 2.1228 - val_rmse: 1.1260\n",
      "Epoch 1116/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22470/22470 [==============================] - 16s 725us/sample - loss: 1.6443 - rmse: 1.1941 - val_loss: 2.2376 - val_rmse: 1.1516\n",
      "Epoch 1117/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.6378 - rmse: 1.1935 - val_loss: 2.2105 - val_rmse: 1.1509\n",
      "Epoch 1118/1500\n",
      "22470/22470 [==============================] - 16s 712us/sample - loss: 1.6380 - rmse: 1.1931 - val_loss: 2.2468 - val_rmse: 1.1609\n",
      "Epoch 1119/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.6370 - rmse: 1.1908 - val_loss: 2.1910 - val_rmse: 1.1434\n",
      "Epoch 1120/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.6345 - rmse: 1.1899 - val_loss: 2.2096 - val_rmse: 1.1508\n",
      "Epoch 1121/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.6194 - rmse: 1.1853 - val_loss: 2.2306 - val_rmse: 1.1583\n",
      "Epoch 1122/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.6330 - rmse: 1.1890 - val_loss: 2.2589 - val_rmse: 1.1542\n",
      "Epoch 1123/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.6330 - rmse: 1.1898 - val_loss: 2.2498 - val_rmse: 1.1511\n",
      "Epoch 1124/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.6370 - rmse: 1.1902 - val_loss: 2.4552 - val_rmse: 1.2004\n",
      "Epoch 1125/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.6304 - rmse: 1.1885 - val_loss: 2.1913 - val_rmse: 1.1399\n",
      "Epoch 1126/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.6290 - rmse: 1.1880 - val_loss: 2.1343 - val_rmse: 1.1334\n",
      "Epoch 1127/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.6127 - rmse: 1.1830 - val_loss: 2.2468 - val_rmse: 1.1499\n",
      "Epoch 1128/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.6344 - rmse: 1.1897 - val_loss: 2.3111 - val_rmse: 1.1789\n",
      "Epoch 1129/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.6262 - rmse: 1.1880 - val_loss: 2.2551 - val_rmse: 1.1550\n",
      "Epoch 1130/1500\n",
      "22470/22470 [==============================] - 16s 726us/sample - loss: 1.6203 - rmse: 1.1858 - val_loss: 2.1038 - val_rmse: 1.1186\n",
      "Epoch 1131/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.6271 - rmse: 1.1875 - val_loss: 2.1685 - val_rmse: 1.1352\n",
      "Epoch 1132/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.6092 - rmse: 1.1823 - val_loss: 2.2433 - val_rmse: 1.1513\n",
      "Epoch 1133/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.6327 - rmse: 1.1905 - val_loss: 2.2378 - val_rmse: 1.1510\n",
      "Epoch 1134/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.6261 - rmse: 1.1870 - val_loss: 2.1114 - val_rmse: 1.1248\n",
      "Epoch 1135/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.6273 - rmse: 1.1871 - val_loss: 2.2392 - val_rmse: 1.1670\n",
      "Epoch 1136/1500\n",
      "22470/22470 [==============================] - 16s 718us/sample - loss: 1.6395 - rmse: 1.1897 - val_loss: 2.3892 - val_rmse: 1.1825\n",
      "Epoch 1137/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.6205 - rmse: 1.1854 - val_loss: 2.1878 - val_rmse: 1.1336\n",
      "Epoch 1138/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.6180 - rmse: 1.1849 - val_loss: 2.3098 - val_rmse: 1.1611\n",
      "Epoch 1139/1500\n",
      "22470/22470 [==============================] - 16s 713us/sample - loss: 1.6239 - rmse: 1.1867 - val_loss: 2.2205 - val_rmse: 1.1429\n",
      "Epoch 1140/1500\n",
      "22470/22470 [==============================] - 16s 720us/sample - loss: 1.6203 - rmse: 1.1867 - val_loss: 2.4951 - val_rmse: 1.2109\n",
      "Epoch 1141/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.6162 - rmse: 1.1849 - val_loss: 2.1877 - val_rmse: 1.1312\n",
      "Epoch 1142/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.6157 - rmse: 1.1840 - val_loss: 2.1568 - val_rmse: 1.1323\n",
      "Epoch 1143/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.6231 - rmse: 1.1846 - val_loss: 2.1803 - val_rmse: 1.1340\n",
      "Epoch 1144/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.6191 - rmse: 1.1829 - val_loss: 2.3396 - val_rmse: 1.1705\n",
      "Epoch 1145/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.6226 - rmse: 1.1854 - val_loss: 2.2185 - val_rmse: 1.1443\n",
      "Epoch 1146/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.6305 - rmse: 1.1881 - val_loss: 2.2139 - val_rmse: 1.1436\n",
      "Epoch 1147/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.6177 - rmse: 1.1838 - val_loss: 2.2351 - val_rmse: 1.1481\n",
      "Epoch 1148/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.6170 - rmse: 1.1833 - val_loss: 2.2186 - val_rmse: 1.1523\n",
      "Epoch 1149/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.6185 - rmse: 1.1843 - val_loss: 2.0950 - val_rmse: 1.1350\n",
      "Epoch 1150/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.6102 - rmse: 1.1818 - val_loss: 2.1868 - val_rmse: 1.1439\n",
      "Epoch 1151/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.6294 - rmse: 1.1888 - val_loss: 2.1846 - val_rmse: 1.1401\n",
      "Epoch 1152/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.6170 - rmse: 1.1845 - val_loss: 2.3867 - val_rmse: 1.1843\n",
      "Epoch 1153/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.6224 - rmse: 1.1848 - val_loss: 2.1981 - val_rmse: 1.1447\n",
      "Epoch 1154/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.6082 - rmse: 1.1792 - val_loss: 2.2940 - val_rmse: 1.1591\n",
      "Epoch 1155/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.6096 - rmse: 1.1816 - val_loss: 2.2825 - val_rmse: 1.1622\n",
      "Epoch 1156/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.6054 - rmse: 1.1801 - val_loss: 2.2224 - val_rmse: 1.1451\n",
      "Epoch 1157/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.6015 - rmse: 1.1777 - val_loss: 2.2112 - val_rmse: 1.1459\n",
      "Epoch 1158/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.6169 - rmse: 1.1840 - val_loss: 2.1574 - val_rmse: 1.1515\n",
      "Epoch 1159/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.6076 - rmse: 1.1807 - val_loss: 2.1382 - val_rmse: 1.1237\n",
      "Epoch 1160/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.6063 - rmse: 1.1799 - val_loss: 2.3206 - val_rmse: 1.1757\n",
      "Epoch 1161/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.6117 - rmse: 1.1808 - val_loss: 2.2082 - val_rmse: 1.1411\n",
      "Epoch 1162/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.6044 - rmse: 1.1799 - val_loss: 2.1284 - val_rmse: 1.1269\n",
      "Epoch 1163/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.6029 - rmse: 1.1793 - val_loss: 2.3374 - val_rmse: 1.1694\n",
      "Epoch 1164/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.6173 - rmse: 1.1825 - val_loss: 2.1460 - val_rmse: 1.1274\n",
      "Epoch 1165/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.6043 - rmse: 1.1788 - val_loss: 2.2803 - val_rmse: 1.1559\n",
      "Epoch 1166/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.6117 - rmse: 1.1814 - val_loss: 2.3038 - val_rmse: 1.1705\n",
      "Epoch 1167/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.6047 - rmse: 1.1777 - val_loss: 2.1212 - val_rmse: 1.1234\n",
      "Epoch 1168/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.6001 - rmse: 1.1782 - val_loss: 2.2445 - val_rmse: 1.1534\n",
      "Epoch 1169/1500\n",
      "22470/22470 [==============================] - 16s 720us/sample - loss: 1.6043 - rmse: 1.1784 - val_loss: 2.0909 - val_rmse: 1.1171\n",
      "Epoch 1170/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.5981 - rmse: 1.1769 - val_loss: 2.1107 - val_rmse: 1.1206\n",
      "Epoch 1171/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.5878 - rmse: 1.1730 - val_loss: 2.4622 - val_rmse: 1.2070\n",
      "Epoch 1172/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.6079 - rmse: 1.1790 - val_loss: 2.1252 - val_rmse: 1.1248\n",
      "Epoch 1173/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.6107 - rmse: 1.1804 - val_loss: 2.3737 - val_rmse: 1.1816\n",
      "Epoch 1174/1500\n",
      "22470/22470 [==============================] - 16s 713us/sample - loss: 1.5944 - rmse: 1.1759 - val_loss: 2.1817 - val_rmse: 1.1298\n",
      "Epoch 1175/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.5979 - rmse: 1.1760 - val_loss: 2.1361 - val_rmse: 1.1233\n",
      "Epoch 1176/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.6074 - rmse: 1.1784 - val_loss: 2.2494 - val_rmse: 1.1556\n",
      "Epoch 1177/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.5901 - rmse: 1.1728 - val_loss: 2.2459 - val_rmse: 1.1447\n",
      "Epoch 1178/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.5941 - rmse: 1.1732 - val_loss: 2.1741 - val_rmse: 1.1313\n",
      "Epoch 1179/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.5940 - rmse: 1.1755 - val_loss: 2.3120 - val_rmse: 1.1647\n",
      "Epoch 1180/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.5954 - rmse: 1.1751 - val_loss: 2.2934 - val_rmse: 1.1568\n",
      "Epoch 1181/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.6075 - rmse: 1.1794 - val_loss: 2.2546 - val_rmse: 1.1525\n",
      "Epoch 1182/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.5761 - rmse: 1.1687 - val_loss: 2.2508 - val_rmse: 1.1496\n",
      "Epoch 1183/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.5910 - rmse: 1.1723 - val_loss: 2.1919 - val_rmse: 1.1327\n",
      "Epoch 1184/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.5849 - rmse: 1.1723 - val_loss: 2.2105 - val_rmse: 1.1407\n",
      "Epoch 1185/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.5943 - rmse: 1.1752 - val_loss: 2.3559 - val_rmse: 1.1705\n",
      "Epoch 1186/1500\n",
      "22470/22470 [==============================] - 16s 713us/sample - loss: 1.5828 - rmse: 1.1721 - val_loss: 2.2461 - val_rmse: 1.1465\n",
      "Epoch 1187/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.5853 - rmse: 1.1709 - val_loss: 2.2017 - val_rmse: 1.1389\n",
      "Epoch 1188/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.5977 - rmse: 1.1774 - val_loss: 2.2285 - val_rmse: 1.1429\n",
      "Epoch 1189/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.5945 - rmse: 1.1761 - val_loss: 2.2287 - val_rmse: 1.1530\n",
      "Epoch 1190/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.5937 - rmse: 1.1745 - val_loss: 2.2255 - val_rmse: 1.1464\n",
      "Epoch 1191/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.5878 - rmse: 1.1737 - val_loss: 2.1511 - val_rmse: 1.1282\n",
      "Epoch 1192/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.5946 - rmse: 1.1756 - val_loss: 2.3436 - val_rmse: 1.1894\n",
      "Epoch 1193/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.5990 - rmse: 1.1759 - val_loss: 2.4542 - val_rmse: 1.1929\n",
      "Epoch 1194/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.5979 - rmse: 1.1756 - val_loss: 2.3630 - val_rmse: 1.1747\n",
      "Epoch 1195/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.5856 - rmse: 1.1716 - val_loss: 2.2277 - val_rmse: 1.1484\n",
      "Epoch 1196/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.5848 - rmse: 1.1713 - val_loss: 2.3447 - val_rmse: 1.1683\n",
      "Epoch 1197/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.5922 - rmse: 1.1735 - val_loss: 2.1694 - val_rmse: 1.1393\n",
      "Epoch 1198/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.5970 - rmse: 1.1746 - val_loss: 2.1396 - val_rmse: 1.1185\n",
      "Epoch 1199/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.5979 - rmse: 1.1757 - val_loss: 2.3551 - val_rmse: 1.1665\n",
      "Epoch 1200/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.5729 - rmse: 1.1675 - val_loss: 2.1756 - val_rmse: 1.1319\n",
      "Epoch 1201/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.5797 - rmse: 1.1687 - val_loss: 2.2228 - val_rmse: 1.1419\n",
      "Epoch 1202/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.6025 - rmse: 1.1759 - val_loss: 2.1635 - val_rmse: 1.1324\n",
      "Epoch 1203/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.5837 - rmse: 1.1709 - val_loss: 2.1225 - val_rmse: 1.1217\n",
      "Epoch 1204/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.5887 - rmse: 1.1724 - val_loss: 2.1467 - val_rmse: 1.1250\n",
      "Epoch 1205/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.5808 - rmse: 1.1695 - val_loss: 2.1990 - val_rmse: 1.1369\n",
      "Epoch 1206/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.5916 - rmse: 1.1739 - val_loss: 2.1467 - val_rmse: 1.1227\n",
      "Epoch 1207/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.5794 - rmse: 1.1689 - val_loss: 2.1680 - val_rmse: 1.1299\n",
      "Epoch 1208/1500\n",
      "22470/22470 [==============================] - 16s 718us/sample - loss: 1.5675 - rmse: 1.1639 - val_loss: 2.2753 - val_rmse: 1.1655\n",
      "Epoch 1209/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.5635 - rmse: 1.1634 - val_loss: 2.2015 - val_rmse: 1.1344\n",
      "Epoch 1210/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.5944 - rmse: 1.1734 - val_loss: 2.3160 - val_rmse: 1.1764\n",
      "Epoch 1211/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.5708 - rmse: 1.1644 - val_loss: 2.1745 - val_rmse: 1.1273\n",
      "Epoch 1212/1500\n",
      "22470/22470 [==============================] - 16s 719us/sample - loss: 1.5859 - rmse: 1.1716 - val_loss: 2.4005 - val_rmse: 1.1801\n",
      "Epoch 1213/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.5910 - rmse: 1.1725 - val_loss: 2.1358 - val_rmse: 1.1337\n",
      "Epoch 1214/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.5926 - rmse: 1.1719 - val_loss: 2.1520 - val_rmse: 1.1284\n",
      "Epoch 1215/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.5680 - rmse: 1.1654 - val_loss: 2.2298 - val_rmse: 1.1350\n",
      "Epoch 1216/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.5841 - rmse: 1.1693 - val_loss: 2.1627 - val_rmse: 1.1266\n",
      "Epoch 1217/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.5709 - rmse: 1.1664 - val_loss: 2.1257 - val_rmse: 1.1228\n",
      "Epoch 1218/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.5817 - rmse: 1.1688 - val_loss: 2.1934 - val_rmse: 1.1315\n",
      "Epoch 1219/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.5829 - rmse: 1.1692 - val_loss: 2.2186 - val_rmse: 1.1359\n",
      "Epoch 1220/1500\n",
      "22470/22470 [==============================] - 16s 718us/sample - loss: 1.5850 - rmse: 1.1697 - val_loss: 2.4066 - val_rmse: 1.1764\n",
      "Epoch 1221/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.5755 - rmse: 1.1674 - val_loss: 2.1588 - val_rmse: 1.1343\n",
      "Epoch 1222/1500\n",
      "22470/22470 [==============================] - 16s 721us/sample - loss: 1.5713 - rmse: 1.1663 - val_loss: 2.0590 - val_rmse: 1.1057\n",
      "Epoch 1223/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.5791 - rmse: 1.1689 - val_loss: 2.1921 - val_rmse: 1.1371\n",
      "Epoch 1224/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.5814 - rmse: 1.1686 - val_loss: 2.1738 - val_rmse: 1.1298\n",
      "Epoch 1225/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.5752 - rmse: 1.1647 - val_loss: 2.0852 - val_rmse: 1.1101\n",
      "Epoch 1226/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.5757 - rmse: 1.1658 - val_loss: 2.1063 - val_rmse: 1.1214\n",
      "Epoch 1227/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.5691 - rmse: 1.1641 - val_loss: 2.1286 - val_rmse: 1.1193\n",
      "Epoch 1228/1500\n",
      "22470/22470 [==============================] - 16s 713us/sample - loss: 1.5702 - rmse: 1.1661 - val_loss: 2.1028 - val_rmse: 1.1105\n",
      "Epoch 1229/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.5856 - rmse: 1.1692 - val_loss: 2.1293 - val_rmse: 1.1191\n",
      "Epoch 1230/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.5732 - rmse: 1.1666 - val_loss: 2.1369 - val_rmse: 1.1195\n",
      "Epoch 1231/1500\n",
      "22470/22470 [==============================] - 16s 713us/sample - loss: 1.5924 - rmse: 1.1715 - val_loss: 2.2134 - val_rmse: 1.1388\n",
      "Epoch 1232/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.5664 - rmse: 1.1631 - val_loss: 2.2911 - val_rmse: 1.1619\n",
      "Epoch 1233/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.5940 - rmse: 1.1729 - val_loss: 2.2973 - val_rmse: 1.1578\n",
      "Epoch 1234/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.5772 - rmse: 1.1678 - val_loss: 2.2288 - val_rmse: 1.1390\n",
      "Epoch 1235/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.5496 - rmse: 1.1579 - val_loss: 2.2889 - val_rmse: 1.1780\n",
      "Epoch 1236/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.5666 - rmse: 1.1637 - val_loss: 2.1283 - val_rmse: 1.1173\n",
      "Epoch 1237/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.5649 - rmse: 1.1622 - val_loss: 2.2536 - val_rmse: 1.1432\n",
      "Epoch 1238/1500\n",
      "22470/22470 [==============================] - 16s 713us/sample - loss: 1.5600 - rmse: 1.1627 - val_loss: 2.1551 - val_rmse: 1.1298\n",
      "Epoch 1239/1500\n",
      "22470/22470 [==============================] - 16s 718us/sample - loss: 1.5602 - rmse: 1.1614 - val_loss: 2.3771 - val_rmse: 1.1715\n",
      "Epoch 1240/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.5789 - rmse: 1.1672 - val_loss: 2.2162 - val_rmse: 1.1443\n",
      "Epoch 1241/1500\n",
      "22470/22470 [==============================] - 16s 723us/sample - loss: 1.5570 - rmse: 1.1601 - val_loss: 2.1836 - val_rmse: 1.1241\n",
      "Epoch 1242/1500\n",
      "22470/22470 [==============================] - 16s 718us/sample - loss: 1.5557 - rmse: 1.1597 - val_loss: 2.3140 - val_rmse: 1.1558\n",
      "Epoch 1243/1500\n",
      "22470/22470 [==============================] - 16s 719us/sample - loss: 1.5639 - rmse: 1.1619 - val_loss: 2.1530 - val_rmse: 1.1192\n",
      "Epoch 1244/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.5506 - rmse: 1.1573 - val_loss: 2.2666 - val_rmse: 1.1474\n",
      "Epoch 1245/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.5630 - rmse: 1.1606 - val_loss: 2.0993 - val_rmse: 1.1091\n",
      "Epoch 1246/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.5690 - rmse: 1.1640 - val_loss: 2.2140 - val_rmse: 1.1325\n",
      "Epoch 1247/1500\n",
      "22470/22470 [==============================] - 16s 719us/sample - loss: 1.5815 - rmse: 1.1690 - val_loss: 2.2096 - val_rmse: 1.1346\n",
      "Epoch 1248/1500\n",
      "22470/22470 [==============================] - 16s 719us/sample - loss: 1.5571 - rmse: 1.1608 - val_loss: 2.1890 - val_rmse: 1.1324\n",
      "Epoch 1249/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.5696 - rmse: 1.1637 - val_loss: 2.1002 - val_rmse: 1.1132\n",
      "Epoch 1250/1500\n",
      "22470/22470 [==============================] - 16s 725us/sample - loss: 1.5535 - rmse: 1.1591 - val_loss: 2.0759 - val_rmse: 1.1055\n",
      "Epoch 1251/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.5708 - rmse: 1.1644 - val_loss: 2.1626 - val_rmse: 1.1293\n",
      "Epoch 1252/1500\n",
      "22470/22470 [==============================] - 16s 718us/sample - loss: 1.5633 - rmse: 1.1627 - val_loss: 2.1703 - val_rmse: 1.1275\n",
      "Epoch 1253/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.5690 - rmse: 1.1649 - val_loss: 2.1471 - val_rmse: 1.1220\n",
      "Epoch 1254/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.5620 - rmse: 1.1603 - val_loss: 2.3170 - val_rmse: 1.1581\n",
      "Epoch 1255/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.5747 - rmse: 1.1660 - val_loss: 2.2250 - val_rmse: 1.1362\n",
      "Epoch 1256/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.5528 - rmse: 1.1579 - val_loss: 2.1633 - val_rmse: 1.1214\n",
      "Epoch 1257/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.5699 - rmse: 1.1652 - val_loss: 2.3823 - val_rmse: 1.1764\n",
      "Epoch 1258/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.5508 - rmse: 1.1582 - val_loss: 2.1811 - val_rmse: 1.1349\n",
      "Epoch 1259/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.5636 - rmse: 1.1622 - val_loss: 2.2410 - val_rmse: 1.1400\n",
      "Epoch 1260/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.5497 - rmse: 1.1572 - val_loss: 2.1381 - val_rmse: 1.1314\n",
      "Epoch 1261/1500\n",
      "22470/22470 [==============================] - 16s 721us/sample - loss: 1.5664 - rmse: 1.1623 - val_loss: 2.0732 - val_rmse: 1.1041\n",
      "Epoch 1262/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.5566 - rmse: 1.1594 - val_loss: 2.1886 - val_rmse: 1.1328\n",
      "Epoch 1263/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.5531 - rmse: 1.1568 - val_loss: 2.3502 - val_rmse: 1.1602\n",
      "Epoch 1264/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.5587 - rmse: 1.1593 - val_loss: 2.2513 - val_rmse: 1.1337\n",
      "Epoch 1265/1500\n",
      "22470/22470 [==============================] - 16s 719us/sample - loss: 1.5659 - rmse: 1.1621 - val_loss: 2.3685 - val_rmse: 1.1703\n",
      "Epoch 1266/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.5544 - rmse: 1.1578 - val_loss: 2.1175 - val_rmse: 1.1131\n",
      "Epoch 1267/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.5590 - rmse: 1.1585 - val_loss: 2.0959 - val_rmse: 1.1141\n",
      "Epoch 1268/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.5456 - rmse: 1.1566 - val_loss: 2.3406 - val_rmse: 1.1603\n",
      "Epoch 1269/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.5631 - rmse: 1.1613 - val_loss: 2.3749 - val_rmse: 1.1695\n",
      "Epoch 1270/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.5381 - rmse: 1.1536 - val_loss: 2.1691 - val_rmse: 1.1280\n",
      "Epoch 1271/1500\n",
      "22470/22470 [==============================] - 16s 719us/sample - loss: 1.5586 - rmse: 1.1600 - val_loss: 2.2108 - val_rmse: 1.1305\n",
      "Epoch 1272/1500\n",
      "22470/22470 [==============================] - 16s 718us/sample - loss: 1.5514 - rmse: 1.1581 - val_loss: 2.1521 - val_rmse: 1.1336\n",
      "Epoch 1273/1500\n",
      "22470/22470 [==============================] - 16s 720us/sample - loss: 1.5454 - rmse: 1.1552 - val_loss: 2.2394 - val_rmse: 1.1365\n",
      "Epoch 1274/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.5456 - rmse: 1.1549 - val_loss: 2.3167 - val_rmse: 1.1687\n",
      "Epoch 1275/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.5506 - rmse: 1.1566 - val_loss: 2.0705 - val_rmse: 1.1070\n",
      "Epoch 1276/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.5508 - rmse: 1.1566 - val_loss: 2.1723 - val_rmse: 1.1255\n",
      "Epoch 1277/1500\n",
      "22470/22470 [==============================] - 16s 721us/sample - loss: 1.5452 - rmse: 1.1549 - val_loss: 2.3449 - val_rmse: 1.1646\n",
      "Epoch 1278/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.5420 - rmse: 1.1549 - val_loss: 2.2368 - val_rmse: 1.1389\n",
      "Epoch 1279/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.5514 - rmse: 1.1560 - val_loss: 2.1126 - val_rmse: 1.1073\n",
      "Epoch 1280/1500\n",
      "22470/22470 [==============================] - 16s 718us/sample - loss: 1.5571 - rmse: 1.1591 - val_loss: 2.2891 - val_rmse: 1.1554\n",
      "Epoch 1281/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.5651 - rmse: 1.1615 - val_loss: 2.1259 - val_rmse: 1.1157\n",
      "Epoch 1282/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.5345 - rmse: 1.1506 - val_loss: 2.2029 - val_rmse: 1.1264\n",
      "Epoch 1283/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.5446 - rmse: 1.1543 - val_loss: 2.1166 - val_rmse: 1.1078\n",
      "Epoch 1284/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.5429 - rmse: 1.1535 - val_loss: 2.2093 - val_rmse: 1.1309\n",
      "Epoch 1285/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.5499 - rmse: 1.1579 - val_loss: 2.1155 - val_rmse: 1.1140\n",
      "Epoch 1286/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.5563 - rmse: 1.1575 - val_loss: 2.2797 - val_rmse: 1.1463\n",
      "Epoch 1287/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.5470 - rmse: 1.1564 - val_loss: 2.1764 - val_rmse: 1.1281\n",
      "Epoch 1288/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.5506 - rmse: 1.1558 - val_loss: 2.2185 - val_rmse: 1.1362\n",
      "Epoch 1289/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.5392 - rmse: 1.1525 - val_loss: 2.4646 - val_rmse: 1.1935\n",
      "Epoch 1290/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.5416 - rmse: 1.1533 - val_loss: 2.2099 - val_rmse: 1.1387\n",
      "Epoch 1291/1500\n",
      "22470/22470 [==============================] - 16s 720us/sample - loss: 1.5468 - rmse: 1.1540 - val_loss: 2.0618 - val_rmse: 1.1041\n",
      "Epoch 1292/1500\n",
      "22470/22470 [==============================] - 16s 721us/sample - loss: 1.5445 - rmse: 1.1544 - val_loss: 2.0569 - val_rmse: 1.1006\n",
      "Epoch 1293/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.5492 - rmse: 1.1551 - val_loss: 2.1655 - val_rmse: 1.1269\n",
      "Epoch 1294/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.5498 - rmse: 1.1565 - val_loss: 2.4291 - val_rmse: 1.1879\n",
      "Epoch 1295/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.5347 - rmse: 1.1513 - val_loss: 2.1297 - val_rmse: 1.1228\n",
      "Epoch 1296/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.5468 - rmse: 1.1541 - val_loss: 2.1684 - val_rmse: 1.1244\n",
      "Epoch 1297/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.5553 - rmse: 1.1567 - val_loss: 2.1574 - val_rmse: 1.1222\n",
      "Epoch 1298/1500\n",
      "22470/22470 [==============================] - 16s 713us/sample - loss: 1.5412 - rmse: 1.1544 - val_loss: 2.0517 - val_rmse: 1.1031\n",
      "Epoch 1299/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.5308 - rmse: 1.1496 - val_loss: 2.2831 - val_rmse: 1.1452\n",
      "Epoch 1300/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.5469 - rmse: 1.1560 - val_loss: 2.2934 - val_rmse: 1.1456\n",
      "Epoch 1301/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.5346 - rmse: 1.1500 - val_loss: 2.2377 - val_rmse: 1.1348\n",
      "Epoch 1302/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.5386 - rmse: 1.1516 - val_loss: 2.2456 - val_rmse: 1.1476\n",
      "Epoch 1303/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.5503 - rmse: 1.1550 - val_loss: 2.2421 - val_rmse: 1.1390\n",
      "Epoch 1304/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.5388 - rmse: 1.1521 - val_loss: 2.1068 - val_rmse: 1.1126\n",
      "Epoch 1305/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.5372 - rmse: 1.1507 - val_loss: 2.1971 - val_rmse: 1.1302\n",
      "Epoch 1306/1500\n",
      "22470/22470 [==============================] - 16s 713us/sample - loss: 1.5468 - rmse: 1.1551 - val_loss: 2.1622 - val_rmse: 1.1241\n",
      "Epoch 1307/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.5531 - rmse: 1.1554 - val_loss: 2.1996 - val_rmse: 1.1265\n",
      "Epoch 1308/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.5269 - rmse: 1.1462 - val_loss: 2.1632 - val_rmse: 1.1155\n",
      "Epoch 1309/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.5328 - rmse: 1.1491 - val_loss: 2.1784 - val_rmse: 1.1231\n",
      "Epoch 1310/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.5368 - rmse: 1.1529 - val_loss: 2.1730 - val_rmse: 1.1196\n",
      "Epoch 1311/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.5434 - rmse: 1.1527 - val_loss: 2.1054 - val_rmse: 1.1027\n",
      "Epoch 1312/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.5460 - rmse: 1.1557 - val_loss: 2.1812 - val_rmse: 1.1189\n",
      "Epoch 1313/1500\n",
      "22470/22470 [==============================] - 16s 713us/sample - loss: 1.5336 - rmse: 1.1495 - val_loss: 2.2074 - val_rmse: 1.1253\n",
      "Epoch 1314/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.5232 - rmse: 1.1465 - val_loss: 2.1163 - val_rmse: 1.1086\n",
      "Epoch 1315/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.5307 - rmse: 1.1482 - val_loss: 2.1331 - val_rmse: 1.1106\n",
      "Epoch 1316/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.5297 - rmse: 1.1485 - val_loss: 2.2501 - val_rmse: 1.1443\n",
      "Epoch 1317/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.5326 - rmse: 1.1490 - val_loss: 2.3060 - val_rmse: 1.1503\n",
      "Epoch 1318/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.5240 - rmse: 1.1470 - val_loss: 2.1146 - val_rmse: 1.1037\n",
      "Epoch 1319/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.5238 - rmse: 1.1473 - val_loss: 2.2787 - val_rmse: 1.1445\n",
      "Epoch 1320/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.5319 - rmse: 1.1493 - val_loss: 2.1832 - val_rmse: 1.1230\n",
      "Epoch 1321/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.5338 - rmse: 1.1500 - val_loss: 2.2578 - val_rmse: 1.1340\n",
      "Epoch 1322/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.5231 - rmse: 1.1454 - val_loss: 2.1858 - val_rmse: 1.1283\n",
      "Epoch 1323/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.5267 - rmse: 1.1478 - val_loss: 2.2073 - val_rmse: 1.1278\n",
      "Epoch 1324/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.5552 - rmse: 1.1568 - val_loss: 2.2189 - val_rmse: 1.1303\n",
      "Epoch 1325/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.5254 - rmse: 1.1455 - val_loss: 2.1524 - val_rmse: 1.1154\n",
      "Epoch 1326/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.5235 - rmse: 1.1464 - val_loss: 2.1251 - val_rmse: 1.1106\n",
      "Epoch 1327/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.5219 - rmse: 1.1445 - val_loss: 2.4291 - val_rmse: 1.1807\n",
      "Epoch 1328/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.5276 - rmse: 1.1467 - val_loss: 2.3483 - val_rmse: 1.1640\n",
      "Epoch 1329/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.5380 - rmse: 1.1512 - val_loss: 2.0947 - val_rmse: 1.1030\n",
      "Epoch 1330/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.5342 - rmse: 1.1482 - val_loss: 2.4005 - val_rmse: 1.1722\n",
      "Epoch 1331/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.5306 - rmse: 1.1475 - val_loss: 2.1342 - val_rmse: 1.1064\n",
      "Epoch 1332/1500\n",
      "22470/22470 [==============================] - 16s 718us/sample - loss: 1.5311 - rmse: 1.1474 - val_loss: 2.1246 - val_rmse: 1.1179\n",
      "Epoch 1333/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.5271 - rmse: 1.1476 - val_loss: 2.1149 - val_rmse: 1.1149\n",
      "Epoch 1334/1500\n",
      "22470/22470 [==============================] - 16s 718us/sample - loss: 1.5233 - rmse: 1.1456 - val_loss: 2.2069 - val_rmse: 1.1247\n",
      "Epoch 1335/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.5255 - rmse: 1.1455 - val_loss: 2.1155 - val_rmse: 1.1072\n",
      "Epoch 1336/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22470/22470 [==============================] - 16s 720us/sample - loss: 1.5339 - rmse: 1.1488 - val_loss: 2.0624 - val_rmse: 1.0988\n",
      "Epoch 1337/1500\n",
      "22470/22470 [==============================] - 16s 713us/sample - loss: 1.5101 - rmse: 1.1415 - val_loss: 2.2678 - val_rmse: 1.1425\n",
      "Epoch 1338/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.5081 - rmse: 1.1412 - val_loss: 2.0761 - val_rmse: 1.0988\n",
      "Epoch 1339/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.5363 - rmse: 1.1487 - val_loss: 2.1357 - val_rmse: 1.1280\n",
      "Epoch 1340/1500\n",
      "22470/22470 [==============================] - 16s 726us/sample - loss: 1.5378 - rmse: 1.1479 - val_loss: 2.1919 - val_rmse: 1.1252\n",
      "Epoch 1341/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.5201 - rmse: 1.1454 - val_loss: 2.2335 - val_rmse: 1.1366\n",
      "Epoch 1342/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.5131 - rmse: 1.1426 - val_loss: 2.1604 - val_rmse: 1.1134\n",
      "Epoch 1343/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.5296 - rmse: 1.1476 - val_loss: 2.1339 - val_rmse: 1.1118\n",
      "Epoch 1344/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.5091 - rmse: 1.1413 - val_loss: 2.1440 - val_rmse: 1.1133\n",
      "Epoch 1345/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.5178 - rmse: 1.1435 - val_loss: 2.0505 - val_rmse: 1.1032\n",
      "Epoch 1346/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.5192 - rmse: 1.1443 - val_loss: 2.2717 - val_rmse: 1.1383\n",
      "Epoch 1347/1500\n",
      "22470/22470 [==============================] - 16s 718us/sample - loss: 1.5197 - rmse: 1.1443 - val_loss: 2.3633 - val_rmse: 1.1631\n",
      "Epoch 1348/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.5239 - rmse: 1.1461 - val_loss: 2.0947 - val_rmse: 1.1023\n",
      "Epoch 1349/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.5235 - rmse: 1.1447 - val_loss: 2.1709 - val_rmse: 1.1171\n",
      "Epoch 1350/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.5121 - rmse: 1.1412 - val_loss: 2.0968 - val_rmse: 1.1055\n",
      "Epoch 1351/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.5011 - rmse: 1.1389 - val_loss: 2.2100 - val_rmse: 1.1360\n",
      "Epoch 1352/1500\n",
      "22470/22470 [==============================] - 16s 721us/sample - loss: 1.5266 - rmse: 1.1457 - val_loss: 2.2802 - val_rmse: 1.1412\n",
      "Epoch 1353/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.5119 - rmse: 1.1416 - val_loss: 2.2067 - val_rmse: 1.1275\n",
      "Epoch 1354/1500\n",
      "22470/22470 [==============================] - 16s 713us/sample - loss: 1.5098 - rmse: 1.1411 - val_loss: 2.2607 - val_rmse: 1.1345\n",
      "Epoch 1355/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.5150 - rmse: 1.1417 - val_loss: 2.1953 - val_rmse: 1.1200\n",
      "Epoch 1356/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.5141 - rmse: 1.1420 - val_loss: 2.1159 - val_rmse: 1.1079\n",
      "Epoch 1357/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.5243 - rmse: 1.1451 - val_loss: 2.1362 - val_rmse: 1.1123\n",
      "Epoch 1358/1500\n",
      "22470/22470 [==============================] - 16s 713us/sample - loss: 1.5063 - rmse: 1.1380 - val_loss: 2.0920 - val_rmse: 1.1095\n",
      "Epoch 1359/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.5316 - rmse: 1.1485 - val_loss: 2.1244 - val_rmse: 1.1130\n",
      "Epoch 1360/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.5194 - rmse: 1.1434 - val_loss: 2.1207 - val_rmse: 1.1104\n",
      "Epoch 1361/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.5165 - rmse: 1.1426 - val_loss: 2.0749 - val_rmse: 1.1010\n",
      "Epoch 1362/1500\n",
      "22470/22470 [==============================] - 16s 718us/sample - loss: 1.5230 - rmse: 1.1440 - val_loss: 2.0723 - val_rmse: 1.0990\n",
      "Epoch 1363/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.5151 - rmse: 1.1417 - val_loss: 2.0591 - val_rmse: 1.1116\n",
      "Epoch 1364/1500\n",
      "22470/22470 [==============================] - 16s 718us/sample - loss: 1.5114 - rmse: 1.1403 - val_loss: 2.1406 - val_rmse: 1.1137\n",
      "Epoch 1365/1500\n",
      "22470/22470 [==============================] - 16s 713us/sample - loss: 1.5127 - rmse: 1.1396 - val_loss: 2.1685 - val_rmse: 1.1199\n",
      "Epoch 1366/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.5020 - rmse: 1.1367 - val_loss: 2.2568 - val_rmse: 1.1352\n",
      "Epoch 1367/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.4988 - rmse: 1.1361 - val_loss: 2.2959 - val_rmse: 1.1466\n",
      "Epoch 1368/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.5148 - rmse: 1.1417 - val_loss: 2.2051 - val_rmse: 1.1261\n",
      "Epoch 1369/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.5250 - rmse: 1.1463 - val_loss: 2.1603 - val_rmse: 1.1161\n",
      "Epoch 1370/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.5301 - rmse: 1.1460 - val_loss: 2.1949 - val_rmse: 1.1221\n",
      "Epoch 1371/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.5066 - rmse: 1.1391 - val_loss: 2.2604 - val_rmse: 1.1372\n",
      "Epoch 1372/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.5093 - rmse: 1.1403 - val_loss: 2.1039 - val_rmse: 1.1022\n",
      "Epoch 1373/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.4978 - rmse: 1.1372 - val_loss: 2.2061 - val_rmse: 1.1205\n",
      "Epoch 1374/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.4957 - rmse: 1.1358 - val_loss: 2.1093 - val_rmse: 1.1054\n",
      "Epoch 1375/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.5001 - rmse: 1.1364 - val_loss: 2.1375 - val_rmse: 1.1072\n",
      "Epoch 1376/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.5041 - rmse: 1.1375 - val_loss: 2.0988 - val_rmse: 1.1015\n",
      "Epoch 1377/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.5290 - rmse: 1.1459 - val_loss: 2.2076 - val_rmse: 1.1213\n",
      "Epoch 1378/1500\n",
      "22470/22470 [==============================] - 16s 721us/sample - loss: 1.5260 - rmse: 1.1447 - val_loss: 2.0935 - val_rmse: 1.0987\n",
      "Epoch 1379/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.5127 - rmse: 1.1393 - val_loss: 2.1917 - val_rmse: 1.1483\n",
      "Epoch 1380/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.5039 - rmse: 1.1379 - val_loss: 2.1613 - val_rmse: 1.1117\n",
      "Epoch 1381/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.5089 - rmse: 1.1399 - val_loss: 2.3137 - val_rmse: 1.1435\n",
      "Epoch 1382/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.5254 - rmse: 1.1459 - val_loss: 2.2192 - val_rmse: 1.1369\n",
      "Epoch 1383/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.5141 - rmse: 1.1410 - val_loss: 2.2103 - val_rmse: 1.1260\n",
      "Epoch 1384/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.4928 - rmse: 1.1329 - val_loss: 2.1147 - val_rmse: 1.1199\n",
      "Epoch 1385/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.5141 - rmse: 1.1408 - val_loss: 2.1491 - val_rmse: 1.1153\n",
      "Epoch 1386/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.4891 - rmse: 1.1320 - val_loss: 2.2940 - val_rmse: 1.1472\n",
      "Epoch 1387/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.4969 - rmse: 1.1362 - val_loss: 2.1573 - val_rmse: 1.1148\n",
      "Epoch 1388/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.4992 - rmse: 1.1356 - val_loss: 2.3111 - val_rmse: 1.1480\n",
      "Epoch 1389/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.5084 - rmse: 1.1393 - val_loss: 2.1267 - val_rmse: 1.1046\n",
      "Epoch 1390/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.5133 - rmse: 1.1407 - val_loss: 2.3476 - val_rmse: 1.1586\n",
      "Epoch 1391/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.5048 - rmse: 1.1359 - val_loss: 2.2158 - val_rmse: 1.1228\n",
      "Epoch 1392/1500\n",
      "22470/22470 [==============================] - 16s 718us/sample - loss: 1.4995 - rmse: 1.1375 - val_loss: 2.1147 - val_rmse: 1.1003\n",
      "Epoch 1393/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.4940 - rmse: 1.1341 - val_loss: 2.0787 - val_rmse: 1.1020\n",
      "Epoch 1394/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.5140 - rmse: 1.1413 - val_loss: 2.1905 - val_rmse: 1.1200\n",
      "Epoch 1395/1500\n",
      "22470/22470 [==============================] - 16s 712us/sample - loss: 1.5029 - rmse: 1.1377 - val_loss: 2.1829 - val_rmse: 1.1178\n",
      "Epoch 1396/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.5000 - rmse: 1.1357 - val_loss: 2.3501 - val_rmse: 1.1618\n",
      "Epoch 1397/1500\n",
      "22470/22470 [==============================] - 16s 713us/sample - loss: 1.5046 - rmse: 1.1366 - val_loss: 2.1681 - val_rmse: 1.1127\n",
      "Epoch 1398/1500\n",
      "22470/22470 [==============================] - 16s 713us/sample - loss: 1.5015 - rmse: 1.1356 - val_loss: 2.1263 - val_rmse: 1.1069\n",
      "Epoch 1399/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.5070 - rmse: 1.1369 - val_loss: 2.2254 - val_rmse: 1.1303\n",
      "Epoch 1400/1500\n",
      "22470/22470 [==============================] - 16s 721us/sample - loss: 1.5056 - rmse: 1.1385 - val_loss: 2.0898 - val_rmse: 1.0961\n",
      "Epoch 1401/1500\n",
      "22470/22470 [==============================] - 16s 723us/sample - loss: 1.4958 - rmse: 1.1336 - val_loss: 2.2827 - val_rmse: 1.1384\n",
      "Epoch 1402/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.4982 - rmse: 1.1351 - val_loss: 2.1389 - val_rmse: 1.1046\n",
      "Epoch 1403/1500\n",
      "22470/22470 [==============================] - 16s 719us/sample - loss: 1.5095 - rmse: 1.1387 - val_loss: 2.0728 - val_rmse: 1.0941\n",
      "Epoch 1404/1500\n",
      "22470/22470 [==============================] - 16s 718us/sample - loss: 1.4954 - rmse: 1.1336 - val_loss: 2.2931 - val_rmse: 1.1400\n",
      "Epoch 1405/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.4911 - rmse: 1.1335 - val_loss: 2.1711 - val_rmse: 1.1147\n",
      "Epoch 1406/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.4887 - rmse: 1.1323 - val_loss: 2.1010 - val_rmse: 1.1088\n",
      "Epoch 1407/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.4921 - rmse: 1.1330 - val_loss: 2.1809 - val_rmse: 1.1156\n",
      "Epoch 1408/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.5038 - rmse: 1.1356 - val_loss: 2.1930 - val_rmse: 1.1172\n",
      "Epoch 1409/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.4884 - rmse: 1.1309 - val_loss: 2.2219 - val_rmse: 1.1316\n",
      "Epoch 1410/1500\n",
      "22470/22470 [==============================] - 16s 713us/sample - loss: 1.5011 - rmse: 1.1357 - val_loss: 2.2090 - val_rmse: 1.1187\n",
      "Epoch 1411/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.4930 - rmse: 1.1321 - val_loss: 2.1847 - val_rmse: 1.1188\n",
      "Epoch 1412/1500\n",
      "22470/22470 [==============================] - 16s 721us/sample - loss: 1.4925 - rmse: 1.1329 - val_loss: 2.0424 - val_rmse: 1.0858\n",
      "Epoch 1413/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.4785 - rmse: 1.1298 - val_loss: 2.1543 - val_rmse: 1.1111\n",
      "Epoch 1414/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.4917 - rmse: 1.1323 - val_loss: 2.0831 - val_rmse: 1.1011\n",
      "Epoch 1415/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.4880 - rmse: 1.1317 - val_loss: 2.2271 - val_rmse: 1.1249\n",
      "Epoch 1416/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.5002 - rmse: 1.1356 - val_loss: 2.2491 - val_rmse: 1.1313\n",
      "Epoch 1417/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.5015 - rmse: 1.1348 - val_loss: 2.1214 - val_rmse: 1.1049\n",
      "Epoch 1418/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.4819 - rmse: 1.1302 - val_loss: 2.1370 - val_rmse: 1.1044\n",
      "Epoch 1419/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.5021 - rmse: 1.1380 - val_loss: 2.2237 - val_rmse: 1.1203\n",
      "Epoch 1420/1500\n",
      "22470/22470 [==============================] - 16s 719us/sample - loss: 1.5119 - rmse: 1.1393 - val_loss: 2.1431 - val_rmse: 1.1049\n",
      "Epoch 1421/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.4845 - rmse: 1.1297 - val_loss: 2.2454 - val_rmse: 1.1455\n",
      "Epoch 1422/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.4924 - rmse: 1.1334 - val_loss: 2.0862 - val_rmse: 1.0970\n",
      "Epoch 1423/1500\n",
      "22470/22470 [==============================] - 16s 713us/sample - loss: 1.4892 - rmse: 1.1306 - val_loss: 2.1692 - val_rmse: 1.1159\n",
      "Epoch 1424/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.4907 - rmse: 1.1339 - val_loss: 2.2016 - val_rmse: 1.1211\n",
      "Epoch 1425/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.4963 - rmse: 1.1345 - val_loss: 2.1814 - val_rmse: 1.1135\n",
      "Epoch 1426/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.4981 - rmse: 1.1340 - val_loss: 2.3238 - val_rmse: 1.1481\n",
      "Epoch 1427/1500\n",
      "22470/22470 [==============================] - 16s 713us/sample - loss: 1.4854 - rmse: 1.1297 - val_loss: 2.1963 - val_rmse: 1.1170\n",
      "Epoch 1428/1500\n",
      "22470/22470 [==============================] - 16s 718us/sample - loss: 1.4905 - rmse: 1.1326 - val_loss: 2.1278 - val_rmse: 1.0982\n",
      "Epoch 1429/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.4995 - rmse: 1.1339 - val_loss: 2.2060 - val_rmse: 1.1177\n",
      "Epoch 1430/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.5057 - rmse: 1.1357 - val_loss: 2.1581 - val_rmse: 1.1102\n",
      "Epoch 1431/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.4884 - rmse: 1.1314 - val_loss: 2.3135 - val_rmse: 1.1470\n",
      "Epoch 1432/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.4907 - rmse: 1.1310 - val_loss: 2.1126 - val_rmse: 1.0996\n",
      "Epoch 1433/1500\n",
      "22470/22470 [==============================] - 16s 713us/sample - loss: 1.4835 - rmse: 1.1278 - val_loss: 2.1994 - val_rmse: 1.1229\n",
      "Epoch 1434/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.4907 - rmse: 1.1313 - val_loss: 2.3171 - val_rmse: 1.1426\n",
      "Epoch 1435/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.4859 - rmse: 1.1291 - val_loss: 2.0820 - val_rmse: 1.0965\n",
      "Epoch 1436/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.4911 - rmse: 1.1315 - val_loss: 2.2777 - val_rmse: 1.1382\n",
      "Epoch 1437/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.4942 - rmse: 1.1326 - val_loss: 2.2435 - val_rmse: 1.1267\n",
      "Epoch 1438/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.4789 - rmse: 1.1284 - val_loss: 2.3054 - val_rmse: 1.1488\n",
      "Epoch 1439/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.4758 - rmse: 1.1264 - val_loss: 2.2238 - val_rmse: 1.1234\n",
      "Epoch 1440/1500\n",
      "22470/22470 [==============================] - 16s 723us/sample - loss: 1.4851 - rmse: 1.1282 - val_loss: 2.0130 - val_rmse: 1.0826\n",
      "Epoch 1441/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.4864 - rmse: 1.1285 - val_loss: 2.2588 - val_rmse: 1.1289\n",
      "Epoch 1442/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.4803 - rmse: 1.1274 - val_loss: 2.2575 - val_rmse: 1.1381\n",
      "Epoch 1443/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.4857 - rmse: 1.1294 - val_loss: 2.0617 - val_rmse: 1.0967\n",
      "Epoch 1444/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.4835 - rmse: 1.1287 - val_loss: 2.0849 - val_rmse: 1.0945\n",
      "Epoch 1445/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.4836 - rmse: 1.1287 - val_loss: 2.1653 - val_rmse: 1.1149\n",
      "Epoch 1446/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.4849 - rmse: 1.1305 - val_loss: 2.2950 - val_rmse: 1.1411\n",
      "Epoch 1447/1500\n",
      "22470/22470 [==============================] - 16s 713us/sample - loss: 1.4864 - rmse: 1.1289 - val_loss: 2.2579 - val_rmse: 1.1426\n",
      "Epoch 1448/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.4905 - rmse: 1.1312 - val_loss: 2.0650 - val_rmse: 1.0978\n",
      "Epoch 1449/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.4844 - rmse: 1.1304 - val_loss: 2.1858 - val_rmse: 1.1138\n",
      "Epoch 1450/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.4942 - rmse: 1.1309 - val_loss: 2.1815 - val_rmse: 1.1166\n",
      "Epoch 1451/1500\n",
      "22470/22470 [==============================] - 16s 713us/sample - loss: 1.4746 - rmse: 1.1254 - val_loss: 2.3096 - val_rmse: 1.1450\n",
      "Epoch 1452/1500\n",
      "22470/22470 [==============================] - 16s 718us/sample - loss: 1.4862 - rmse: 1.1296 - val_loss: 2.0647 - val_rmse: 1.0909\n",
      "Epoch 1453/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.4726 - rmse: 1.1251 - val_loss: 2.1069 - val_rmse: 1.0974\n",
      "Epoch 1454/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.4814 - rmse: 1.1277 - val_loss: 2.2994 - val_rmse: 1.1378\n",
      "Epoch 1455/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.4652 - rmse: 1.1234 - val_loss: 2.1341 - val_rmse: 1.1063\n",
      "Epoch 1456/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.4636 - rmse: 1.1228 - val_loss: 2.2936 - val_rmse: 1.1317\n",
      "Epoch 1457/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.4786 - rmse: 1.1268 - val_loss: 2.1383 - val_rmse: 1.0968\n",
      "Epoch 1458/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.4798 - rmse: 1.1266 - val_loss: 2.1653 - val_rmse: 1.1077\n",
      "Epoch 1459/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.4846 - rmse: 1.1287 - val_loss: 2.3881 - val_rmse: 1.1588\n",
      "Epoch 1460/1500\n",
      "22470/22470 [==============================] - 16s 718us/sample - loss: 1.4643 - rmse: 1.1223 - val_loss: 2.1680 - val_rmse: 1.1097\n",
      "Epoch 1461/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.4858 - rmse: 1.1294 - val_loss: 2.2298 - val_rmse: 1.1203\n",
      "Epoch 1462/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.4806 - rmse: 1.1265 - val_loss: 2.2236 - val_rmse: 1.1213\n",
      "Epoch 1463/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.4696 - rmse: 1.1234 - val_loss: 2.1722 - val_rmse: 1.1114\n",
      "Epoch 1464/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.4892 - rmse: 1.1314 - val_loss: 2.3242 - val_rmse: 1.1520\n",
      "Epoch 1465/1500\n",
      "22470/22470 [==============================] - 16s 719us/sample - loss: 1.4667 - rmse: 1.1220 - val_loss: 2.1328 - val_rmse: 1.0978\n",
      "Epoch 1466/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.4691 - rmse: 1.1228 - val_loss: 2.2372 - val_rmse: 1.1254\n",
      "Epoch 1467/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.4804 - rmse: 1.1276 - val_loss: 2.3276 - val_rmse: 1.1425\n",
      "Epoch 1468/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.4737 - rmse: 1.1252 - val_loss: 2.0551 - val_rmse: 1.0830\n",
      "Epoch 1469/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.4742 - rmse: 1.1256 - val_loss: 2.3472 - val_rmse: 1.1495\n",
      "Epoch 1470/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.4787 - rmse: 1.1259 - val_loss: 2.4455 - val_rmse: 1.1700\n",
      "Epoch 1471/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.4732 - rmse: 1.1251 - val_loss: 2.2883 - val_rmse: 1.1377\n",
      "Epoch 1472/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.4804 - rmse: 1.1262 - val_loss: 2.1975 - val_rmse: 1.1297\n",
      "Epoch 1473/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.4834 - rmse: 1.1280 - val_loss: 2.1316 - val_rmse: 1.0979\n",
      "Epoch 1474/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.4672 - rmse: 1.1231 - val_loss: 2.2229 - val_rmse: 1.1186\n",
      "Epoch 1475/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.4898 - rmse: 1.1297 - val_loss: 2.1602 - val_rmse: 1.1054\n",
      "Epoch 1476/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.4772 - rmse: 1.1263 - val_loss: 2.0474 - val_rmse: 1.0833\n",
      "Epoch 1477/1500\n",
      "22470/22470 [==============================] - 16s 718us/sample - loss: 1.4623 - rmse: 1.1209 - val_loss: 2.0431 - val_rmse: 1.0800\n",
      "Epoch 1478/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.4843 - rmse: 1.1280 - val_loss: 2.0673 - val_rmse: 1.0848\n",
      "Epoch 1479/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.4525 - rmse: 1.1168 - val_loss: 2.0992 - val_rmse: 1.0863\n",
      "Epoch 1480/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.4955 - rmse: 1.1316 - val_loss: 2.0842 - val_rmse: 1.0901\n",
      "Epoch 1481/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.4735 - rmse: 1.1256 - val_loss: 2.0574 - val_rmse: 1.0878\n",
      "Epoch 1482/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.4614 - rmse: 1.1195 - val_loss: 2.2645 - val_rmse: 1.1327\n",
      "Epoch 1483/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.4591 - rmse: 1.1190 - val_loss: 2.2556 - val_rmse: 1.1213\n",
      "Epoch 1484/1500\n",
      "22470/22470 [==============================] - 16s 716us/sample - loss: 1.4684 - rmse: 1.1238 - val_loss: 2.2575 - val_rmse: 1.1266\n",
      "Epoch 1485/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.4811 - rmse: 1.1253 - val_loss: 2.1442 - val_rmse: 1.1024\n",
      "Epoch 1486/1500\n",
      "22470/22470 [==============================] - 16s 729us/sample - loss: 1.4688 - rmse: 1.1221 - val_loss: 2.1879 - val_rmse: 1.1114\n",
      "Epoch 1487/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.4710 - rmse: 1.1239 - val_loss: 2.2586 - val_rmse: 1.1254\n",
      "Epoch 1488/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.4717 - rmse: 1.1240 - val_loss: 2.2790 - val_rmse: 1.1316\n",
      "Epoch 1489/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.4699 - rmse: 1.1233 - val_loss: 2.1084 - val_rmse: 1.1018\n",
      "Epoch 1490/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.4537 - rmse: 1.1171 - val_loss: 2.2566 - val_rmse: 1.1281\n",
      "Epoch 1491/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.4792 - rmse: 1.1253 - val_loss: 2.3229 - val_rmse: 1.1413\n",
      "Epoch 1492/1500\n",
      "22470/22470 [==============================] - 16s 720us/sample - loss: 1.4624 - rmse: 1.1197 - val_loss: 2.0212 - val_rmse: 1.0759\n",
      "Epoch 1493/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.4715 - rmse: 1.1226 - val_loss: 2.2931 - val_rmse: 1.1290\n",
      "Epoch 1494/1500\n",
      "22470/22470 [==============================] - 16s 718us/sample - loss: 1.4626 - rmse: 1.1203 - val_loss: 2.1473 - val_rmse: 1.1046\n",
      "Epoch 1495/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.4709 - rmse: 1.1228 - val_loss: 2.2168 - val_rmse: 1.1257\n",
      "Epoch 1496/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.4727 - rmse: 1.1236 - val_loss: 2.3155 - val_rmse: 1.1416\n",
      "Epoch 1497/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.4584 - rmse: 1.1186 - val_loss: 2.2300 - val_rmse: 1.1317\n",
      "Epoch 1498/1500\n",
      "22470/22470 [==============================] - 16s 717us/sample - loss: 1.4701 - rmse: 1.1234 - val_loss: 2.1154 - val_rmse: 1.0937\n",
      "Epoch 1499/1500\n",
      "22470/22470 [==============================] - 16s 714us/sample - loss: 1.4709 - rmse: 1.1214 - val_loss: 2.3302 - val_rmse: 1.1458\n",
      "Epoch 1500/1500\n",
      "22470/22470 [==============================] - 16s 715us/sample - loss: 1.4700 - rmse: 1.1212 - val_loss: 2.1994 - val_rmse: 1.1139\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "USE_SAVED_MODEL = False\n",
    "\n",
    "if USE_SAVED_MODEL == False:\n",
    "    history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs = 1500,\n",
    "                    batch_size = 256,\n",
    "                    validation_split = 0.2, #data = (x_test, y_test),\n",
    "                    callbacks = callbacks\n",
    "                    )\n",
    "    plt.style.use('seaborn-darkgrid')\n",
    "    plt.plot(history.history['rmse'])\n",
    "    \n",
    "     #save history as JSON file\n",
    "    with open(history_dir +'history.json', 'w') as f:\n",
    "        json.dump(str(history.history), f)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Root Mean Square Error')\n",
    "    plt.title('Model Training Error')\n",
    "    plt.show() \n",
    "    \n",
    "else:\n",
    "    model.load_weights(model_dir+\"final_model_weights.h5\")\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookid_dir = '../input/IdLookupTable.csv'\n",
    "lookid_data = pd.read_csv(lookid_dir)\n",
    "test_data = pd.read_csv(test_file)\n",
    "\n",
    "x_test = []\n",
    "for i in range(0,len(test_data)):\n",
    "    img = test_data['Image'][i].split(' ')\n",
    "    x_test.append(img)\n",
    "    \n",
    "x_test = np.array(x_test,dtype = 'float')\n",
    "x_test = x_test/255.0\n",
    "x_test = x_test.reshape(-1,96,96,1)    \n",
    "\n",
    "y_test = model.predict(x_test)\n",
    "y_test = np.clip(y_test,0,96)\n",
    "\n",
    "lookid_list = list(lookid_data['FeatureName'])\n",
    "imageID = list(lookid_data['ImageId']-1)\n",
    "pred_list = list(y_test)\n",
    "\n",
    "rowid = list(lookid_data['RowId'])\n",
    "\n",
    "feature = []\n",
    "for f in list(lookid_data['FeatureName']):\n",
    "    feature.append(lookid_list.index(f))\n",
    "    \n",
    "    \n",
    "submit_data = []\n",
    "for x,y in zip(imageID,feature):\n",
    "    submit_data.append(pred_list[x][y])\n",
    "rowid = pd.Series(rowid,name = 'RowId')\n",
    "loc = pd.Series(submit_data,name = 'Location')\n",
    "submission = pd.concat([rowid,loc],axis = 1)\n",
    "submission.to_csv('../output/w207_temp_submission.csv',index = False)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "    sess = get_session()\n",
    "    clear_session()\n",
    "    sess.close()\n",
    " \n",
    "    try:\n",
    "        del model # this is from global space - change this as you need\n",
    "    except:\n",
    "        print(\"Model clear Failed\")\n",
    "    print(gc.collect())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
