{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4dbced162440c393a0a5b7e5aee344711a30e994"
   },
   "source": [
    "# Facial Keypoint Detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "369fa247a546e39d82bdfdc5b7d4ed58baa40e4f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/user1/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/user1/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/user1/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/user1/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/user1/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/user1/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.backend.tensorflow_backend import clear_session\n",
    "from keras.backend.tensorflow_backend import get_session\n",
    "from tensorflow.keras.layers import LeakyReLU, ReLU\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Activation, Convolution2D, BatchNormalization, Flatten\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, MaxPool2D\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "from keras import backend\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import cv2\n",
    "import os, gc, json, math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n",
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print(keras.__version__)\n",
    "import tensorflow\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pixels(data):\n",
    "    \"\"\"\n",
    "    Convert pixels to the right intensity 0-1 and in a square matrix.\n",
    "    \"\"\"\n",
    "    data = np.array([row.split(' ') for row in data['Image']],dtype='float') / 255.0\n",
    "    data = data.reshape(-1,96,96,1)\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_img(sample_img,coord=None):\n",
    "    \"\"\"\n",
    "    Display an image. For debugging, display a coordinate on the image.\n",
    "    input:\n",
    "        - sample_img: numpy array. image to be displayed\n",
    "        - coord: lst. of coordinates in form [[x_coordinate,y_coordinate],[x_coordinate,y_coordinate]]\n",
    "    TODO handle multiple coordinates. Work out bugs with multiple coordinates\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.imshow(sample_img.reshape(96,96),cmap='gray')\n",
    "    if coord is not None:\n",
    "        plt.scatter(coord[0],coord[1],marker = '*',c='r')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_facial_keypoints(data,ind):\n",
    "    \"\"\"\n",
    "    Structure the coordinates for all facial keypoints for a single image.\n",
    "    inputs:\n",
    "        - data: numpy array containing rows as each image sample and columns as facial keypoint coordinates\n",
    "        - ind: index of the image\n",
    "    output:\n",
    "        - numpy array with format [[list of x-coordinates],[list of y-coordinates]]\n",
    "    \"\"\"\n",
    "    data[ind]\n",
    "    it = iter(data[ind])\n",
    "    x_coord = []\n",
    "    y_coord = []\n",
    "\n",
    "    for x in it:\n",
    "        x_coord.append(x)\n",
    "        y_coord.append(next(it))\n",
    "    \n",
    "    return(np.array([x_coord,y_coord]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "# Reset Keras Session\n",
    "def reset_keras():\n",
    "    sess = get_session()\n",
    "    clear_session()\n",
    "    sess.close()\n",
    "    sess = get_session()\n",
    "\n",
    "    try:\n",
    "        del model # this is from global space - change this as you need\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    print(gc.collect()) # if it's done something you should see a number being outputted\n",
    "\n",
    "    # use the same config as you used to create the session\n",
    "    config = tensorflow.ConfigProto()\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = 1\n",
    "    config.gpu_options.visible_device_list = \"0\"\n",
    "    set_session(tensorflow.Session(config=config))\n",
    "    \n",
    "reset_keras()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "fa1b76273d02502e3fd668dddf74ecf522044524"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('../output/'):\n",
    "    os.makedirs('../output/model')\n",
    "    os.makedirs('../output/history')\n",
    "    \n",
    "    \n",
    "model_dir = \"../output/model/\"\n",
    "history_dir = \"../output/history/\"\n",
    "\n",
    "train_file = '../input/training/training.csv'\n",
    "test_file = '../input/test/test.csv'\n",
    "train_data = pd.read_csv(train_file)  \n",
    "#test_data = pd.read_csv(test_file)\n",
    "\n",
    "\n",
    "bad_samples = [1747, 1731, 1877, 1881, 1979, 2199, 2289, 2321, 2453, 3173, 3296, 3447, 4180, 6859,\n",
    "              2090, 2175, 1907, 2562, 2818, 3296, 3447, 4263, 4482, 4490, 4636, 5059, 6493, 6585, 6906]\n",
    "\n",
    "train_data = train_data.drop(bad_samples).reset_index(drop=True)\n",
    "train_clean = train_data.dropna()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e1b88f1528838c0a8fec61f9a02a70b5077312e9"
   },
   "source": [
    "Create training vector with images and normalize thee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = convert_pixels(train_data)\n",
    "x_clean = convert_pixels(train_clean)\n",
    "y_train = train_data[[col for col in train_data.columns if col != 'Image']].to_numpy()\n",
    "y_clean = train_clean[[col for col in train_clean.columns if col != 'Image']].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a833f4cc5e559774d3a310fd09d40d31e49e71da"
   },
   "source": [
    "Generate labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "e9d804a035809cdf8ffda19f41ce3feb278a38fb"
   },
   "outputs": [],
   "source": [
    "\n",
    "#imputer = KNNImputer(n_neighbors=3, weights='distance')\n",
    "\n",
    "imputer = IterativeImputer(max_iter=1000, tol=0.01, random_state=42)\n",
    "y_train = imputer.fit_transform(y_train)\n",
    "\n",
    "\n",
    "\n",
    "bad_bottom_lip = [210, 350, 499, 512, 810, 839, 895, 1058, 1194,1230, 1245, 1546, 1548]\n",
    "\n",
    "def adjust_mouth_coord(y_train):\n",
    "    \n",
    "\n",
    "    for sample in bad_bottom_lip:\n",
    "        y_train[sample][29] = 94\n",
    "        y_train[sample][28] = y_train[sample][26]\n",
    "          \n",
    "    \n",
    "    \n",
    "    for sample in range(len(y_train)):\n",
    "        if(y_train[sample][29] < y_train[sample][27]+1):\n",
    "             y_train[sample][27] = y_train[sample][29] -1\n",
    "   \n",
    "        if((y_train[sample][23] + y_train[sample][25]) > (2*y_train[sample][29]-1)):\n",
    "             diff = y_train[sample][23] - y_train[sample][25]\n",
    "       \n",
    "             if(diff > 0):\n",
    "                y_train[sample][23] = y_train[sample][29] -1\n",
    "                y_train[sample][25] = y_train[sample][29] -1 - diff\n",
    "           \n",
    "             else:\n",
    "                y_train[sample][23] = y_train[sample][29] -1 + diff\n",
    "                y_train[sample][25] = y_train[sample][29] -1 \n",
    "                \n",
    "    return(y_train)           \n",
    "\n",
    "y_train = adjust_mouth_coord(y_train)    \n",
    " \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set feature engineering parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_na = False\n",
    "add_flip_horiz = True\n",
    "add_blur_img = False\n",
    "add_rotate_img = False\n",
    "add_contrast_img = True\n",
    "add_translate_img = False\n",
    "orig_x_train = x_train.copy()\n",
    "orig_y_train = y_train.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill NA in the training labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if fill_na:\n",
    "    # https://stackoverflow.com/questions/18689235/numpy-array-replace-nan-values-with-average-of-columns\n",
    "    # get column means\n",
    "    col_mean = np.nanmean(y_train,axis=0)\n",
    "\n",
    "    # find the x,y indices that are missing from y_train\n",
    "    inds = np.where(np.isnan(y_train))\n",
    "\n",
    "    # fill in missing values in y_train with the column means. \"take\" is much more efficient than fancy indexing\n",
    "    y_train[inds] = np.take(col_mean, inds[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flip images horizontally and add to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_img_horiz(train_data):\n",
    "    \"\"\"\n",
    "    Flip images horizontally for all training images\n",
    "    \"\"\"\n",
    "    # Flip images\n",
    "    x_train = convert_pixels(train_data)\n",
    "    flip_img = np.array([np.fliplr(x_train[[ind]][0]) for ind in range(x_train.shape[0])])\n",
    "    \n",
    "    # Flip coordinates\n",
    "    train_data_flip = train_data.copy()\n",
    "    x_columns = [col for col in train_data.columns if '_x' in col]\n",
    "    train_data_flip[x_columns] = train_data[x_columns].applymap(lambda x: 96-x)\n",
    "    \n",
    "    #left and right are swapped so undo\n",
    "    left_columns = [col for col in train_data.columns if 'left' in col]\n",
    "    right_columns = [col for col in train_data.columns if 'right' in col]\n",
    "    train_data_flip[left_columns+right_columns] = train_data_flip[right_columns+left_columns]\n",
    "    \n",
    "    flip_coord = train_data_flip[[col for col in train_data if col != 'Image']].to_numpy()\n",
    "    return(flip_img,flip_coord)\n",
    "\n",
    "if add_flip_horiz:\n",
    "    # Apply the augmentation and add the new data to the training set\n",
    "    flipped_img,flipped_coord = flip_img_horiz(train_data)\n",
    "    \n",
    "    imputer = IterativeImputer(max_iter=1000, tol=0.01, random_state=3)\n",
    "    flipped_coord = imputer.fit_transform(flipped_coord)\n",
    "    flipped_coord = adjust_mouth_coord(flipped_coord)\n",
    "    x_train = np.append(x_train,flipped_img,axis=0)\n",
    "    y_train = np.append(y_train,flipped_coord,axis=0)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Gaussian blurring with a 5x5 filter with $\\sigma$ = 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blur_img():\n",
    "    \"\"\"\n",
    "    Add Gaussian blurring to the images\n",
    "    \"\"\"\n",
    "    # https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_filtering/py_filtering.html\n",
    "    blur_img = np.array([cv2.GaussianBlur(orig_x_train[[ind]][0],(5,5),2).reshape(96,96,1) for ind in range(orig_x_train.shape[0])])\n",
    "    \n",
    "    return(blur_img)\n",
    "\n",
    "if add_blur_img:\n",
    "    x_train = np.append(x_train,blur_img(),axis=0)\n",
    "    y_train = np.append(y_train,orig_y_train,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add image rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_img(x_train, y_train):\n",
    "    \"\"\"\"\n",
    "    Rotate images by angles between [5, 10, 14 degrees]\n",
    "    \"\"\"\n",
    "    angles = [5, -5, 10, -10, 14, -14]\n",
    "    b = np.ones((1,3))\n",
    "    rows,cols = (96,96)\n",
    "    x_train_rot = []\n",
    "    y_train_rot = y_train.copy()\n",
    "    M_angles = [cv2.getRotationMatrix2D((cols/2,rows/2),angle,1) for angle in angles]\n",
    "    \n",
    "    for i in range(x_train.shape[0]):\n",
    "        #M = cv2.getRotationMatrix2D((cols/2,rows/2),np.random.choice(angles,1),1)\n",
    "        M = M_angles[np.random.choice(len(M_angles))]\n",
    "        x_train_rot.append((cv2.warpAffine(x_train[[i]].reshape(rows,cols,1),M,(cols,rows)).reshape(96,96,1)))\n",
    "       \n",
    "        #apply affine transformation to (x,y) labels\n",
    "        for j in range(int(y_train.shape[1]/2)):\n",
    "            b[:,0:2] = y_train[i,2*j:2*j+2]\n",
    "            y_train_rot[i,2*j:2*j+2] = np.dot(b,M.transpose()) \n",
    "    \n",
    "    x_train_rot = np.array(x_train_rot)\n",
    "    return x_train_rot, y_train_rot\n",
    "\n",
    "if add_rotate_img:\n",
    "    \n",
    "    x_rotate, y_rotate = rotate_img(x_train,y_train)\n",
    "    x_train = np.append(x_train,x_rotate,axis=0)\n",
    "    y_train = np.append(y_train,y_rotate,axis=0)   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add image contrast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrast_img(apply_random,brightness=.5):\n",
    "    \"\"\"\n",
    "    Add brighter and darker images to the training set with the range of pixel values allowed set at 0 and 1\n",
    "    Only applies contrast to the original images. Need to make sure we're not setting random seed\n",
    "    input:\n",
    "        - brightness: float between -1 and 1\n",
    "        - apply_random: boolean. Applies random brightness to every sample in the data set.\n",
    "          Ignores brightness setting\n",
    "    \"\"\"\n",
    "    if apply_random:\n",
    "        # uses a .1 increment [-.3,.8) to pick out a brightness number for each sample\n",
    "        # numbers are chosen such that they are still realistically visible and\n",
    "        # the added training data has an appreciable change in contrast\n",
    "        brightness = np.random.choice(np.round(np.arange(-.3,.8,.1),2),size=orig_x_train.shape[0])\n",
    "        bright_img = np.array([orig_x_train[[ind]][0]+brightness[ind] for ind in range(orig_x_train.shape[0])])\n",
    "    else:\n",
    "        bright_img = orig_x_train + brightness\n",
    "\n",
    "    bright_img[bright_img > 1] = 1\n",
    "    bright_img[bright_img < 0] = 0\n",
    "    return(bright_img)\n",
    "    \n",
    "if add_contrast_img:\n",
    "    # create two sets of images undergoing contrast changes\n",
    "    x_train = np.append(x_train,contrast_img(apply_random=True),axis=0)\n",
    "    y_train = np.append(y_train,orig_y_train,axis=0)\n",
    "    \n",
    "# testing code\n",
    "# view_img(contrast_img(apply_random=True)[[30]])\n",
    "# view_img(contrast_img(apply_random=True)[[30]])\n",
    "# view_img(contrast_img(apply_random=True)[[30]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_img():\n",
    "    \"\"\"\n",
    "    Add translational shift to the images randomly\n",
    "    \"\"\"\n",
    "    trans_train_data = train_data.copy()\n",
    "    \n",
    "    rows,cols = (96,96)\n",
    "    shift_x = 96*np.random.choice(np.arange(-.4,.4,.05),size=trans_train_data.shape[0])\n",
    "    shift_y = 96*np.random.choice(np.arange(-.4,.4,.05),size=trans_train_data.shape[0])\n",
    "    M = [np.float32([[1,0,shift_x[ind]],[0,1,shift_y[ind]]]) for ind in range(trans_train_data.shape[0])]\n",
    "    trans_img = np.array([cv2.warpAffine(x_train[[ind]].reshape(96,96,1),M[ind],(cols,rows)).reshape(96,96,1) for ind in range(trans_train_data.shape[0])])\n",
    "    \n",
    "    x_col = [col for col in train_data.columns if ((col != 'Image') & ('_x' in col))]\n",
    "    y_col = [col for col in train_data.columns if ((col != 'Image') & ('_y' in col))]\n",
    "\n",
    "    shift_x_array = np.array([np.repeat(x,len(x_col))for x in shift_x])\n",
    "    shift_y_array = np.array([np.repeat(y,len(y_col))for y in shift_y])\n",
    "    \n",
    "    trans_train_data[x_col] += shift_x_array\n",
    "    trans_train_data[y_col] += shift_y_array\n",
    "    \n",
    "    trans_coord = np.array(trans_train_data[[col for col in trans_train_data.columns if col != 'Image']])\n",
    "\n",
    "    # TODO should we force these to be nan or leave as is? If leave as is, comment this out.\n",
    "    trans_coord[trans_coord > 96]= np.nan\n",
    "    trans_coord[trans_coord < 0]= np.nan\n",
    "    return(trans_img,trans_coord)\n",
    "\n",
    "if add_translate_img:\n",
    "    trans_img,trans_coord = translate_img()\n",
    "    x_train = np.append(x_train,trans_img,axis=0)\n",
    "    y_train = np.append(y_train,trans_coord,axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callback function if detailed log required\n",
    "class History(tensorflow.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.train_loss = []\n",
    "        self.train_rmse = []\n",
    "        self.val_rmse = []\n",
    "        self.val_loss = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.train_loss.append(logs.get('loss'))\n",
    "        self.train_rmse.append(logs.get('rmse'))\n",
    "        \n",
    "    def on_epoch_end(self, batch, logs={}):    \n",
    "        self.val_rmse.append(logs.get('val_rmse'))\n",
    "        self.val_loss.append(logs.get('val_loss'))\n",
    "        \n",
    "# Implement ModelCheckPoint callback function to save CNN model\n",
    "class CNN_ModelCheckpoint(tensorflow.keras.callbacks.Callback):\n",
    "\n",
    "    def __init__(self, model, filename):\n",
    "        self.filename = filename\n",
    "        self.cnn_model = model\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.max_val_rmse = math.inf\n",
    "        \n",
    " \n",
    "    def on_epoch_end(self, batch, logs={}):    \n",
    "        val_rmse = logs.get('val_rmse')\n",
    "        if(val_rmse < self.max_val_rmse):\n",
    "           self.max_val_rmse = val_rmse\n",
    "           self.cnn_model.save_weights(self.filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/user1/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/user1/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 96, 96, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 96, 96, 32)        288       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 96, 96, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_1 (Ba (None, 96, 96, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 48, 48, 64)        18432     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 48, 48, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_2 (Ba (None, 48, 48, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 48, 48, 64)        36864     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 48, 48, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_3 (Ba (None, 48, 48, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 24, 24, 96)        55296     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 24, 24, 96)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_4 (Ba (None, 24, 24, 96)        384       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 24, 24, 96)        82944     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 24, 24, 96)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_5 (Ba (None, 24, 24, 96)        384       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 12, 12, 96)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 12, 12, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 12, 12, 128)       110592    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_6 (Ba (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 12, 12, 128)       147456    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_7 (Ba (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 6, 6, 256)         294912    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_8 (Ba (None, 6, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 6, 6, 256)         589824    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_9 (Ba (None, 6, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 3, 3, 512)         1179648   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 3, 3, 512)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_10 (B (None, 3, 3, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 3, 3, 512)         2359296   \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_11 (B (None, 3, 3, 512)         2048      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               2359808   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                15390     \n",
      "=================================================================\n",
      "Total params: 7,259,326\n",
      "Trainable params: 7,255,038\n",
      "Non-trainable params: 4,288\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def final_model():\n",
    "    model_input = Input(shape=(96,96,1))\n",
    "\n",
    "    x = Convolution2D(32, (3,3), padding='same', use_bias=False)(model_input)\n",
    "    x = LeakyReLU(alpha = 0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Convolution2D(32, (3,3), padding='same', use_bias=False)(model_input)\n",
    "    x = LeakyReLU(alpha = 0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Convolution2D(64, (3,3), padding='same', use_bias=False)(x)\n",
    "    x = LeakyReLU(alpha = 0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Convolution2D(64, (3,3), padding='same', use_bias=False)(x)\n",
    "    x = LeakyReLU(alpha = 0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Convolution2D(96, (3,3), padding='same', use_bias=False)(x)\n",
    "    x = LeakyReLU(alpha = 0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Convolution2D(96, (3,3), padding='same', use_bias=False)(x)\n",
    "    x = LeakyReLU(alpha = 0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Convolution2D(128, (3,3),padding='same', use_bias=False)(x)\n",
    "    x = LeakyReLU(alpha = 0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Convolution2D(128, (3,3),padding='same', use_bias=False)(x)\n",
    "    x = LeakyReLU(alpha = 0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    #x = Dropout(0.1)(x)\n",
    "    x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Convolution2D(256, (3,3),padding='same',use_bias=False)(x)\n",
    "    x = LeakyReLU(alpha = 0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Convolution2D(256, (3,3),padding='same',use_bias=False)(x)\n",
    "    x = LeakyReLU(alpha = 0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Convolution2D(512, (3,3), padding='same', use_bias=False)(x)\n",
    "    x = LeakyReLU(alpha = 0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Convolution2D(512, (3,3), activation='relu', padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(512,activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    model_output = Dense(30)(x)\n",
    "    model = Model(model_input, model_output, name=\"final_model\")\n",
    "    return model\n",
    "\n",
    "model = final_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/user1/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/utils/losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "# Custom RMSE metric\n",
    "def rmse(y_true, y_pred):\n",
    "    return backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))\n",
    "\n",
    "#from tensorflow.keras.optimizers.schedules import InverseTimeDecay\n",
    "\n",
    "# Use Nadam optimizer with variable learning rate\n",
    "optimizer = Nadam(lr=0.00001,\n",
    "                  beta_1=0.9,\n",
    "                  beta_2=0.999,\n",
    "                  epsilon=1e-08,\n",
    "                  schedule_decay=0.004)\n",
    "\n",
    "\n",
    "# Loss: MSE and Metric = RMSE\n",
    "model.compile(optimizer= optimizer, \n",
    "              loss='mean_squared_error',\n",
    "              metrics=[rmse])\n",
    "\n",
    "#Callback to save the best model\n",
    "saveBase_Model = CNN_ModelCheckpoint(model, model_dir+\"final_model_weights.h5\")\n",
    "\n",
    "#define callback functions\n",
    "callbacks = [#EarlyStopping(monitor='val_rmse', patience=3, verbose=2),\n",
    "             saveBase_Model]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e4cf4686b410841f2e34dbb081f3429d1b0f67e9"
   },
   "source": [
    "Run for 1000 epochs and keeping 20% train-valid split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "894af9cbfcf2dca50e7407946cad318157b77d0a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16852 samples, validate on 4214 samples\n",
      "Epoch 1/1300\n",
      "16852/16852 [==============================] - 14s 820us/sample - loss: 2.1682 - rmse: 1.3865 - val_loss: 1.9043 - val_rmse: 1.1371\n",
      "Epoch 2/1300\n",
      "16852/16852 [==============================] - 12s 724us/sample - loss: 2.1574 - rmse: 1.3824 - val_loss: 1.9065 - val_rmse: 1.1419\n",
      "Epoch 3/1300\n",
      "16852/16852 [==============================] - 12s 711us/sample - loss: 2.1841 - rmse: 1.3908 - val_loss: 1.9222 - val_rmse: 1.1447\n",
      "Epoch 4/1300\n",
      "16852/16852 [==============================] - 12s 710us/sample - loss: 2.1612 - rmse: 1.3836 - val_loss: 1.9844 - val_rmse: 1.1678\n",
      "Epoch 5/1300\n",
      "16852/16852 [==============================] - 12s 710us/sample - loss: 2.1343 - rmse: 1.3756 - val_loss: 1.9544 - val_rmse: 1.1549\n",
      "Epoch 6/1300\n",
      "16852/16852 [==============================] - 12s 714us/sample - loss: 2.1573 - rmse: 1.3825 - val_loss: 1.9271 - val_rmse: 1.1460\n",
      "Epoch 7/1300\n",
      "16852/16852 [==============================] - 12s 712us/sample - loss: 2.1557 - rmse: 1.3815 - val_loss: 1.9204 - val_rmse: 1.1438\n",
      "Epoch 8/1300\n",
      "16852/16852 [==============================] - 12s 711us/sample - loss: 2.1311 - rmse: 1.3756 - val_loss: 1.9644 - val_rmse: 1.1492\n",
      "Epoch 9/1300\n",
      "16852/16852 [==============================] - 12s 709us/sample - loss: 2.1578 - rmse: 1.3803 - val_loss: 1.9199 - val_rmse: 1.1410\n",
      "Epoch 10/1300\n",
      "16852/16852 [==============================] - 12s 705us/sample - loss: 2.1530 - rmse: 1.3829 - val_loss: 1.9183 - val_rmse: 1.1377\n",
      "Epoch 11/1300\n",
      "16852/16852 [==============================] - 12s 720us/sample - loss: 2.1481 - rmse: 1.3815 - val_loss: 1.9318 - val_rmse: 1.1452\n",
      "Epoch 12/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 2.1398 - rmse: 1.3783 - val_loss: 1.9127 - val_rmse: 1.1405\n",
      "Epoch 13/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 2.1363 - rmse: 1.3760 - val_loss: 1.9052 - val_rmse: 1.1394\n",
      "Epoch 14/1300\n",
      "16852/16852 [==============================] - 12s 717us/sample - loss: 2.1551 - rmse: 1.3809 - val_loss: 1.9120 - val_rmse: 1.1359\n",
      "Epoch 15/1300\n",
      "16852/16852 [==============================] - 12s 714us/sample - loss: 2.1363 - rmse: 1.3756 - val_loss: 1.9124 - val_rmse: 1.1358\n",
      "Epoch 16/1300\n",
      "16852/16852 [==============================] - 12s 712us/sample - loss: 2.1491 - rmse: 1.3797 - val_loss: 1.9324 - val_rmse: 1.1448\n",
      "Epoch 17/1300\n",
      "16852/16852 [==============================] - 12s 707us/sample - loss: 2.1451 - rmse: 1.3785 - val_loss: 1.9192 - val_rmse: 1.1445\n",
      "Epoch 18/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 2.1531 - rmse: 1.3805 - val_loss: 1.9518 - val_rmse: 1.1503\n",
      "Epoch 19/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 2.1384 - rmse: 1.3764 - val_loss: 1.9272 - val_rmse: 1.1430\n",
      "Epoch 20/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 2.1307 - rmse: 1.3736 - val_loss: 2.0260 - val_rmse: 1.1753\n",
      "Epoch 21/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 2.1207 - rmse: 1.3708 - val_loss: 1.9075 - val_rmse: 1.1369\n",
      "Epoch 22/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 2.1495 - rmse: 1.3789 - val_loss: 1.9157 - val_rmse: 1.1405\n",
      "Epoch 23/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 2.1282 - rmse: 1.3723 - val_loss: 1.9274 - val_rmse: 1.1429\n",
      "Epoch 24/1300\n",
      "16852/16852 [==============================] - 12s 693us/sample - loss: 2.1214 - rmse: 1.3711 - val_loss: 1.9155 - val_rmse: 1.1365\n",
      "Epoch 25/1300\n",
      "16852/16852 [==============================] - 12s 700us/sample - loss: 2.1171 - rmse: 1.3692 - val_loss: 1.8816 - val_rmse: 1.1294\n",
      "Epoch 26/1300\n",
      "16852/16852 [==============================] - 12s 703us/sample - loss: 2.1308 - rmse: 1.3726 - val_loss: 1.8615 - val_rmse: 1.1194\n",
      "Epoch 27/1300\n",
      "16852/16852 [==============================] - 13s 762us/sample - loss: 2.1253 - rmse: 1.3720 - val_loss: 1.9370 - val_rmse: 1.1440\n",
      "Epoch 28/1300\n",
      "16852/16852 [==============================] - 12s 710us/sample - loss: 2.1197 - rmse: 1.3706 - val_loss: 1.8769 - val_rmse: 1.1228\n",
      "Epoch 29/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 2.1388 - rmse: 1.3746 - val_loss: 1.9043 - val_rmse: 1.1353\n",
      "Epoch 30/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 2.1209 - rmse: 1.3701 - val_loss: 1.8540 - val_rmse: 1.1200\n",
      "Epoch 31/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 2.1097 - rmse: 1.3660 - val_loss: 1.9805 - val_rmse: 1.1589\n",
      "Epoch 32/1300\n",
      "16852/16852 [==============================] - 12s 703us/sample - loss: 2.1365 - rmse: 1.3742 - val_loss: 1.9189 - val_rmse: 1.1362\n",
      "Epoch 33/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 2.1375 - rmse: 1.3734 - val_loss: 2.0351 - val_rmse: 1.1773\n",
      "Epoch 34/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 2.1201 - rmse: 1.3697 - val_loss: 1.8991 - val_rmse: 1.1316\n",
      "Epoch 35/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 2.1269 - rmse: 1.3716 - val_loss: 1.9708 - val_rmse: 1.1585\n",
      "Epoch 36/1300\n",
      "16852/16852 [==============================] - 12s 700us/sample - loss: 2.1368 - rmse: 1.3741 - val_loss: 1.9044 - val_rmse: 1.1345\n",
      "Epoch 37/1300\n",
      "16852/16852 [==============================] - 12s 700us/sample - loss: 2.0825 - rmse: 1.3588 - val_loss: 1.9255 - val_rmse: 1.1462\n",
      "Epoch 38/1300\n",
      "16852/16852 [==============================] - 12s 712us/sample - loss: 2.1085 - rmse: 1.3669 - val_loss: 1.8906 - val_rmse: 1.1284\n",
      "Epoch 39/1300\n",
      "16852/16852 [==============================] - 12s 717us/sample - loss: 2.1289 - rmse: 1.3722 - val_loss: 1.9299 - val_rmse: 1.1383\n",
      "Epoch 40/1300\n",
      "16852/16852 [==============================] - 12s 712us/sample - loss: 2.1054 - rmse: 1.3648 - val_loss: 1.8975 - val_rmse: 1.1322\n",
      "Epoch 41/1300\n",
      "16852/16852 [==============================] - 12s 716us/sample - loss: 2.0975 - rmse: 1.3633 - val_loss: 1.9262 - val_rmse: 1.1412\n",
      "Epoch 42/1300\n",
      "16852/16852 [==============================] - 12s 711us/sample - loss: 2.0952 - rmse: 1.3621 - val_loss: 1.8952 - val_rmse: 1.1310\n",
      "Epoch 43/1300\n",
      "16852/16852 [==============================] - 12s 715us/sample - loss: 2.0991 - rmse: 1.3642 - val_loss: 1.8837 - val_rmse: 1.1244\n",
      "Epoch 44/1300\n",
      "16852/16852 [==============================] - 12s 715us/sample - loss: 2.1248 - rmse: 1.3713 - val_loss: 1.9023 - val_rmse: 1.1310\n",
      "Epoch 45/1300\n",
      "16852/16852 [==============================] - 12s 703us/sample - loss: 2.0860 - rmse: 1.3599 - val_loss: 1.9124 - val_rmse: 1.1379\n",
      "Epoch 46/1300\n",
      "16852/16852 [==============================] - 12s 702us/sample - loss: 2.0809 - rmse: 1.3579 - val_loss: 1.8929 - val_rmse: 1.1294\n",
      "Epoch 47/1300\n",
      "16852/16852 [==============================] - 12s 709us/sample - loss: 2.0971 - rmse: 1.3616 - val_loss: 1.8759 - val_rmse: 1.1190\n",
      "Epoch 48/1300\n",
      "16852/16852 [==============================] - 12s 700us/sample - loss: 2.1354 - rmse: 1.3724 - val_loss: 1.8693 - val_rmse: 1.1220\n",
      "Epoch 49/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 2.1063 - rmse: 1.3641 - val_loss: 1.9020 - val_rmse: 1.1337\n",
      "Epoch 50/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 2.1098 - rmse: 1.3643 - val_loss: 1.9785 - val_rmse: 1.1585\n",
      "Epoch 51/1300\n",
      "16852/16852 [==============================] - 12s 700us/sample - loss: 2.0993 - rmse: 1.3620 - val_loss: 1.8732 - val_rmse: 1.1195\n",
      "Epoch 52/1300\n",
      "16852/16852 [==============================] - 12s 707us/sample - loss: 2.0873 - rmse: 1.3568 - val_loss: 1.9099 - val_rmse: 1.1307\n",
      "Epoch 53/1300\n",
      "16852/16852 [==============================] - 12s 702us/sample - loss: 2.0751 - rmse: 1.3559 - val_loss: 1.8998 - val_rmse: 1.1281\n",
      "Epoch 54/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 2.0729 - rmse: 1.3564 - val_loss: 1.9321 - val_rmse: 1.1388\n",
      "Epoch 55/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 2.0695 - rmse: 1.3524 - val_loss: 1.9162 - val_rmse: 1.1275\n",
      "Epoch 56/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 2.0831 - rmse: 1.3568 - val_loss: 1.9266 - val_rmse: 1.1374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 2.0946 - rmse: 1.3593 - val_loss: 1.9570 - val_rmse: 1.1443\n",
      "Epoch 58/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 2.1009 - rmse: 1.3610 - val_loss: 1.9180 - val_rmse: 1.1328\n",
      "Epoch 59/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 2.0752 - rmse: 1.3547 - val_loss: 1.8891 - val_rmse: 1.1238\n",
      "Epoch 60/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 2.0871 - rmse: 1.3574 - val_loss: 1.8830 - val_rmse: 1.1303\n",
      "Epoch 61/1300\n",
      "16852/16852 [==============================] - 12s 707us/sample - loss: 2.0761 - rmse: 1.3535 - val_loss: 1.8537 - val_rmse: 1.1178\n",
      "Epoch 62/1300\n",
      "16852/16852 [==============================] - 12s 725us/sample - loss: 2.0790 - rmse: 1.3530 - val_loss: 1.8666 - val_rmse: 1.1207\n",
      "Epoch 63/1300\n",
      "16852/16852 [==============================] - 12s 742us/sample - loss: 2.0935 - rmse: 1.3596 - val_loss: 1.9792 - val_rmse: 1.1496\n",
      "Epoch 64/1300\n",
      "16852/16852 [==============================] - 12s 693us/sample - loss: 2.0433 - rmse: 1.3446 - val_loss: 1.9007 - val_rmse: 1.1284\n",
      "Epoch 65/1300\n",
      "16852/16852 [==============================] - 12s 692us/sample - loss: 2.0801 - rmse: 1.3575 - val_loss: 1.8943 - val_rmse: 1.1238\n",
      "Epoch 66/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 2.0598 - rmse: 1.3487 - val_loss: 1.9991 - val_rmse: 1.1599\n",
      "Epoch 67/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 2.0682 - rmse: 1.3510 - val_loss: 1.9861 - val_rmse: 1.1538\n",
      "Epoch 68/1300\n",
      "16852/16852 [==============================] - 12s 716us/sample - loss: 2.0591 - rmse: 1.3481 - val_loss: 1.8582 - val_rmse: 1.1156\n",
      "Epoch 69/1300\n",
      "16852/16852 [==============================] - 12s 709us/sample - loss: 2.0634 - rmse: 1.3500 - val_loss: 1.9099 - val_rmse: 1.1327\n",
      "Epoch 70/1300\n",
      "16852/16852 [==============================] - 12s 700us/sample - loss: 2.0458 - rmse: 1.3438 - val_loss: 1.8243 - val_rmse: 1.1063\n",
      "Epoch 71/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 2.0719 - rmse: 1.3516 - val_loss: 1.8993 - val_rmse: 1.1263\n",
      "Epoch 72/1300\n",
      "16852/16852 [==============================] - 12s 707us/sample - loss: 2.0488 - rmse: 1.3465 - val_loss: 1.8338 - val_rmse: 1.1104\n",
      "Epoch 73/1300\n",
      "16852/16852 [==============================] - 12s 709us/sample - loss: 2.0601 - rmse: 1.3479 - val_loss: 1.9115 - val_rmse: 1.1463\n",
      "Epoch 74/1300\n",
      "16852/16852 [==============================] - 12s 700us/sample - loss: 2.0552 - rmse: 1.3484 - val_loss: 1.8247 - val_rmse: 1.1021\n",
      "Epoch 75/1300\n",
      "16852/16852 [==============================] - 12s 693us/sample - loss: 2.0561 - rmse: 1.3483 - val_loss: 1.8317 - val_rmse: 1.1053\n",
      "Epoch 76/1300\n",
      "16852/16852 [==============================] - 12s 711us/sample - loss: 2.0743 - rmse: 1.3548 - val_loss: 1.8713 - val_rmse: 1.1174\n",
      "Epoch 77/1300\n",
      "16852/16852 [==============================] - 12s 714us/sample - loss: 2.0430 - rmse: 1.3437 - val_loss: 1.9018 - val_rmse: 1.1249\n",
      "Epoch 78/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 2.0672 - rmse: 1.3503 - val_loss: 1.8664 - val_rmse: 1.1128\n",
      "Epoch 79/1300\n",
      "16852/16852 [==============================] - 12s 699us/sample - loss: 2.0748 - rmse: 1.3511 - val_loss: 1.9388 - val_rmse: 1.1359\n",
      "Epoch 80/1300\n",
      "16852/16852 [==============================] - 12s 707us/sample - loss: 2.0607 - rmse: 1.3490 - val_loss: 1.8368 - val_rmse: 1.1121\n",
      "Epoch 81/1300\n",
      "16852/16852 [==============================] - 12s 712us/sample - loss: 2.0570 - rmse: 1.3475 - val_loss: 1.8310 - val_rmse: 1.1099\n",
      "Epoch 82/1300\n",
      "16852/16852 [==============================] - 12s 717us/sample - loss: 2.0279 - rmse: 1.3398 - val_loss: 1.8180 - val_rmse: 1.0975\n",
      "Epoch 83/1300\n",
      "16852/16852 [==============================] - 12s 710us/sample - loss: 2.0545 - rmse: 1.3473 - val_loss: 1.8350 - val_rmse: 1.1079\n",
      "Epoch 84/1300\n",
      "16852/16852 [==============================] - 12s 713us/sample - loss: 2.0519 - rmse: 1.3437 - val_loss: 1.8781 - val_rmse: 1.1179\n",
      "Epoch 85/1300\n",
      "16852/16852 [==============================] - 12s 709us/sample - loss: 2.0563 - rmse: 1.3460 - val_loss: 1.8522 - val_rmse: 1.1093\n",
      "Epoch 86/1300\n",
      "16852/16852 [==============================] - 12s 714us/sample - loss: 2.0341 - rmse: 1.3400 - val_loss: 1.8949 - val_rmse: 1.1303\n",
      "Epoch 87/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 2.0474 - rmse: 1.3430 - val_loss: 1.8860 - val_rmse: 1.1259\n",
      "Epoch 88/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 2.0489 - rmse: 1.3451 - val_loss: 1.9651 - val_rmse: 1.1495\n",
      "Epoch 89/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 2.0595 - rmse: 1.3472 - val_loss: 1.8356 - val_rmse: 1.1068\n",
      "Epoch 90/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 2.0407 - rmse: 1.3410 - val_loss: 1.8695 - val_rmse: 1.1166\n",
      "Epoch 91/1300\n",
      "16852/16852 [==============================] - 12s 700us/sample - loss: 2.0582 - rmse: 1.3449 - val_loss: 1.9048 - val_rmse: 1.1249\n",
      "Epoch 92/1300\n",
      "16852/16852 [==============================] - 12s 693us/sample - loss: 2.0478 - rmse: 1.3444 - val_loss: 1.8494 - val_rmse: 1.1091\n",
      "Epoch 93/1300\n",
      "16852/16852 [==============================] - 12s 693us/sample - loss: 2.0394 - rmse: 1.3433 - val_loss: 1.8793 - val_rmse: 1.1146\n",
      "Epoch 94/1300\n",
      "16852/16852 [==============================] - 12s 693us/sample - loss: 2.0362 - rmse: 1.3404 - val_loss: 1.8522 - val_rmse: 1.1148\n",
      "Epoch 95/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 2.0254 - rmse: 1.3383 - val_loss: 1.8589 - val_rmse: 1.1133\n",
      "Epoch 96/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 2.0495 - rmse: 1.3420 - val_loss: 1.8779 - val_rmse: 1.1200\n",
      "Epoch 97/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 2.0342 - rmse: 1.3389 - val_loss: 1.8615 - val_rmse: 1.1095\n",
      "Epoch 98/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 2.0396 - rmse: 1.3419 - val_loss: 1.9062 - val_rmse: 1.1228\n",
      "Epoch 99/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 2.0238 - rmse: 1.3354 - val_loss: 1.8569 - val_rmse: 1.1157\n",
      "Epoch 100/1300\n",
      "16852/16852 [==============================] - 12s 709us/sample - loss: 2.0365 - rmse: 1.3389 - val_loss: 1.8863 - val_rmse: 1.1197\n",
      "Epoch 101/1300\n",
      "16852/16852 [==============================] - 12s 710us/sample - loss: 2.0135 - rmse: 1.3322 - val_loss: 1.8401 - val_rmse: 1.1096\n",
      "Epoch 102/1300\n",
      "16852/16852 [==============================] - 12s 709us/sample - loss: 2.0230 - rmse: 1.3356 - val_loss: 1.8606 - val_rmse: 1.1159\n",
      "Epoch 103/1300\n",
      "16852/16852 [==============================] - 12s 710us/sample - loss: 2.0236 - rmse: 1.3370 - val_loss: 1.8496 - val_rmse: 1.1097\n",
      "Epoch 104/1300\n",
      "16852/16852 [==============================] - 12s 709us/sample - loss: 2.0180 - rmse: 1.3332 - val_loss: 1.9483 - val_rmse: 1.1455\n",
      "Epoch 105/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 2.0266 - rmse: 1.3363 - val_loss: 1.8293 - val_rmse: 1.1053\n",
      "Epoch 106/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 2.0380 - rmse: 1.3394 - val_loss: 1.8443 - val_rmse: 1.1067\n",
      "Epoch 107/1300\n",
      "16852/16852 [==============================] - 12s 699us/sample - loss: 2.0378 - rmse: 1.3389 - val_loss: 1.8443 - val_rmse: 1.1094\n",
      "Epoch 108/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 2.0036 - rmse: 1.3298 - val_loss: 1.8335 - val_rmse: 1.1028\n",
      "Epoch 109/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 2.0230 - rmse: 1.3341 - val_loss: 1.8290 - val_rmse: 1.1062\n",
      "Epoch 110/1300\n",
      "16852/16852 [==============================] - 12s 693us/sample - loss: 2.0291 - rmse: 1.3367 - val_loss: 1.9301 - val_rmse: 1.1363\n",
      "Epoch 111/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 2.0248 - rmse: 1.3359 - val_loss: 1.8523 - val_rmse: 1.1065\n",
      "Epoch 112/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 2.0059 - rmse: 1.3296 - val_loss: 1.8694 - val_rmse: 1.1173\n",
      "Epoch 113/1300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16852/16852 [==============================] - 12s 695us/sample - loss: 2.0174 - rmse: 1.3333 - val_loss: 1.8690 - val_rmse: 1.1102\n",
      "Epoch 114/1300\n",
      "16852/16852 [==============================] - 12s 693us/sample - loss: 2.0078 - rmse: 1.3305 - val_loss: 1.8372 - val_rmse: 1.1019\n",
      "Epoch 115/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.9977 - rmse: 1.3276 - val_loss: 1.8385 - val_rmse: 1.1012\n",
      "Epoch 116/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 2.0284 - rmse: 1.3366 - val_loss: 1.8724 - val_rmse: 1.1119\n",
      "Epoch 117/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 2.0106 - rmse: 1.3312 - val_loss: 1.8752 - val_rmse: 1.1168\n",
      "Epoch 118/1300\n",
      "16852/16852 [==============================] - 12s 693us/sample - loss: 2.0180 - rmse: 1.3325 - val_loss: 1.8880 - val_rmse: 1.1141\n",
      "Epoch 119/1300\n",
      "16852/16852 [==============================] - 12s 693us/sample - loss: 1.9997 - rmse: 1.3273 - val_loss: 1.8864 - val_rmse: 1.1209\n",
      "Epoch 120/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 2.0132 - rmse: 1.3300 - val_loss: 1.8233 - val_rmse: 1.1022\n",
      "Epoch 121/1300\n",
      "16852/16852 [==============================] - 12s 693us/sample - loss: 2.0051 - rmse: 1.3282 - val_loss: 1.8263 - val_rmse: 1.0995\n",
      "Epoch 122/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 2.0172 - rmse: 1.3302 - val_loss: 1.8373 - val_rmse: 1.1021\n",
      "Epoch 123/1300\n",
      "16852/16852 [==============================] - 12s 703us/sample - loss: 1.9999 - rmse: 1.3268 - val_loss: 1.8265 - val_rmse: 1.0964\n",
      "Epoch 124/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 1.9895 - rmse: 1.3233 - val_loss: 1.8844 - val_rmse: 1.1133\n",
      "Epoch 125/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 1.9938 - rmse: 1.3246 - val_loss: 1.9402 - val_rmse: 1.1269\n",
      "Epoch 126/1300\n",
      "16852/16852 [==============================] - 12s 692us/sample - loss: 1.9981 - rmse: 1.3254 - val_loss: 1.8712 - val_rmse: 1.1116\n",
      "Epoch 127/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.9767 - rmse: 1.3202 - val_loss: 1.8469 - val_rmse: 1.1115\n",
      "Epoch 128/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 2.0204 - rmse: 1.3339 - val_loss: 1.8314 - val_rmse: 1.0989\n",
      "Epoch 129/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 1.9766 - rmse: 1.3194 - val_loss: 1.8111 - val_rmse: 1.0970\n",
      "Epoch 130/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.9876 - rmse: 1.3224 - val_loss: 1.8533 - val_rmse: 1.1096\n",
      "Epoch 131/1300\n",
      "16852/16852 [==============================] - 12s 693us/sample - loss: 1.9849 - rmse: 1.3226 - val_loss: 1.8696 - val_rmse: 1.1106\n",
      "Epoch 132/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.9830 - rmse: 1.3222 - val_loss: 1.8170 - val_rmse: 1.0979\n",
      "Epoch 133/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.9966 - rmse: 1.3256 - val_loss: 1.8242 - val_rmse: 1.0989\n",
      "Epoch 134/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.9816 - rmse: 1.3209 - val_loss: 1.8237 - val_rmse: 1.0964\n",
      "Epoch 135/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 1.9732 - rmse: 1.3167 - val_loss: 1.8502 - val_rmse: 1.1036\n",
      "Epoch 136/1300\n",
      "16852/16852 [==============================] - 12s 692us/sample - loss: 2.0079 - rmse: 1.3285 - val_loss: 1.8451 - val_rmse: 1.1074\n",
      "Epoch 137/1300\n",
      "16852/16852 [==============================] - 12s 703us/sample - loss: 1.9833 - rmse: 1.3205 - val_loss: 1.8157 - val_rmse: 1.0939\n",
      "Epoch 138/1300\n",
      "16852/16852 [==============================] - 12s 693us/sample - loss: 1.9771 - rmse: 1.3203 - val_loss: 1.9276 - val_rmse: 1.1357\n",
      "Epoch 139/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.9799 - rmse: 1.3198 - val_loss: 1.8801 - val_rmse: 1.1080\n",
      "Epoch 140/1300\n",
      "16852/16852 [==============================] - 12s 699us/sample - loss: 1.9832 - rmse: 1.3191 - val_loss: 1.8772 - val_rmse: 1.1100\n",
      "Epoch 141/1300\n",
      "16852/16852 [==============================] - 12s 704us/sample - loss: 1.9806 - rmse: 1.3207 - val_loss: 1.8360 - val_rmse: 1.0968\n",
      "Epoch 142/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 1.9718 - rmse: 1.3175 - val_loss: 1.8586 - val_rmse: 1.1034\n",
      "Epoch 143/1300\n",
      "16852/16852 [==============================] - 12s 703us/sample - loss: 1.9632 - rmse: 1.3161 - val_loss: 1.7895 - val_rmse: 1.0832\n",
      "Epoch 144/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.9863 - rmse: 1.3223 - val_loss: 1.8390 - val_rmse: 1.1014\n",
      "Epoch 145/1300\n",
      "16852/16852 [==============================] - 12s 699us/sample - loss: 1.9733 - rmse: 1.3179 - val_loss: 1.8159 - val_rmse: 1.0929\n",
      "Epoch 146/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 1.9769 - rmse: 1.3179 - val_loss: 1.8580 - val_rmse: 1.1045\n",
      "Epoch 147/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.9839 - rmse: 1.3208 - val_loss: 1.8173 - val_rmse: 1.0921\n",
      "Epoch 148/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.9720 - rmse: 1.3176 - val_loss: 1.8222 - val_rmse: 1.0957\n",
      "Epoch 149/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 1.9999 - rmse: 1.3256 - val_loss: 1.8250 - val_rmse: 1.0973\n",
      "Epoch 150/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.9660 - rmse: 1.3149 - val_loss: 1.8210 - val_rmse: 1.0918\n",
      "Epoch 151/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.9721 - rmse: 1.3175 - val_loss: 1.8350 - val_rmse: 1.1006\n",
      "Epoch 152/1300\n",
      "16852/16852 [==============================] - 12s 699us/sample - loss: 1.9655 - rmse: 1.3148 - val_loss: 1.8239 - val_rmse: 1.0960\n",
      "Epoch 153/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.9499 - rmse: 1.3094 - val_loss: 1.8255 - val_rmse: 1.0921\n",
      "Epoch 154/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.9617 - rmse: 1.3136 - val_loss: 1.8796 - val_rmse: 1.1103\n",
      "Epoch 155/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.9541 - rmse: 1.3114 - val_loss: 1.8140 - val_rmse: 1.0889\n",
      "Epoch 156/1300\n",
      "16852/16852 [==============================] - 12s 693us/sample - loss: 1.9723 - rmse: 1.3153 - val_loss: 1.8224 - val_rmse: 1.0930\n",
      "Epoch 157/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.9627 - rmse: 1.3128 - val_loss: 1.8540 - val_rmse: 1.1072\n",
      "Epoch 158/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 1.9594 - rmse: 1.3129 - val_loss: 1.8910 - val_rmse: 1.1143\n",
      "Epoch 159/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 1.9652 - rmse: 1.3127 - val_loss: 1.8380 - val_rmse: 1.1019\n",
      "Epoch 160/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.9482 - rmse: 1.3096 - val_loss: 1.8451 - val_rmse: 1.1085\n",
      "Epoch 161/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.9704 - rmse: 1.3155 - val_loss: 1.8102 - val_rmse: 1.0927\n",
      "Epoch 162/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 1.9597 - rmse: 1.3123 - val_loss: 1.8267 - val_rmse: 1.0947\n",
      "Epoch 163/1300\n",
      "16852/16852 [==============================] - 12s 699us/sample - loss: 1.9398 - rmse: 1.3057 - val_loss: 1.8126 - val_rmse: 1.0921\n",
      "Epoch 164/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.9639 - rmse: 1.3122 - val_loss: 1.7993 - val_rmse: 1.0987\n",
      "Epoch 165/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 1.9502 - rmse: 1.3112 - val_loss: 1.8679 - val_rmse: 1.1059\n",
      "Epoch 166/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 1.9600 - rmse: 1.3115 - val_loss: 1.8482 - val_rmse: 1.0974\n",
      "Epoch 167/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 1.9596 - rmse: 1.3123 - val_loss: 1.8354 - val_rmse: 1.0977\n",
      "Epoch 168/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.9528 - rmse: 1.3092 - val_loss: 1.8651 - val_rmse: 1.1126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 169/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.9478 - rmse: 1.3072 - val_loss: 1.8600 - val_rmse: 1.1048\n",
      "Epoch 170/1300\n",
      "16852/16852 [==============================] - 12s 699us/sample - loss: 1.9617 - rmse: 1.3118 - val_loss: 1.7791 - val_rmse: 1.0810\n",
      "Epoch 171/1300\n",
      "16852/16852 [==============================] - 12s 707us/sample - loss: 1.9576 - rmse: 1.3112 - val_loss: 1.8434 - val_rmse: 1.0940\n",
      "Epoch 172/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 1.9676 - rmse: 1.3125 - val_loss: 1.8498 - val_rmse: 1.0973\n",
      "Epoch 173/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.9475 - rmse: 1.3088 - val_loss: 1.8191 - val_rmse: 1.0979\n",
      "Epoch 174/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 1.9485 - rmse: 1.3086 - val_loss: 1.8212 - val_rmse: 1.0966\n",
      "Epoch 175/1300\n",
      "16852/16852 [==============================] - 12s 710us/sample - loss: 1.9457 - rmse: 1.3076 - val_loss: 1.7696 - val_rmse: 1.0747\n",
      "Epoch 176/1300\n",
      "16852/16852 [==============================] - 12s 708us/sample - loss: 1.9854 - rmse: 1.3182 - val_loss: 1.7934 - val_rmse: 1.0788\n",
      "Epoch 177/1300\n",
      "16852/16852 [==============================] - 12s 705us/sample - loss: 1.9310 - rmse: 1.3021 - val_loss: 1.8198 - val_rmse: 1.0918\n",
      "Epoch 178/1300\n",
      "16852/16852 [==============================] - 12s 708us/sample - loss: 1.9462 - rmse: 1.3074 - val_loss: 1.8279 - val_rmse: 1.0888\n",
      "Epoch 179/1300\n",
      "16852/16852 [==============================] - 12s 709us/sample - loss: 1.9487 - rmse: 1.3089 - val_loss: 1.7923 - val_rmse: 1.0850\n",
      "Epoch 180/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 1.9269 - rmse: 1.3021 - val_loss: 1.7596 - val_rmse: 1.0799\n",
      "Epoch 181/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.9096 - rmse: 1.2964 - val_loss: 1.8271 - val_rmse: 1.0908\n",
      "Epoch 182/1300\n",
      "16852/16852 [==============================] - 12s 693us/sample - loss: 1.9504 - rmse: 1.3073 - val_loss: 1.8767 - val_rmse: 1.1127\n",
      "Epoch 183/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.9304 - rmse: 1.3023 - val_loss: 1.8153 - val_rmse: 1.0883\n",
      "Epoch 184/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 1.9236 - rmse: 1.2996 - val_loss: 1.7742 - val_rmse: 1.0773\n",
      "Epoch 185/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 1.9492 - rmse: 1.3074 - val_loss: 1.8165 - val_rmse: 1.0879\n",
      "Epoch 186/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.9333 - rmse: 1.3016 - val_loss: 1.8387 - val_rmse: 1.1001\n",
      "Epoch 187/1300\n",
      "16852/16852 [==============================] - 12s 692us/sample - loss: 1.9580 - rmse: 1.3094 - val_loss: 1.9194 - val_rmse: 1.1276\n",
      "Epoch 188/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.9344 - rmse: 1.3014 - val_loss: 1.8234 - val_rmse: 1.0908\n",
      "Epoch 189/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.9330 - rmse: 1.3014 - val_loss: 1.8053 - val_rmse: 1.0828\n",
      "Epoch 190/1300\n",
      "16852/16852 [==============================] - 12s 705us/sample - loss: 1.9496 - rmse: 1.3067 - val_loss: 1.7656 - val_rmse: 1.0733\n",
      "Epoch 191/1300\n",
      "16852/16852 [==============================] - 12s 710us/sample - loss: 1.9206 - rmse: 1.2979 - val_loss: 1.8084 - val_rmse: 1.0926\n",
      "Epoch 192/1300\n",
      "16852/16852 [==============================] - 12s 706us/sample - loss: 1.9290 - rmse: 1.3011 - val_loss: 1.8011 - val_rmse: 1.0834\n",
      "Epoch 193/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.9210 - rmse: 1.2982 - val_loss: 1.7901 - val_rmse: 1.0819\n",
      "Epoch 194/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.9416 - rmse: 1.3042 - val_loss: 1.8097 - val_rmse: 1.0872\n",
      "Epoch 195/1300\n",
      "16852/16852 [==============================] - 12s 692us/sample - loss: 1.9405 - rmse: 1.3038 - val_loss: 1.7964 - val_rmse: 1.0798\n",
      "Epoch 196/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.9437 - rmse: 1.3043 - val_loss: 1.7839 - val_rmse: 1.0792\n",
      "Epoch 197/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 1.9151 - rmse: 1.2954 - val_loss: 1.8809 - val_rmse: 1.1128\n",
      "Epoch 198/1300\n",
      "16852/16852 [==============================] - 12s 693us/sample - loss: 1.9181 - rmse: 1.2982 - val_loss: 1.8125 - val_rmse: 1.0854\n",
      "Epoch 199/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 1.9241 - rmse: 1.2997 - val_loss: 1.8268 - val_rmse: 1.0946\n",
      "Epoch 200/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.9164 - rmse: 1.2962 - val_loss: 1.7833 - val_rmse: 1.0742\n",
      "Epoch 201/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 1.9164 - rmse: 1.2964 - val_loss: 1.8079 - val_rmse: 1.0844\n",
      "Epoch 202/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.9319 - rmse: 1.2999 - val_loss: 1.8807 - val_rmse: 1.1089\n",
      "Epoch 203/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.9188 - rmse: 1.2966 - val_loss: 1.7824 - val_rmse: 1.0812\n",
      "Epoch 204/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.9100 - rmse: 1.2935 - val_loss: 1.8087 - val_rmse: 1.0838\n",
      "Epoch 205/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.9343 - rmse: 1.3015 - val_loss: 1.8201 - val_rmse: 1.0822\n",
      "Epoch 206/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.9220 - rmse: 1.2985 - val_loss: 1.8472 - val_rmse: 1.0907\n",
      "Epoch 207/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 1.9160 - rmse: 1.2952 - val_loss: 1.8003 - val_rmse: 1.0779\n",
      "Epoch 208/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 1.9116 - rmse: 1.2948 - val_loss: 1.7832 - val_rmse: 1.0738\n",
      "Epoch 209/1300\n",
      "16852/16852 [==============================] - 12s 701us/sample - loss: 1.8970 - rmse: 1.2891 - val_loss: 1.9498 - val_rmse: 1.1339\n",
      "Epoch 210/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.9209 - rmse: 1.2966 - val_loss: 1.8124 - val_rmse: 1.0835\n",
      "Epoch 211/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 1.9159 - rmse: 1.2952 - val_loss: 1.7953 - val_rmse: 1.0769\n",
      "Epoch 212/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.9166 - rmse: 1.2949 - val_loss: 1.8877 - val_rmse: 1.1099\n",
      "Epoch 213/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 1.9086 - rmse: 1.2938 - val_loss: 1.7910 - val_rmse: 1.0773\n",
      "Epoch 214/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.8986 - rmse: 1.2898 - val_loss: 1.8636 - val_rmse: 1.1077\n",
      "Epoch 215/1300\n",
      "16852/16852 [==============================] - 12s 703us/sample - loss: 1.8943 - rmse: 1.2868 - val_loss: 1.7864 - val_rmse: 1.0740\n",
      "Epoch 216/1300\n",
      "16852/16852 [==============================] - 12s 716us/sample - loss: 1.8979 - rmse: 1.2907 - val_loss: 1.8193 - val_rmse: 1.0883\n",
      "Epoch 217/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.8900 - rmse: 1.2882 - val_loss: 1.7742 - val_rmse: 1.0737\n",
      "Epoch 218/1300\n",
      "16852/16852 [==============================] - 12s 701us/sample - loss: 1.8908 - rmse: 1.2885 - val_loss: 1.7489 - val_rmse: 1.0640\n",
      "Epoch 219/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.9022 - rmse: 1.2898 - val_loss: 1.8206 - val_rmse: 1.0830\n",
      "Epoch 220/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.9083 - rmse: 1.2923 - val_loss: 1.7977 - val_rmse: 1.0818\n",
      "Epoch 221/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.9043 - rmse: 1.2920 - val_loss: 1.7789 - val_rmse: 1.0747\n",
      "Epoch 222/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 1.9239 - rmse: 1.2984 - val_loss: 1.8703 - val_rmse: 1.1031\n",
      "Epoch 223/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 1.8959 - rmse: 1.2885 - val_loss: 1.7895 - val_rmse: 1.0774\n",
      "Epoch 224/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 1.9107 - rmse: 1.2930 - val_loss: 1.8204 - val_rmse: 1.0863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 225/1300\n",
      "16852/16852 [==============================] - 12s 693us/sample - loss: 1.8979 - rmse: 1.2899 - val_loss: 1.7750 - val_rmse: 1.0727\n",
      "Epoch 226/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 1.8865 - rmse: 1.2866 - val_loss: 1.7624 - val_rmse: 1.0689\n",
      "Epoch 227/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 1.9077 - rmse: 1.2912 - val_loss: 1.7866 - val_rmse: 1.0806\n",
      "Epoch 228/1300\n",
      "16852/16852 [==============================] - 12s 690us/sample - loss: 1.8897 - rmse: 1.2873 - val_loss: 1.7519 - val_rmse: 1.0674\n",
      "Epoch 229/1300\n",
      "16852/16852 [==============================] - 12s 693us/sample - loss: 1.8957 - rmse: 1.2894 - val_loss: 1.7690 - val_rmse: 1.0726\n",
      "Epoch 230/1300\n",
      "16852/16852 [==============================] - 12s 705us/sample - loss: 1.8842 - rmse: 1.2851 - val_loss: 1.8530 - val_rmse: 1.0968\n",
      "Epoch 231/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.9022 - rmse: 1.2910 - val_loss: 1.7789 - val_rmse: 1.0751\n",
      "Epoch 232/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.8703 - rmse: 1.2804 - val_loss: 1.7436 - val_rmse: 1.0647\n",
      "Epoch 233/1300\n",
      "16852/16852 [==============================] - 12s 691us/sample - loss: 1.8593 - rmse: 1.2765 - val_loss: 1.7759 - val_rmse: 1.0729\n",
      "Epoch 234/1300\n",
      "16852/16852 [==============================] - 12s 693us/sample - loss: 1.8977 - rmse: 1.2890 - val_loss: 1.7602 - val_rmse: 1.0705\n",
      "Epoch 235/1300\n",
      "16852/16852 [==============================] - 12s 693us/sample - loss: 1.8866 - rmse: 1.2830 - val_loss: 1.7586 - val_rmse: 1.0672\n",
      "Epoch 236/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 1.8779 - rmse: 1.2828 - val_loss: 1.7617 - val_rmse: 1.0685\n",
      "Epoch 237/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.8822 - rmse: 1.2856 - val_loss: 1.8841 - val_rmse: 1.1067\n",
      "Epoch 238/1300\n",
      "16852/16852 [==============================] - 12s 693us/sample - loss: 1.8740 - rmse: 1.2824 - val_loss: 1.8844 - val_rmse: 1.1162\n",
      "Epoch 239/1300\n",
      "16852/16852 [==============================] - 12s 692us/sample - loss: 1.8693 - rmse: 1.2794 - val_loss: 1.8169 - val_rmse: 1.0864\n",
      "Epoch 240/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.8770 - rmse: 1.2820 - val_loss: 1.7603 - val_rmse: 1.0685\n",
      "Epoch 241/1300\n",
      "16852/16852 [==============================] - 12s 706us/sample - loss: 1.8859 - rmse: 1.2847 - val_loss: 1.7395 - val_rmse: 1.0583\n",
      "Epoch 242/1300\n",
      "16852/16852 [==============================] - 12s 699us/sample - loss: 1.8746 - rmse: 1.2808 - val_loss: 1.7780 - val_rmse: 1.0749\n",
      "Epoch 243/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 1.8850 - rmse: 1.2844 - val_loss: 1.7694 - val_rmse: 1.0704\n",
      "Epoch 244/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.8906 - rmse: 1.2879 - val_loss: 1.7918 - val_rmse: 1.0772\n",
      "Epoch 245/1300\n",
      "16852/16852 [==============================] - 12s 701us/sample - loss: 1.8855 - rmse: 1.2859 - val_loss: 1.9961 - val_rmse: 1.1559\n",
      "Epoch 246/1300\n",
      "16852/16852 [==============================] - 12s 692us/sample - loss: 1.8743 - rmse: 1.2805 - val_loss: 1.7626 - val_rmse: 1.0643\n",
      "Epoch 247/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.8787 - rmse: 1.2825 - val_loss: 1.7649 - val_rmse: 1.0620\n",
      "Epoch 248/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.8806 - rmse: 1.2827 - val_loss: 1.7688 - val_rmse: 1.0659\n",
      "Epoch 249/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.8802 - rmse: 1.2832 - val_loss: 1.7423 - val_rmse: 1.0635\n",
      "Epoch 250/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.8662 - rmse: 1.2785 - val_loss: 1.7842 - val_rmse: 1.0746\n",
      "Epoch 251/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.8631 - rmse: 1.2777 - val_loss: 1.8469 - val_rmse: 1.1024\n",
      "Epoch 252/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.8762 - rmse: 1.2814 - val_loss: 1.7621 - val_rmse: 1.0683\n",
      "Epoch 253/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 1.8835 - rmse: 1.2825 - val_loss: 1.8003 - val_rmse: 1.0758\n",
      "Epoch 254/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.8870 - rmse: 1.2834 - val_loss: 1.7703 - val_rmse: 1.0791\n",
      "Epoch 255/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.8546 - rmse: 1.2746 - val_loss: 1.7677 - val_rmse: 1.0740\n",
      "Epoch 256/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.8512 - rmse: 1.2736 - val_loss: 1.7260 - val_rmse: 1.0617\n",
      "Epoch 257/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.8700 - rmse: 1.2799 - val_loss: 1.7597 - val_rmse: 1.0675\n",
      "Epoch 258/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 1.8656 - rmse: 1.2787 - val_loss: 1.7455 - val_rmse: 1.0603\n",
      "Epoch 259/1300\n",
      "16852/16852 [==============================] - 12s 692us/sample - loss: 1.8620 - rmse: 1.2765 - val_loss: 1.8243 - val_rmse: 1.0913\n",
      "Epoch 260/1300\n",
      "16852/16852 [==============================] - 12s 701us/sample - loss: 1.8828 - rmse: 1.2829 - val_loss: 1.7205 - val_rmse: 1.0554\n",
      "Epoch 261/1300\n",
      "16852/16852 [==============================] - 12s 704us/sample - loss: 1.8811 - rmse: 1.2814 - val_loss: 1.7911 - val_rmse: 1.0712\n",
      "Epoch 262/1300\n",
      "16852/16852 [==============================] - 12s 692us/sample - loss: 1.8634 - rmse: 1.2755 - val_loss: 1.8001 - val_rmse: 1.0722\n",
      "Epoch 263/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 1.8581 - rmse: 1.2753 - val_loss: 1.7880 - val_rmse: 1.0730\n",
      "Epoch 264/1300\n",
      "16852/16852 [==============================] - 12s 693us/sample - loss: 1.8536 - rmse: 1.2733 - val_loss: 1.8269 - val_rmse: 1.0865\n",
      "Epoch 265/1300\n",
      "16852/16852 [==============================] - 12s 699us/sample - loss: 1.8644 - rmse: 1.2764 - val_loss: 1.8370 - val_rmse: 1.0965\n",
      "Epoch 266/1300\n",
      "16852/16852 [==============================] - 12s 705us/sample - loss: 1.8722 - rmse: 1.2798 - val_loss: 1.7754 - val_rmse: 1.0680\n",
      "Epoch 267/1300\n",
      "16852/16852 [==============================] - 12s 716us/sample - loss: 1.8563 - rmse: 1.2727 - val_loss: 1.7896 - val_rmse: 1.0733\n",
      "Epoch 268/1300\n",
      "16852/16852 [==============================] - 12s 710us/sample - loss: 1.8695 - rmse: 1.2785 - val_loss: 1.8475 - val_rmse: 1.0877\n",
      "Epoch 269/1300\n",
      "16852/16852 [==============================] - 12s 712us/sample - loss: 1.8456 - rmse: 1.2710 - val_loss: 1.7132 - val_rmse: 1.0504\n",
      "Epoch 270/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.8514 - rmse: 1.2716 - val_loss: 1.7801 - val_rmse: 1.0681\n",
      "Epoch 271/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.8771 - rmse: 1.2793 - val_loss: 1.7502 - val_rmse: 1.0606\n",
      "Epoch 272/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.8485 - rmse: 1.2703 - val_loss: 1.7970 - val_rmse: 1.0733\n",
      "Epoch 273/1300\n",
      "16852/16852 [==============================] - 12s 706us/sample - loss: 1.8710 - rmse: 1.2792 - val_loss: 1.7243 - val_rmse: 1.0535\n",
      "Epoch 274/1300\n",
      "16852/16852 [==============================] - 12s 706us/sample - loss: 1.8523 - rmse: 1.2723 - val_loss: 1.7906 - val_rmse: 1.0755\n",
      "Epoch 275/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 1.8310 - rmse: 1.2655 - val_loss: 1.8139 - val_rmse: 1.0876\n",
      "Epoch 276/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.8272 - rmse: 1.2647 - val_loss: 1.7583 - val_rmse: 1.0591\n",
      "Epoch 277/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 1.8367 - rmse: 1.2678 - val_loss: 1.8247 - val_rmse: 1.0850\n",
      "Epoch 278/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.8279 - rmse: 1.2651 - val_loss: 1.7325 - val_rmse: 1.0508\n",
      "Epoch 279/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 1.8543 - rmse: 1.2715 - val_loss: 1.7756 - val_rmse: 1.0661\n",
      "Epoch 280/1300\n",
      "16852/16852 [==============================] - 12s 706us/sample - loss: 1.8544 - rmse: 1.2716 - val_loss: 1.7461 - val_rmse: 1.0580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 281/1300\n",
      "16852/16852 [==============================] - 12s 711us/sample - loss: 1.8572 - rmse: 1.2737 - val_loss: 1.7255 - val_rmse: 1.0526\n",
      "Epoch 282/1300\n",
      "16852/16852 [==============================] - 12s 706us/sample - loss: 1.8401 - rmse: 1.2671 - val_loss: 1.7772 - val_rmse: 1.0686\n",
      "Epoch 283/1300\n",
      "16852/16852 [==============================] - 12s 715us/sample - loss: 1.8624 - rmse: 1.2746 - val_loss: 1.7224 - val_rmse: 1.0487\n",
      "Epoch 284/1300\n",
      "16852/16852 [==============================] - 12s 706us/sample - loss: 1.8384 - rmse: 1.2661 - val_loss: 1.7586 - val_rmse: 1.0584\n",
      "Epoch 285/1300\n",
      "16852/16852 [==============================] - 12s 705us/sample - loss: 1.8448 - rmse: 1.2686 - val_loss: 1.7604 - val_rmse: 1.0609\n",
      "Epoch 286/1300\n",
      "16852/16852 [==============================] - 12s 693us/sample - loss: 1.8330 - rmse: 1.2658 - val_loss: 1.7595 - val_rmse: 1.0597\n",
      "Epoch 287/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.8578 - rmse: 1.2726 - val_loss: 1.7470 - val_rmse: 1.0554\n",
      "Epoch 288/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.8375 - rmse: 1.2683 - val_loss: 1.7689 - val_rmse: 1.0753\n",
      "Epoch 289/1300\n",
      "16852/16852 [==============================] - 12s 691us/sample - loss: 1.8449 - rmse: 1.2682 - val_loss: 1.7347 - val_rmse: 1.0584\n",
      "Epoch 290/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 1.8291 - rmse: 1.2647 - val_loss: 1.7741 - val_rmse: 1.0563\n",
      "Epoch 291/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.8361 - rmse: 1.2662 - val_loss: 1.7317 - val_rmse: 1.0548\n",
      "Epoch 292/1300\n",
      "16852/16852 [==============================] - 12s 699us/sample - loss: 1.8582 - rmse: 1.2728 - val_loss: 1.7359 - val_rmse: 1.0539\n",
      "Epoch 293/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.8304 - rmse: 1.2648 - val_loss: 1.7273 - val_rmse: 1.0544\n",
      "Epoch 294/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 1.8352 - rmse: 1.2652 - val_loss: 1.7426 - val_rmse: 1.0561\n",
      "Epoch 295/1300\n",
      "16852/16852 [==============================] - 12s 703us/sample - loss: 1.8286 - rmse: 1.2639 - val_loss: 1.7292 - val_rmse: 1.0487\n",
      "Epoch 296/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 1.8210 - rmse: 1.2609 - val_loss: 1.7810 - val_rmse: 1.0627\n",
      "Epoch 297/1300\n",
      "16852/16852 [==============================] - 12s 703us/sample - loss: 1.8313 - rmse: 1.2653 - val_loss: 1.7562 - val_rmse: 1.0568\n",
      "Epoch 298/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.8456 - rmse: 1.2688 - val_loss: 1.7525 - val_rmse: 1.0561\n",
      "Epoch 299/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 1.8210 - rmse: 1.2623 - val_loss: 1.7653 - val_rmse: 1.0566\n",
      "Epoch 300/1300\n",
      "16852/16852 [==============================] - 12s 704us/sample - loss: 1.8363 - rmse: 1.2659 - val_loss: 1.7410 - val_rmse: 1.0577\n",
      "Epoch 301/1300\n",
      "16852/16852 [==============================] - 12s 704us/sample - loss: 1.8377 - rmse: 1.2642 - val_loss: 1.7550 - val_rmse: 1.0614\n",
      "Epoch 302/1300\n",
      "16852/16852 [==============================] - 12s 702us/sample - loss: 1.8194 - rmse: 1.2599 - val_loss: 1.7560 - val_rmse: 1.0545\n",
      "Epoch 303/1300\n",
      "16852/16852 [==============================] - 12s 706us/sample - loss: 1.8406 - rmse: 1.2666 - val_loss: 1.7884 - val_rmse: 1.0676\n",
      "Epoch 304/1300\n",
      "16852/16852 [==============================] - 12s 708us/sample - loss: 1.8289 - rmse: 1.2634 - val_loss: 1.7635 - val_rmse: 1.0647\n",
      "Epoch 305/1300\n",
      "16852/16852 [==============================] - 12s 707us/sample - loss: 1.8234 - rmse: 1.2621 - val_loss: 1.7581 - val_rmse: 1.0574\n",
      "Epoch 306/1300\n",
      "16852/16852 [==============================] - 12s 707us/sample - loss: 1.8235 - rmse: 1.2622 - val_loss: 1.7666 - val_rmse: 1.0592\n",
      "Epoch 307/1300\n",
      "16852/16852 [==============================] - 12s 707us/sample - loss: 1.8205 - rmse: 1.2612 - val_loss: 1.7373 - val_rmse: 1.0505\n",
      "Epoch 308/1300\n",
      "16852/16852 [==============================] - 12s 708us/sample - loss: 1.8093 - rmse: 1.2575 - val_loss: 1.8203 - val_rmse: 1.0794\n",
      "Epoch 309/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 1.8513 - rmse: 1.2705 - val_loss: 1.7747 - val_rmse: 1.0683\n",
      "Epoch 310/1300\n",
      "16852/16852 [==============================] - 12s 693us/sample - loss: 1.8166 - rmse: 1.2576 - val_loss: 1.7471 - val_rmse: 1.0598\n",
      "Epoch 311/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.8138 - rmse: 1.2575 - val_loss: 1.8211 - val_rmse: 1.0797\n",
      "Epoch 312/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 1.8334 - rmse: 1.2631 - val_loss: 1.7628 - val_rmse: 1.0556\n",
      "Epoch 313/1300\n",
      "16852/16852 [==============================] - 12s 701us/sample - loss: 1.8288 - rmse: 1.2618 - val_loss: 1.7500 - val_rmse: 1.0608\n",
      "Epoch 314/1300\n",
      "16852/16852 [==============================] - 12s 708us/sample - loss: 1.8174 - rmse: 1.2605 - val_loss: 1.7616 - val_rmse: 1.0622\n",
      "Epoch 315/1300\n",
      "16852/16852 [==============================] - 12s 705us/sample - loss: 1.8197 - rmse: 1.2598 - val_loss: 1.7881 - val_rmse: 1.0654\n",
      "Epoch 316/1300\n",
      "16852/16852 [==============================] - 12s 712us/sample - loss: 1.8326 - rmse: 1.2635 - val_loss: 1.7253 - val_rmse: 1.0468\n",
      "Epoch 317/1300\n",
      "16852/16852 [==============================] - 12s 705us/sample - loss: 1.8137 - rmse: 1.2594 - val_loss: 1.7705 - val_rmse: 1.0636\n",
      "Epoch 318/1300\n",
      "16852/16852 [==============================] - 12s 711us/sample - loss: 1.8158 - rmse: 1.2585 - val_loss: 1.7303 - val_rmse: 1.0572\n",
      "Epoch 319/1300\n",
      "16852/16852 [==============================] - 12s 708us/sample - loss: 1.8289 - rmse: 1.2632 - val_loss: 1.7218 - val_rmse: 1.0469\n",
      "Epoch 320/1300\n",
      "16852/16852 [==============================] - 12s 707us/sample - loss: 1.8415 - rmse: 1.2652 - val_loss: 1.7730 - val_rmse: 1.0601\n",
      "Epoch 321/1300\n",
      "16852/16852 [==============================] - 12s 707us/sample - loss: 1.8075 - rmse: 1.2549 - val_loss: 1.7753 - val_rmse: 1.0591\n",
      "Epoch 322/1300\n",
      "16852/16852 [==============================] - 12s 693us/sample - loss: 1.8385 - rmse: 1.2632 - val_loss: 1.7650 - val_rmse: 1.0680\n",
      "Epoch 323/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.8040 - rmse: 1.2537 - val_loss: 1.7286 - val_rmse: 1.0522\n",
      "Epoch 324/1300\n",
      "16852/16852 [==============================] - 12s 706us/sample - loss: 1.8088 - rmse: 1.2562 - val_loss: 1.7416 - val_rmse: 1.0569\n",
      "Epoch 325/1300\n",
      "16852/16852 [==============================] - 12s 706us/sample - loss: 1.8077 - rmse: 1.2543 - val_loss: 1.7815 - val_rmse: 1.0687\n",
      "Epoch 326/1300\n",
      "16852/16852 [==============================] - 12s 703us/sample - loss: 1.8059 - rmse: 1.2543 - val_loss: 1.7312 - val_rmse: 1.0449\n",
      "Epoch 327/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 1.8139 - rmse: 1.2580 - val_loss: 1.7197 - val_rmse: 1.0477\n",
      "Epoch 328/1300\n",
      "16852/16852 [==============================] - 12s 714us/sample - loss: 1.8207 - rmse: 1.2587 - val_loss: 1.7634 - val_rmse: 1.0570\n",
      "Epoch 329/1300\n",
      "16852/16852 [==============================] - 12s 707us/sample - loss: 1.7899 - rmse: 1.2501 - val_loss: 1.7415 - val_rmse: 1.0536\n",
      "Epoch 330/1300\n",
      "16852/16852 [==============================] - 12s 706us/sample - loss: 1.8141 - rmse: 1.2566 - val_loss: 1.7830 - val_rmse: 1.0693\n",
      "Epoch 331/1300\n",
      "16852/16852 [==============================] - 12s 715us/sample - loss: 1.8267 - rmse: 1.2610 - val_loss: 1.6919 - val_rmse: 1.0376\n",
      "Epoch 332/1300\n",
      "16852/16852 [==============================] - 12s 707us/sample - loss: 1.7793 - rmse: 1.2451 - val_loss: 1.7542 - val_rmse: 1.0527\n",
      "Epoch 333/1300\n",
      "16852/16852 [==============================] - 12s 708us/sample - loss: 1.7841 - rmse: 1.2483 - val_loss: 1.8270 - val_rmse: 1.0810\n",
      "Epoch 334/1300\n",
      "16852/16852 [==============================] - 12s 706us/sample - loss: 1.8003 - rmse: 1.2512 - val_loss: 1.8149 - val_rmse: 1.0730\n",
      "Epoch 335/1300\n",
      "16852/16852 [==============================] - 12s 706us/sample - loss: 1.8225 - rmse: 1.2590 - val_loss: 1.7129 - val_rmse: 1.0410\n",
      "Epoch 336/1300\n",
      "16852/16852 [==============================] - 12s 709us/sample - loss: 1.7977 - rmse: 1.2516 - val_loss: 1.7200 - val_rmse: 1.0415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 337/1300\n",
      "16852/16852 [==============================] - 12s 702us/sample - loss: 1.7943 - rmse: 1.2510 - val_loss: 1.7610 - val_rmse: 1.0570\n",
      "Epoch 338/1300\n",
      "16852/16852 [==============================] - 12s 692us/sample - loss: 1.7998 - rmse: 1.2526 - val_loss: 1.7856 - val_rmse: 1.0639\n",
      "Epoch 339/1300\n",
      "16852/16852 [==============================] - 12s 702us/sample - loss: 1.8000 - rmse: 1.2507 - val_loss: 1.7395 - val_rmse: 1.0514\n",
      "Epoch 340/1300\n",
      "16852/16852 [==============================] - 12s 703us/sample - loss: 1.7688 - rmse: 1.2429 - val_loss: 1.7789 - val_rmse: 1.0611\n",
      "Epoch 341/1300\n",
      "16852/16852 [==============================] - 12s 707us/sample - loss: 1.7915 - rmse: 1.2506 - val_loss: 1.7146 - val_rmse: 1.0412\n",
      "Epoch 342/1300\n",
      "16852/16852 [==============================] - 12s 703us/sample - loss: 1.7891 - rmse: 1.2489 - val_loss: 1.7752 - val_rmse: 1.0623\n",
      "Epoch 343/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 1.7994 - rmse: 1.2519 - val_loss: 1.7118 - val_rmse: 1.0411\n",
      "Epoch 344/1300\n",
      "16852/16852 [==============================] - 12s 709us/sample - loss: 1.7823 - rmse: 1.2461 - val_loss: 1.7182 - val_rmse: 1.0404\n",
      "Epoch 345/1300\n",
      "16852/16852 [==============================] - 12s 706us/sample - loss: 1.7859 - rmse: 1.2467 - val_loss: 1.7064 - val_rmse: 1.0406\n",
      "Epoch 346/1300\n",
      "16852/16852 [==============================] - 12s 705us/sample - loss: 1.7891 - rmse: 1.2485 - val_loss: 1.7675 - val_rmse: 1.0522\n",
      "Epoch 347/1300\n",
      "16852/16852 [==============================] - 12s 704us/sample - loss: 1.7868 - rmse: 1.2471 - val_loss: 1.7418 - val_rmse: 1.0507\n",
      "Epoch 348/1300\n",
      "16852/16852 [==============================] - 12s 692us/sample - loss: 1.8139 - rmse: 1.2542 - val_loss: 1.7459 - val_rmse: 1.0503\n",
      "Epoch 349/1300\n",
      "16852/16852 [==============================] - 12s 693us/sample - loss: 1.7952 - rmse: 1.2492 - val_loss: 1.7731 - val_rmse: 1.0565\n",
      "Epoch 350/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.7900 - rmse: 1.2488 - val_loss: 1.7533 - val_rmse: 1.0533\n",
      "Epoch 351/1300\n",
      "16852/16852 [==============================] - 12s 700us/sample - loss: 1.7845 - rmse: 1.2490 - val_loss: 1.7271 - val_rmse: 1.0375\n",
      "Epoch 352/1300\n",
      "16852/16852 [==============================] - 12s 707us/sample - loss: 1.7936 - rmse: 1.2495 - val_loss: 1.7675 - val_rmse: 1.0571\n",
      "Epoch 353/1300\n",
      "16852/16852 [==============================] - 12s 703us/sample - loss: 1.7585 - rmse: 1.2385 - val_loss: 1.7240 - val_rmse: 1.0436\n",
      "Epoch 354/1300\n",
      "16852/16852 [==============================] - 12s 706us/sample - loss: 1.7997 - rmse: 1.2511 - val_loss: 1.7350 - val_rmse: 1.0445\n",
      "Epoch 355/1300\n",
      "16852/16852 [==============================] - 12s 711us/sample - loss: 1.7930 - rmse: 1.2494 - val_loss: 1.6957 - val_rmse: 1.0357\n",
      "Epoch 356/1300\n",
      "16852/16852 [==============================] - 12s 709us/sample - loss: 1.8044 - rmse: 1.2529 - val_loss: 1.7646 - val_rmse: 1.0546\n",
      "Epoch 357/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 1.7942 - rmse: 1.2496 - val_loss: 1.7470 - val_rmse: 1.0494\n",
      "Epoch 358/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 1.7840 - rmse: 1.2446 - val_loss: 1.7970 - val_rmse: 1.0664\n",
      "Epoch 359/1300\n",
      "16852/16852 [==============================] - 12s 700us/sample - loss: 1.7946 - rmse: 1.2498 - val_loss: 1.6938 - val_rmse: 1.0322\n",
      "Epoch 360/1300\n",
      "16852/16852 [==============================] - 12s 704us/sample - loss: 1.7838 - rmse: 1.2468 - val_loss: 1.7262 - val_rmse: 1.0431\n",
      "Epoch 361/1300\n",
      "16852/16852 [==============================] - 12s 704us/sample - loss: 1.7881 - rmse: 1.2480 - val_loss: 1.7481 - val_rmse: 1.0500\n",
      "Epoch 362/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 1.7969 - rmse: 1.2496 - val_loss: 1.7202 - val_rmse: 1.0381\n",
      "Epoch 363/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.7818 - rmse: 1.2459 - val_loss: 1.7114 - val_rmse: 1.0402\n",
      "Epoch 364/1300\n",
      "16852/16852 [==============================] - 12s 702us/sample - loss: 1.7827 - rmse: 1.2450 - val_loss: 1.6777 - val_rmse: 1.0305\n",
      "Epoch 365/1300\n",
      "16852/16852 [==============================] - 12s 705us/sample - loss: 1.7885 - rmse: 1.2459 - val_loss: 1.7295 - val_rmse: 1.0484\n",
      "Epoch 366/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.7558 - rmse: 1.2361 - val_loss: 1.7955 - val_rmse: 1.0645\n",
      "Epoch 367/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 1.7652 - rmse: 1.2400 - val_loss: 1.7320 - val_rmse: 1.0420\n",
      "Epoch 368/1300\n",
      "16852/16852 [==============================] - 12s 701us/sample - loss: 1.7766 - rmse: 1.2427 - val_loss: 1.7041 - val_rmse: 1.0450\n",
      "Epoch 369/1300\n",
      "16852/16852 [==============================] - 12s 710us/sample - loss: 1.7797 - rmse: 1.2442 - val_loss: 1.7337 - val_rmse: 1.0430\n",
      "Epoch 370/1300\n",
      "16852/16852 [==============================] - 12s 705us/sample - loss: 1.7750 - rmse: 1.2430 - val_loss: 1.7843 - val_rmse: 1.0606\n",
      "Epoch 371/1300\n",
      "16852/16852 [==============================] - 12s 705us/sample - loss: 1.7794 - rmse: 1.2443 - val_loss: 1.7271 - val_rmse: 1.0451\n",
      "Epoch 372/1300\n",
      "16852/16852 [==============================] - 12s 707us/sample - loss: 1.7697 - rmse: 1.2403 - val_loss: 1.7422 - val_rmse: 1.0504\n",
      "Epoch 373/1300\n",
      "16852/16852 [==============================] - 12s 707us/sample - loss: 1.7957 - rmse: 1.2486 - val_loss: 1.7155 - val_rmse: 1.0419\n",
      "Epoch 374/1300\n",
      "16852/16852 [==============================] - 12s 707us/sample - loss: 1.7703 - rmse: 1.2407 - val_loss: 1.7232 - val_rmse: 1.0434\n",
      "Epoch 375/1300\n",
      "16852/16852 [==============================] - 12s 705us/sample - loss: 1.7643 - rmse: 1.2395 - val_loss: 1.7387 - val_rmse: 1.0485\n",
      "Epoch 376/1300\n",
      "16852/16852 [==============================] - 12s 709us/sample - loss: 1.7706 - rmse: 1.2404 - val_loss: 1.6960 - val_rmse: 1.0312\n",
      "Epoch 377/1300\n",
      "16852/16852 [==============================] - 12s 712us/sample - loss: 1.7917 - rmse: 1.2478 - val_loss: 1.6728 - val_rmse: 1.0260\n",
      "Epoch 378/1300\n",
      "16852/16852 [==============================] - 12s 705us/sample - loss: 1.7744 - rmse: 1.2420 - val_loss: 1.7376 - val_rmse: 1.0458\n",
      "Epoch 379/1300\n",
      "16852/16852 [==============================] - 12s 710us/sample - loss: 1.7647 - rmse: 1.2401 - val_loss: 1.7200 - val_rmse: 1.0432\n",
      "Epoch 380/1300\n",
      "16852/16852 [==============================] - 12s 703us/sample - loss: 1.7860 - rmse: 1.2468 - val_loss: 1.7522 - val_rmse: 1.0455\n",
      "Epoch 381/1300\n",
      "16852/16852 [==============================] - 12s 706us/sample - loss: 1.7774 - rmse: 1.2423 - val_loss: 1.7793 - val_rmse: 1.0544\n",
      "Epoch 382/1300\n",
      "16852/16852 [==============================] - 12s 708us/sample - loss: 1.7675 - rmse: 1.2396 - val_loss: 1.7353 - val_rmse: 1.0469\n",
      "Epoch 383/1300\n",
      "16852/16852 [==============================] - 12s 710us/sample - loss: 1.7513 - rmse: 1.2346 - val_loss: 1.7455 - val_rmse: 1.0478\n",
      "Epoch 384/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.7659 - rmse: 1.2380 - val_loss: 1.7286 - val_rmse: 1.0502\n",
      "Epoch 385/1300\n",
      "16852/16852 [==============================] - 12s 693us/sample - loss: 1.7776 - rmse: 1.2437 - val_loss: 1.7114 - val_rmse: 1.0368\n",
      "Epoch 386/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.7669 - rmse: 1.2411 - val_loss: 1.7841 - val_rmse: 1.0613\n",
      "Epoch 387/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.7861 - rmse: 1.2450 - val_loss: 1.7745 - val_rmse: 1.0640\n",
      "Epoch 388/1300\n",
      "16852/16852 [==============================] - 12s 693us/sample - loss: 1.7644 - rmse: 1.2391 - val_loss: 1.7771 - val_rmse: 1.0643\n",
      "Epoch 389/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 1.7742 - rmse: 1.2423 - val_loss: 1.7191 - val_rmse: 1.0388\n",
      "Epoch 390/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.7744 - rmse: 1.2408 - val_loss: 1.7299 - val_rmse: 1.0381\n",
      "Epoch 391/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.7659 - rmse: 1.2382 - val_loss: 1.7023 - val_rmse: 1.0361\n",
      "Epoch 392/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 1.7680 - rmse: 1.2387 - val_loss: 1.7350 - val_rmse: 1.0452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 393/1300\n",
      "16852/16852 [==============================] - 12s 693us/sample - loss: 1.7720 - rmse: 1.2404 - val_loss: 1.7159 - val_rmse: 1.0406\n",
      "Epoch 394/1300\n",
      "16852/16852 [==============================] - 12s 692us/sample - loss: 1.7712 - rmse: 1.2393 - val_loss: 1.7253 - val_rmse: 1.0376\n",
      "Epoch 395/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 1.7604 - rmse: 1.2380 - val_loss: 1.8117 - val_rmse: 1.0695\n",
      "Epoch 396/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 1.7536 - rmse: 1.2323 - val_loss: 1.7135 - val_rmse: 1.0332\n",
      "Epoch 397/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.7616 - rmse: 1.2372 - val_loss: 1.7395 - val_rmse: 1.0422\n",
      "Epoch 398/1300\n",
      "16852/16852 [==============================] - 12s 693us/sample - loss: 1.7423 - rmse: 1.2306 - val_loss: 1.7863 - val_rmse: 1.0559\n",
      "Epoch 399/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.7533 - rmse: 1.2352 - val_loss: 1.7745 - val_rmse: 1.0496\n",
      "Epoch 400/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 1.7574 - rmse: 1.2368 - val_loss: 1.7128 - val_rmse: 1.0367\n",
      "Epoch 401/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 1.7599 - rmse: 1.2353 - val_loss: 1.7540 - val_rmse: 1.0471\n",
      "Epoch 402/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.7612 - rmse: 1.2372 - val_loss: 1.7285 - val_rmse: 1.0370\n",
      "Epoch 403/1300\n",
      "16852/16852 [==============================] - 12s 693us/sample - loss: 1.7603 - rmse: 1.2370 - val_loss: 1.7305 - val_rmse: 1.0517\n",
      "Epoch 404/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.7539 - rmse: 1.2360 - val_loss: 1.7238 - val_rmse: 1.0399\n",
      "Epoch 405/1300\n",
      "16852/16852 [==============================] - 12s 693us/sample - loss: 1.7468 - rmse: 1.2324 - val_loss: 1.7349 - val_rmse: 1.0414\n",
      "Epoch 406/1300\n",
      "16852/16852 [==============================] - 12s 692us/sample - loss: 1.7553 - rmse: 1.2338 - val_loss: 1.7198 - val_rmse: 1.0378\n",
      "Epoch 407/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.7429 - rmse: 1.2312 - val_loss: 1.7107 - val_rmse: 1.0315\n",
      "Epoch 408/1300\n",
      "16852/16852 [==============================] - 12s 701us/sample - loss: 1.7647 - rmse: 1.2355 - val_loss: 1.7778 - val_rmse: 1.0552\n",
      "Epoch 409/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 1.7624 - rmse: 1.2360 - val_loss: 1.6896 - val_rmse: 1.0320\n",
      "Epoch 410/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 1.7482 - rmse: 1.2325 - val_loss: 1.6787 - val_rmse: 1.0260\n",
      "Epoch 411/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 1.7146 - rmse: 1.2219 - val_loss: 1.8124 - val_rmse: 1.0663\n",
      "Epoch 412/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.7454 - rmse: 1.2324 - val_loss: 1.7460 - val_rmse: 1.0453\n",
      "Epoch 413/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 1.7529 - rmse: 1.2317 - val_loss: 1.7145 - val_rmse: 1.0348\n",
      "Epoch 414/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.7159 - rmse: 1.2218 - val_loss: 1.7162 - val_rmse: 1.0338\n",
      "Epoch 415/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 1.7570 - rmse: 1.2333 - val_loss: 1.7218 - val_rmse: 1.0347\n",
      "Epoch 416/1300\n",
      "16852/16852 [==============================] - 12s 693us/sample - loss: 1.7556 - rmse: 1.2333 - val_loss: 1.7065 - val_rmse: 1.0412\n",
      "Epoch 417/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.7315 - rmse: 1.2270 - val_loss: 1.8144 - val_rmse: 1.0689\n",
      "Epoch 418/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.7587 - rmse: 1.2349 - val_loss: 1.7654 - val_rmse: 1.0486\n",
      "Epoch 419/1300\n",
      "16852/16852 [==============================] - 12s 706us/sample - loss: 1.7056 - rmse: 1.2183 - val_loss: 1.7271 - val_rmse: 1.0395\n",
      "Epoch 420/1300\n",
      "16852/16852 [==============================] - 12s 700us/sample - loss: 1.7337 - rmse: 1.2275 - val_loss: 1.7092 - val_rmse: 1.0317\n",
      "Epoch 421/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.7713 - rmse: 1.2386 - val_loss: 1.7763 - val_rmse: 1.0515\n",
      "Epoch 422/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.7550 - rmse: 1.2327 - val_loss: 1.7419 - val_rmse: 1.0500\n",
      "Epoch 423/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.7288 - rmse: 1.2257 - val_loss: 1.7190 - val_rmse: 1.0399\n",
      "Epoch 424/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.7303 - rmse: 1.2262 - val_loss: 1.7085 - val_rmse: 1.0299\n",
      "Epoch 425/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 1.7428 - rmse: 1.2307 - val_loss: 1.7209 - val_rmse: 1.0363\n",
      "Epoch 426/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.7405 - rmse: 1.2285 - val_loss: 1.7450 - val_rmse: 1.0407\n",
      "Epoch 427/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.7427 - rmse: 1.2303 - val_loss: 1.7379 - val_rmse: 1.0416\n",
      "Epoch 428/1300\n",
      "16852/16852 [==============================] - 12s 699us/sample - loss: 1.7425 - rmse: 1.2283 - val_loss: 1.7512 - val_rmse: 1.0496\n",
      "Epoch 429/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.7237 - rmse: 1.2252 - val_loss: 1.7917 - val_rmse: 1.0566\n",
      "Epoch 430/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 1.7593 - rmse: 1.2348 - val_loss: 1.7201 - val_rmse: 1.0348\n",
      "Epoch 431/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.7338 - rmse: 1.2267 - val_loss: 1.6933 - val_rmse: 1.0264\n",
      "Epoch 432/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.7451 - rmse: 1.2304 - val_loss: 1.7652 - val_rmse: 1.0472\n",
      "Epoch 433/1300\n",
      "16852/16852 [==============================] - 12s 703us/sample - loss: 1.7415 - rmse: 1.2282 - val_loss: 1.6890 - val_rmse: 1.0258\n",
      "Epoch 434/1300\n",
      "16852/16852 [==============================] - 12s 707us/sample - loss: 1.7234 - rmse: 1.2225 - val_loss: 1.6718 - val_rmse: 1.0210\n",
      "Epoch 435/1300\n",
      "16852/16852 [==============================] - 12s 704us/sample - loss: 1.7328 - rmse: 1.2270 - val_loss: 1.7393 - val_rmse: 1.0418\n",
      "Epoch 436/1300\n",
      "16852/16852 [==============================] - 12s 705us/sample - loss: 1.7356 - rmse: 1.2265 - val_loss: 1.6765 - val_rmse: 1.0212\n",
      "Epoch 437/1300\n",
      "16852/16852 [==============================] - 12s 707us/sample - loss: 1.7436 - rmse: 1.2297 - val_loss: 1.7891 - val_rmse: 1.0558\n",
      "Epoch 438/1300\n",
      "16852/16852 [==============================] - 12s 709us/sample - loss: 1.7199 - rmse: 1.2226 - val_loss: 1.7101 - val_rmse: 1.0327\n",
      "Epoch 439/1300\n",
      "16852/16852 [==============================] - 12s 708us/sample - loss: 1.7510 - rmse: 1.2327 - val_loss: 1.6864 - val_rmse: 1.0220\n",
      "Epoch 440/1300\n",
      "16852/16852 [==============================] - 12s 709us/sample - loss: 1.7343 - rmse: 1.2270 - val_loss: 1.7325 - val_rmse: 1.0412\n",
      "Epoch 441/1300\n",
      "16852/16852 [==============================] - 12s 706us/sample - loss: 1.7287 - rmse: 1.2251 - val_loss: 1.7679 - val_rmse: 1.0503\n",
      "Epoch 442/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.7375 - rmse: 1.2270 - val_loss: 1.7029 - val_rmse: 1.0352\n",
      "Epoch 443/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.7227 - rmse: 1.2237 - val_loss: 1.7484 - val_rmse: 1.0445\n",
      "Epoch 444/1300\n",
      "16852/16852 [==============================] - 12s 701us/sample - loss: 1.7228 - rmse: 1.2214 - val_loss: 1.6656 - val_rmse: 1.0196\n",
      "Epoch 445/1300\n",
      "16852/16852 [==============================] - 12s 711us/sample - loss: 1.7450 - rmse: 1.2299 - val_loss: 1.7493 - val_rmse: 1.0457\n",
      "Epoch 446/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 1.7385 - rmse: 1.2276 - val_loss: 1.7416 - val_rmse: 1.0418\n",
      "Epoch 447/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.7044 - rmse: 1.2157 - val_loss: 1.7282 - val_rmse: 1.0361\n",
      "Epoch 448/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.7253 - rmse: 1.2232 - val_loss: 1.7518 - val_rmse: 1.0449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 449/1300\n",
      "16852/16852 [==============================] - 12s 704us/sample - loss: 1.7090 - rmse: 1.2176 - val_loss: 1.6795 - val_rmse: 1.0245\n",
      "Epoch 450/1300\n",
      "16852/16852 [==============================] - 12s 702us/sample - loss: 1.7155 - rmse: 1.2213 - val_loss: 1.6903 - val_rmse: 1.0223\n",
      "Epoch 451/1300\n",
      "16852/16852 [==============================] - 12s 693us/sample - loss: 1.7221 - rmse: 1.2217 - val_loss: 1.6896 - val_rmse: 1.0266\n",
      "Epoch 452/1300\n",
      "16852/16852 [==============================] - 12s 693us/sample - loss: 1.7281 - rmse: 1.2233 - val_loss: 1.7787 - val_rmse: 1.0550\n",
      "Epoch 453/1300\n",
      "16852/16852 [==============================] - 12s 693us/sample - loss: 1.7059 - rmse: 1.2174 - val_loss: 1.7419 - val_rmse: 1.0381\n",
      "Epoch 454/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.7093 - rmse: 1.2181 - val_loss: 1.6741 - val_rmse: 1.0213\n",
      "Epoch 455/1300\n",
      "16852/16852 [==============================] - 12s 701us/sample - loss: 1.7164 - rmse: 1.2200 - val_loss: 1.6654 - val_rmse: 1.0162\n",
      "Epoch 456/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.7142 - rmse: 1.2211 - val_loss: 1.7609 - val_rmse: 1.0524\n",
      "Epoch 457/1300\n",
      "16852/16852 [==============================] - 12s 702us/sample - loss: 1.7147 - rmse: 1.2194 - val_loss: 1.7239 - val_rmse: 1.0358\n",
      "Epoch 458/1300\n",
      "16852/16852 [==============================] - 12s 709us/sample - loss: 1.7152 - rmse: 1.2203 - val_loss: 1.7128 - val_rmse: 1.0347\n",
      "Epoch 459/1300\n",
      "16852/16852 [==============================] - 12s 706us/sample - loss: 1.7205 - rmse: 1.2217 - val_loss: 1.6888 - val_rmse: 1.0224\n",
      "Epoch 460/1300\n",
      "16852/16852 [==============================] - 12s 705us/sample - loss: 1.7041 - rmse: 1.2157 - val_loss: 1.7081 - val_rmse: 1.0267\n",
      "Epoch 461/1300\n",
      "16852/16852 [==============================] - 12s 706us/sample - loss: 1.7324 - rmse: 1.2242 - val_loss: 1.6704 - val_rmse: 1.0236\n",
      "Epoch 462/1300\n",
      "16852/16852 [==============================] - 12s 704us/sample - loss: 1.7257 - rmse: 1.2230 - val_loss: 1.7435 - val_rmse: 1.0388\n",
      "Epoch 463/1300\n",
      "16852/16852 [==============================] - 12s 707us/sample - loss: 1.7210 - rmse: 1.2210 - val_loss: 1.6984 - val_rmse: 1.0269\n",
      "Epoch 464/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 1.7121 - rmse: 1.2189 - val_loss: 1.6814 - val_rmse: 1.0270\n",
      "Epoch 465/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 1.7161 - rmse: 1.2199 - val_loss: 1.7027 - val_rmse: 1.0261\n",
      "Epoch 466/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.7036 - rmse: 1.2153 - val_loss: 1.7132 - val_rmse: 1.0444\n",
      "Epoch 467/1300\n",
      "16852/16852 [==============================] - 12s 704us/sample - loss: 1.7238 - rmse: 1.2204 - val_loss: 1.6975 - val_rmse: 1.0275\n",
      "Epoch 468/1300\n",
      "16852/16852 [==============================] - 12s 707us/sample - loss: 1.7025 - rmse: 1.2146 - val_loss: 1.6705 - val_rmse: 1.0177\n",
      "Epoch 469/1300\n",
      "16852/16852 [==============================] - 12s 706us/sample - loss: 1.7154 - rmse: 1.2179 - val_loss: 1.6914 - val_rmse: 1.0309\n",
      "Epoch 470/1300\n",
      "16852/16852 [==============================] - 12s 711us/sample - loss: 1.7157 - rmse: 1.2186 - val_loss: 1.6811 - val_rmse: 1.0235\n",
      "Epoch 471/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.6930 - rmse: 1.2127 - val_loss: 1.7046 - val_rmse: 1.0261\n",
      "Epoch 472/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.7263 - rmse: 1.2225 - val_loss: 1.6735 - val_rmse: 1.0182\n",
      "Epoch 473/1300\n",
      "16852/16852 [==============================] - 12s 715us/sample - loss: 1.7200 - rmse: 1.2207 - val_loss: 1.7197 - val_rmse: 1.0295\n",
      "Epoch 474/1300\n",
      "16852/16852 [==============================] - 12s 705us/sample - loss: 1.7087 - rmse: 1.2164 - val_loss: 1.7062 - val_rmse: 1.0260\n",
      "Epoch 475/1300\n",
      "16852/16852 [==============================] - 12s 708us/sample - loss: 1.7010 - rmse: 1.2153 - val_loss: 1.7315 - val_rmse: 1.0317\n",
      "Epoch 476/1300\n",
      "16852/16852 [==============================] - 12s 708us/sample - loss: 1.7256 - rmse: 1.2230 - val_loss: 1.6961 - val_rmse: 1.0272\n",
      "Epoch 477/1300\n",
      "16852/16852 [==============================] - 12s 707us/sample - loss: 1.6956 - rmse: 1.2127 - val_loss: 1.6726 - val_rmse: 1.0237\n",
      "Epoch 478/1300\n",
      "16852/16852 [==============================] - 12s 715us/sample - loss: 1.7026 - rmse: 1.2148 - val_loss: 1.6410 - val_rmse: 1.0072\n",
      "Epoch 479/1300\n",
      "16852/16852 [==============================] - 12s 706us/sample - loss: 1.6894 - rmse: 1.2101 - val_loss: 1.7133 - val_rmse: 1.0293\n",
      "Epoch 480/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.6984 - rmse: 1.2141 - val_loss: 1.6878 - val_rmse: 1.0234\n",
      "Epoch 481/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.7156 - rmse: 1.2173 - val_loss: 1.6733 - val_rmse: 1.0188\n",
      "Epoch 482/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.7005 - rmse: 1.2146 - val_loss: 1.7345 - val_rmse: 1.0389\n",
      "Epoch 483/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 1.6936 - rmse: 1.2124 - val_loss: 1.6908 - val_rmse: 1.0237\n",
      "Epoch 484/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 1.7056 - rmse: 1.2151 - val_loss: 1.6928 - val_rmse: 1.0250\n",
      "Epoch 485/1300\n",
      "16852/16852 [==============================] - 12s 710us/sample - loss: 1.7050 - rmse: 1.2153 - val_loss: 1.7075 - val_rmse: 1.0261\n",
      "Epoch 486/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.6993 - rmse: 1.2130 - val_loss: 1.7094 - val_rmse: 1.0205\n",
      "Epoch 487/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 1.6890 - rmse: 1.2080 - val_loss: 1.7613 - val_rmse: 1.0485\n",
      "Epoch 488/1300\n",
      "16852/16852 [==============================] - 12s 703us/sample - loss: 1.6963 - rmse: 1.2106 - val_loss: 1.7155 - val_rmse: 1.0258\n",
      "Epoch 489/1300\n",
      "16852/16852 [==============================] - 12s 706us/sample - loss: 1.7123 - rmse: 1.2170 - val_loss: 1.7555 - val_rmse: 1.0501\n",
      "Epoch 490/1300\n",
      "16852/16852 [==============================] - 12s 703us/sample - loss: 1.7156 - rmse: 1.2174 - val_loss: 1.7179 - val_rmse: 1.0319\n",
      "Epoch 491/1300\n",
      "16852/16852 [==============================] - 12s 709us/sample - loss: 1.7026 - rmse: 1.2145 - val_loss: 1.6602 - val_rmse: 1.0148\n",
      "Epoch 492/1300\n",
      "16852/16852 [==============================] - 12s 704us/sample - loss: 1.7207 - rmse: 1.2194 - val_loss: 1.6836 - val_rmse: 1.0158\n",
      "Epoch 493/1300\n",
      "16852/16852 [==============================] - 12s 693us/sample - loss: 1.6951 - rmse: 1.2108 - val_loss: 1.7419 - val_rmse: 1.0414\n",
      "Epoch 494/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.6900 - rmse: 1.2115 - val_loss: 1.7163 - val_rmse: 1.0291\n",
      "Epoch 495/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 1.7065 - rmse: 1.2149 - val_loss: 1.6716 - val_rmse: 1.0156\n",
      "Epoch 496/1300\n",
      "16852/16852 [==============================] - 12s 699us/sample - loss: 1.6889 - rmse: 1.2111 - val_loss: 1.6651 - val_rmse: 1.0114\n",
      "Epoch 497/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.7118 - rmse: 1.2165 - val_loss: 1.6700 - val_rmse: 1.0261\n",
      "Epoch 498/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.7009 - rmse: 1.2116 - val_loss: 1.6886 - val_rmse: 1.0223\n",
      "Epoch 499/1300\n",
      "16852/16852 [==============================] - 12s 712us/sample - loss: 1.6868 - rmse: 1.2090 - val_loss: 1.6369 - val_rmse: 1.0058\n",
      "Epoch 500/1300\n",
      "16852/16852 [==============================] - 12s 706us/sample - loss: 1.6842 - rmse: 1.2065 - val_loss: 1.6795 - val_rmse: 1.0189\n",
      "Epoch 501/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 1.7064 - rmse: 1.2175 - val_loss: 1.6686 - val_rmse: 1.0165\n",
      "Epoch 502/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.7080 - rmse: 1.2139 - val_loss: 1.7505 - val_rmse: 1.0383\n",
      "Epoch 503/1300\n",
      "16852/16852 [==============================] - 12s 699us/sample - loss: 1.6903 - rmse: 1.2089 - val_loss: 1.7182 - val_rmse: 1.0279\n",
      "Epoch 504/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 1.7084 - rmse: 1.2142 - val_loss: 1.7727 - val_rmse: 1.0412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 505/1300\n",
      "16852/16852 [==============================] - 12s 693us/sample - loss: 1.6805 - rmse: 1.2061 - val_loss: 1.7134 - val_rmse: 1.0249\n",
      "Epoch 506/1300\n",
      "16852/16852 [==============================] - 12s 702us/sample - loss: 1.7148 - rmse: 1.2169 - val_loss: 1.6280 - val_rmse: 1.0031\n",
      "Epoch 507/1300\n",
      "16852/16852 [==============================] - 12s 704us/sample - loss: 1.7048 - rmse: 1.2136 - val_loss: 1.6737 - val_rmse: 1.0170\n",
      "Epoch 508/1300\n",
      "16852/16852 [==============================] - 12s 704us/sample - loss: 1.6887 - rmse: 1.2080 - val_loss: 1.6823 - val_rmse: 1.0204\n",
      "Epoch 509/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.7142 - rmse: 1.2178 - val_loss: 1.7042 - val_rmse: 1.0216\n",
      "Epoch 510/1300\n",
      "16852/16852 [==============================] - 12s 708us/sample - loss: 1.6767 - rmse: 1.2055 - val_loss: 1.6572 - val_rmse: 1.0179\n",
      "Epoch 511/1300\n",
      "16852/16852 [==============================] - 12s 704us/sample - loss: 1.6952 - rmse: 1.2122 - val_loss: 1.6792 - val_rmse: 1.0224\n",
      "Epoch 512/1300\n",
      "16852/16852 [==============================] - 12s 705us/sample - loss: 1.6826 - rmse: 1.2065 - val_loss: 1.8596 - val_rmse: 1.0893\n",
      "Epoch 513/1300\n",
      "16852/16852 [==============================] - 12s 704us/sample - loss: 1.6836 - rmse: 1.2068 - val_loss: 1.6838 - val_rmse: 1.0188\n",
      "Epoch 514/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.6751 - rmse: 1.2043 - val_loss: 1.7108 - val_rmse: 1.0291\n",
      "Epoch 515/1300\n",
      "16852/16852 [==============================] - 12s 702us/sample - loss: 1.6971 - rmse: 1.2113 - val_loss: 1.6825 - val_rmse: 1.0154\n",
      "Epoch 516/1300\n",
      "16852/16852 [==============================] - 12s 707us/sample - loss: 1.6979 - rmse: 1.2106 - val_loss: 1.6785 - val_rmse: 1.0160\n",
      "Epoch 517/1300\n",
      "16852/16852 [==============================] - 12s 707us/sample - loss: 1.6816 - rmse: 1.2067 - val_loss: 1.7269 - val_rmse: 1.0320\n",
      "Epoch 518/1300\n",
      "16852/16852 [==============================] - 12s 702us/sample - loss: 1.7056 - rmse: 1.2145 - val_loss: 1.6694 - val_rmse: 1.0154\n",
      "Epoch 519/1300\n",
      "16852/16852 [==============================] - 12s 707us/sample - loss: 1.6872 - rmse: 1.2068 - val_loss: 1.6880 - val_rmse: 1.0208\n",
      "Epoch 520/1300\n",
      "16852/16852 [==============================] - 12s 707us/sample - loss: 1.6770 - rmse: 1.2043 - val_loss: 1.7201 - val_rmse: 1.0340\n",
      "Epoch 521/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 1.6835 - rmse: 1.2070 - val_loss: 1.7204 - val_rmse: 1.0253\n",
      "Epoch 522/1300\n",
      "16852/16852 [==============================] - 13s 749us/sample - loss: 1.6960 - rmse: 1.2097 - val_loss: 1.7055 - val_rmse: 1.0254\n",
      "Epoch 523/1300\n",
      "16852/16852 [==============================] - 13s 748us/sample - loss: 1.6739 - rmse: 1.2042 - val_loss: 1.6970 - val_rmse: 1.0212\n",
      "Epoch 524/1300\n",
      "16852/16852 [==============================] - 12s 716us/sample - loss: 1.6828 - rmse: 1.2064 - val_loss: 1.6636 - val_rmse: 1.0093\n",
      "Epoch 525/1300\n",
      "16852/16852 [==============================] - 12s 718us/sample - loss: 1.6792 - rmse: 1.2054 - val_loss: 1.7351 - val_rmse: 1.0288\n",
      "Epoch 526/1300\n",
      "16852/16852 [==============================] - 12s 739us/sample - loss: 1.6754 - rmse: 1.2045 - val_loss: 1.7928 - val_rmse: 1.0627\n",
      "Epoch 527/1300\n",
      "16852/16852 [==============================] - 12s 701us/sample - loss: 1.6782 - rmse: 1.2050 - val_loss: 1.7377 - val_rmse: 1.0350\n",
      "Epoch 528/1300\n",
      "16852/16852 [==============================] - 12s 700us/sample - loss: 1.6702 - rmse: 1.2027 - val_loss: 1.6949 - val_rmse: 1.0212\n",
      "Epoch 529/1300\n",
      "16852/16852 [==============================] - 12s 702us/sample - loss: 1.6770 - rmse: 1.2039 - val_loss: 1.7213 - val_rmse: 1.0252\n",
      "Epoch 530/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 1.6908 - rmse: 1.2092 - val_loss: 1.6866 - val_rmse: 1.0234\n",
      "Epoch 531/1300\n",
      "16852/16852 [==============================] - 12s 700us/sample - loss: 1.6662 - rmse: 1.2021 - val_loss: 1.7220 - val_rmse: 1.0281\n",
      "Epoch 532/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.6627 - rmse: 1.2007 - val_loss: 1.6635 - val_rmse: 1.0091\n",
      "Epoch 533/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 1.6662 - rmse: 1.2014 - val_loss: 1.6719 - val_rmse: 1.0151\n",
      "Epoch 534/1300\n",
      "16852/16852 [==============================] - 12s 699us/sample - loss: 1.6576 - rmse: 1.1997 - val_loss: 1.6688 - val_rmse: 1.0173\n",
      "Epoch 535/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 1.6648 - rmse: 1.2004 - val_loss: 1.6670 - val_rmse: 1.0102\n",
      "Epoch 536/1300\n",
      "16852/16852 [==============================] - 12s 702us/sample - loss: 1.6722 - rmse: 1.2033 - val_loss: 1.6717 - val_rmse: 1.0158\n",
      "Epoch 537/1300\n",
      "16852/16852 [==============================] - 12s 700us/sample - loss: 1.6588 - rmse: 1.1980 - val_loss: 1.6971 - val_rmse: 1.0227\n",
      "Epoch 538/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.6877 - rmse: 1.2089 - val_loss: 1.6885 - val_rmse: 1.0213\n",
      "Epoch 539/1300\n",
      "16852/16852 [==============================] - 12s 706us/sample - loss: 1.6644 - rmse: 1.2006 - val_loss: 1.6404 - val_rmse: 0.9999\n",
      "Epoch 540/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.6596 - rmse: 1.1966 - val_loss: 1.7445 - val_rmse: 1.0388\n",
      "Epoch 541/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 1.6486 - rmse: 1.1960 - val_loss: 1.6756 - val_rmse: 1.0138\n",
      "Epoch 542/1300\n",
      "16852/16852 [==============================] - 12s 700us/sample - loss: 1.6671 - rmse: 1.2021 - val_loss: 1.6859 - val_rmse: 1.0147\n",
      "Epoch 543/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 1.6517 - rmse: 1.1955 - val_loss: 1.6836 - val_rmse: 1.0157\n",
      "Epoch 544/1300\n",
      "16852/16852 [==============================] - 12s 699us/sample - loss: 1.6706 - rmse: 1.2002 - val_loss: 1.6729 - val_rmse: 1.0107\n",
      "Epoch 545/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.6674 - rmse: 1.2002 - val_loss: 1.7151 - val_rmse: 1.0238\n",
      "Epoch 546/1300\n",
      "16852/16852 [==============================] - 12s 699us/sample - loss: 1.6678 - rmse: 1.2006 - val_loss: 1.6512 - val_rmse: 1.0048\n",
      "Epoch 547/1300\n",
      "16852/16852 [==============================] - 12s 701us/sample - loss: 1.6677 - rmse: 1.2012 - val_loss: 1.6758 - val_rmse: 1.0190\n",
      "Epoch 548/1300\n",
      "16852/16852 [==============================] - 12s 704us/sample - loss: 1.6671 - rmse: 1.2006 - val_loss: 1.6631 - val_rmse: 1.0096\n",
      "Epoch 549/1300\n",
      "16852/16852 [==============================] - 12s 700us/sample - loss: 1.6623 - rmse: 1.1981 - val_loss: 1.6727 - val_rmse: 1.0084\n",
      "Epoch 550/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 1.6696 - rmse: 1.2008 - val_loss: 1.7488 - val_rmse: 1.0345\n",
      "Epoch 551/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.6578 - rmse: 1.1984 - val_loss: 1.6921 - val_rmse: 1.0209\n",
      "Epoch 552/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.6572 - rmse: 1.1967 - val_loss: 1.6831 - val_rmse: 1.0128\n",
      "Epoch 553/1300\n",
      "16852/16852 [==============================] - 12s 709us/sample - loss: 1.6491 - rmse: 1.1946 - val_loss: 1.7425 - val_rmse: 1.0296\n",
      "Epoch 554/1300\n",
      "16852/16852 [==============================] - 12s 710us/sample - loss: 1.6786 - rmse: 1.2031 - val_loss: 1.7058 - val_rmse: 1.0207\n",
      "Epoch 555/1300\n",
      "16852/16852 [==============================] - 12s 709us/sample - loss: 1.6580 - rmse: 1.1963 - val_loss: 1.6762 - val_rmse: 1.0135\n",
      "Epoch 556/1300\n",
      "16852/16852 [==============================] - 12s 699us/sample - loss: 1.6727 - rmse: 1.2014 - val_loss: 1.6549 - val_rmse: 1.0066\n",
      "Epoch 557/1300\n",
      "16852/16852 [==============================] - 12s 707us/sample - loss: 1.6593 - rmse: 1.1972 - val_loss: 1.6239 - val_rmse: 1.0022\n",
      "Epoch 558/1300\n",
      "16852/16852 [==============================] - 12s 710us/sample - loss: 1.6715 - rmse: 1.2021 - val_loss: 1.6498 - val_rmse: 1.0041\n",
      "Epoch 559/1300\n",
      "16852/16852 [==============================] - 12s 713us/sample - loss: 1.6743 - rmse: 1.2008 - val_loss: 1.6751 - val_rmse: 1.0147\n",
      "Epoch 560/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 1.6496 - rmse: 1.1939 - val_loss: 1.6907 - val_rmse: 1.0157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 561/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.6947 - rmse: 1.2083 - val_loss: 1.7112 - val_rmse: 1.0320\n",
      "Epoch 562/1300\n",
      "16852/16852 [==============================] - 12s 700us/sample - loss: 1.6777 - rmse: 1.2034 - val_loss: 1.6555 - val_rmse: 1.0049\n",
      "Epoch 563/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.6505 - rmse: 1.1963 - val_loss: 1.6866 - val_rmse: 1.0130\n",
      "Epoch 564/1300\n",
      "16852/16852 [==============================] - 12s 701us/sample - loss: 1.6543 - rmse: 1.1957 - val_loss: 1.8332 - val_rmse: 1.0686\n",
      "Epoch 565/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.6650 - rmse: 1.1999 - val_loss: 1.6794 - val_rmse: 1.0093\n",
      "Epoch 566/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 1.6680 - rmse: 1.1999 - val_loss: 1.7046 - val_rmse: 1.0218\n",
      "Epoch 567/1300\n",
      "16852/16852 [==============================] - 12s 699us/sample - loss: 1.6800 - rmse: 1.2031 - val_loss: 1.7427 - val_rmse: 1.0289\n",
      "Epoch 568/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.6619 - rmse: 1.1980 - val_loss: 1.6678 - val_rmse: 1.0084\n",
      "Epoch 569/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.6580 - rmse: 1.1951 - val_loss: 1.7051 - val_rmse: 1.0198\n",
      "Epoch 570/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.6712 - rmse: 1.2005 - val_loss: 1.7134 - val_rmse: 1.0226\n",
      "Epoch 571/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.6619 - rmse: 1.1995 - val_loss: 1.6974 - val_rmse: 1.0129\n",
      "Epoch 572/1300\n",
      "16852/16852 [==============================] - 12s 700us/sample - loss: 1.6564 - rmse: 1.1968 - val_loss: 1.7551 - val_rmse: 1.0295\n",
      "Epoch 573/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.6588 - rmse: 1.1976 - val_loss: 1.6925 - val_rmse: 1.0245\n",
      "Epoch 574/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 1.6479 - rmse: 1.1926 - val_loss: 1.7107 - val_rmse: 1.0190\n",
      "Epoch 575/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.6671 - rmse: 1.1998 - val_loss: 1.6852 - val_rmse: 1.0121\n",
      "Epoch 576/1300\n",
      "16852/16852 [==============================] - 12s 705us/sample - loss: 1.6535 - rmse: 1.1950 - val_loss: 1.6340 - val_rmse: 0.9944\n",
      "Epoch 577/1300\n",
      "16852/16852 [==============================] - 12s 699us/sample - loss: 1.6518 - rmse: 1.1949 - val_loss: 1.7317 - val_rmse: 1.0276\n",
      "Epoch 578/1300\n",
      "16852/16852 [==============================] - 12s 693us/sample - loss: 1.6471 - rmse: 1.1930 - val_loss: 1.6733 - val_rmse: 1.0099\n",
      "Epoch 579/1300\n",
      "16852/16852 [==============================] - 12s 699us/sample - loss: 1.6536 - rmse: 1.1925 - val_loss: 1.7294 - val_rmse: 1.0290\n",
      "Epoch 580/1300\n",
      "16852/16852 [==============================] - 12s 700us/sample - loss: 1.6512 - rmse: 1.1961 - val_loss: 1.6587 - val_rmse: 1.0030\n",
      "Epoch 581/1300\n",
      "16852/16852 [==============================] - 12s 701us/sample - loss: 1.6806 - rmse: 1.2032 - val_loss: 1.7862 - val_rmse: 1.0594\n",
      "Epoch 582/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 1.6580 - rmse: 1.1942 - val_loss: 1.7191 - val_rmse: 1.0244\n",
      "Epoch 583/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 1.6373 - rmse: 1.1915 - val_loss: 1.6912 - val_rmse: 1.0129\n",
      "Epoch 584/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 1.6506 - rmse: 1.1931 - val_loss: 1.6859 - val_rmse: 1.0150\n",
      "Epoch 585/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.6410 - rmse: 1.1915 - val_loss: 1.7151 - val_rmse: 1.0262\n",
      "Epoch 586/1300\n",
      "16852/16852 [==============================] - 12s 714us/sample - loss: 1.6497 - rmse: 1.1926 - val_loss: 1.6804 - val_rmse: 1.0070\n",
      "Epoch 587/1300\n",
      "16852/16852 [==============================] - 12s 708us/sample - loss: 1.6352 - rmse: 1.1884 - val_loss: 1.6459 - val_rmse: 0.9985\n",
      "Epoch 588/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.6503 - rmse: 1.1935 - val_loss: 1.6883 - val_rmse: 1.0164\n",
      "Epoch 589/1300\n",
      "16852/16852 [==============================] - 12s 699us/sample - loss: 1.6351 - rmse: 1.1891 - val_loss: 1.7197 - val_rmse: 1.0234\n",
      "Epoch 590/1300\n",
      "16852/16852 [==============================] - 12s 700us/sample - loss: 1.6578 - rmse: 1.1963 - val_loss: 1.6449 - val_rmse: 0.9978\n",
      "Epoch 591/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.6518 - rmse: 1.1937 - val_loss: 1.6646 - val_rmse: 1.0052\n",
      "Epoch 592/1300\n",
      "16852/16852 [==============================] - 12s 710us/sample - loss: 1.6416 - rmse: 1.1897 - val_loss: 1.7340 - val_rmse: 1.0203\n",
      "Epoch 593/1300\n",
      "16852/16852 [==============================] - 12s 710us/sample - loss: 1.6317 - rmse: 1.1888 - val_loss: 1.6528 - val_rmse: 1.0011\n",
      "Epoch 594/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.6405 - rmse: 1.1904 - val_loss: 1.6831 - val_rmse: 1.0160\n",
      "Epoch 595/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 1.6457 - rmse: 1.1917 - val_loss: 1.6896 - val_rmse: 1.0116\n",
      "Epoch 596/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.6564 - rmse: 1.1938 - val_loss: 1.6816 - val_rmse: 1.0097\n",
      "Epoch 597/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 1.6595 - rmse: 1.1971 - val_loss: 1.6737 - val_rmse: 1.0106\n",
      "Epoch 598/1300\n",
      "16852/16852 [==============================] - 12s 704us/sample - loss: 1.6487 - rmse: 1.1928 - val_loss: 1.6548 - val_rmse: 1.0062\n",
      "Epoch 599/1300\n",
      "16852/16852 [==============================] - 12s 709us/sample - loss: 1.6318 - rmse: 1.1855 - val_loss: 1.6926 - val_rmse: 1.0144\n",
      "Epoch 600/1300\n",
      "16852/16852 [==============================] - 12s 708us/sample - loss: 1.6470 - rmse: 1.1918 - val_loss: 1.7347 - val_rmse: 1.0302\n",
      "Epoch 601/1300\n",
      "16852/16852 [==============================] - 12s 709us/sample - loss: 1.6548 - rmse: 1.1939 - val_loss: 1.7627 - val_rmse: 1.0389\n",
      "Epoch 602/1300\n",
      "16852/16852 [==============================] - 12s 710us/sample - loss: 1.6298 - rmse: 1.1850 - val_loss: 1.6335 - val_rmse: 0.9964\n",
      "Epoch 603/1300\n",
      "16852/16852 [==============================] - 12s 701us/sample - loss: 1.6353 - rmse: 1.1894 - val_loss: 1.6729 - val_rmse: 1.0068\n",
      "Epoch 604/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 1.6451 - rmse: 1.1908 - val_loss: 1.6744 - val_rmse: 1.0073\n",
      "Epoch 605/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.6357 - rmse: 1.1872 - val_loss: 1.6397 - val_rmse: 1.0005\n",
      "Epoch 606/1300\n",
      "16852/16852 [==============================] - 12s 709us/sample - loss: 1.6147 - rmse: 1.1798 - val_loss: 1.7785 - val_rmse: 1.0376\n",
      "Epoch 607/1300\n",
      "16852/16852 [==============================] - 12s 708us/sample - loss: 1.6599 - rmse: 1.1954 - val_loss: 1.7029 - val_rmse: 1.0209\n",
      "Epoch 608/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.6315 - rmse: 1.1866 - val_loss: 1.7156 - val_rmse: 1.0151\n",
      "Epoch 609/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.6541 - rmse: 1.1926 - val_loss: 1.6433 - val_rmse: 0.9990\n",
      "Epoch 610/1300\n",
      "16852/16852 [==============================] - 12s 699us/sample - loss: 1.6387 - rmse: 1.1871 - val_loss: 1.6784 - val_rmse: 1.0187\n",
      "Epoch 611/1300\n",
      "16852/16852 [==============================] - 12s 699us/sample - loss: 1.6351 - rmse: 1.1904 - val_loss: 1.6929 - val_rmse: 1.0121\n",
      "Epoch 612/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.6164 - rmse: 1.1815 - val_loss: 1.7204 - val_rmse: 1.0188\n",
      "Epoch 613/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.6454 - rmse: 1.1898 - val_loss: 1.6790 - val_rmse: 1.0116\n",
      "Epoch 614/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 1.6373 - rmse: 1.1885 - val_loss: 1.7336 - val_rmse: 1.0219\n",
      "Epoch 615/1300\n",
      "16852/16852 [==============================] - 12s 706us/sample - loss: 1.6071 - rmse: 1.1787 - val_loss: 1.6392 - val_rmse: 0.9979\n",
      "Epoch 616/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 1.6377 - rmse: 1.1884 - val_loss: 1.6443 - val_rmse: 0.9979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 617/1300\n",
      "16852/16852 [==============================] - 12s 708us/sample - loss: 1.6420 - rmse: 1.1896 - val_loss: 1.6582 - val_rmse: 1.0008\n",
      "Epoch 618/1300\n",
      "16852/16852 [==============================] - 12s 710us/sample - loss: 1.6280 - rmse: 1.1848 - val_loss: 1.6906 - val_rmse: 1.0139\n",
      "Epoch 619/1300\n",
      "16852/16852 [==============================] - 12s 704us/sample - loss: 1.6255 - rmse: 1.1840 - val_loss: 1.7581 - val_rmse: 1.0393\n",
      "Epoch 620/1300\n",
      "16852/16852 [==============================] - 12s 707us/sample - loss: 1.6230 - rmse: 1.1845 - val_loss: 1.6530 - val_rmse: 1.0020\n",
      "Epoch 621/1300\n",
      "16852/16852 [==============================] - 12s 708us/sample - loss: 1.6406 - rmse: 1.1900 - val_loss: 1.6503 - val_rmse: 0.9983\n",
      "Epoch 622/1300\n",
      "16852/16852 [==============================] - 12s 709us/sample - loss: 1.6254 - rmse: 1.1840 - val_loss: 1.6580 - val_rmse: 0.9991\n",
      "Epoch 623/1300\n",
      "16852/16852 [==============================] - 12s 710us/sample - loss: 1.6191 - rmse: 1.1825 - val_loss: 1.7208 - val_rmse: 1.0218\n",
      "Epoch 624/1300\n",
      "16852/16852 [==============================] - 12s 716us/sample - loss: 1.6409 - rmse: 1.1883 - val_loss: 1.6296 - val_rmse: 0.9935\n",
      "Epoch 625/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 1.6432 - rmse: 1.1869 - val_loss: 1.7512 - val_rmse: 1.0426\n",
      "Epoch 626/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 1.6289 - rmse: 1.1855 - val_loss: 1.6368 - val_rmse: 0.9965\n",
      "Epoch 627/1300\n",
      "16852/16852 [==============================] - 12s 705us/sample - loss: 1.6354 - rmse: 1.1869 - val_loss: 1.6632 - val_rmse: 1.0025\n",
      "Epoch 628/1300\n",
      "16852/16852 [==============================] - 12s 705us/sample - loss: 1.6222 - rmse: 1.1817 - val_loss: 1.6317 - val_rmse: 0.9922\n",
      "Epoch 629/1300\n",
      "16852/16852 [==============================] - 12s 699us/sample - loss: 1.6411 - rmse: 1.1896 - val_loss: 1.6571 - val_rmse: 1.0013\n",
      "Epoch 630/1300\n",
      "16852/16852 [==============================] - 12s 708us/sample - loss: 1.6189 - rmse: 1.1811 - val_loss: 1.6907 - val_rmse: 1.0085\n",
      "Epoch 631/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 1.6333 - rmse: 1.1864 - val_loss: 1.6578 - val_rmse: 0.9990\n",
      "Epoch 632/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 1.6277 - rmse: 1.1837 - val_loss: 1.7183 - val_rmse: 1.0165\n",
      "Epoch 633/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.6248 - rmse: 1.1825 - val_loss: 1.6795 - val_rmse: 1.0044\n",
      "Epoch 634/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.6277 - rmse: 1.1851 - val_loss: 1.7136 - val_rmse: 1.0272\n",
      "Epoch 635/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 1.6395 - rmse: 1.1879 - val_loss: 1.6750 - val_rmse: 1.0081\n",
      "Epoch 636/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.6275 - rmse: 1.1833 - val_loss: 1.6424 - val_rmse: 1.0048\n",
      "Epoch 637/1300\n",
      "16852/16852 [==============================] - 12s 709us/sample - loss: 1.5970 - rmse: 1.1727 - val_loss: 1.6308 - val_rmse: 0.9943\n",
      "Epoch 638/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.6189 - rmse: 1.1809 - val_loss: 1.6285 - val_rmse: 0.9977\n",
      "Epoch 639/1300\n",
      "16852/16852 [==============================] - 12s 701us/sample - loss: 1.6290 - rmse: 1.1853 - val_loss: 1.6381 - val_rmse: 0.9929\n",
      "Epoch 640/1300\n",
      "16852/16852 [==============================] - 12s 711us/sample - loss: 1.6476 - rmse: 1.1900 - val_loss: 1.7055 - val_rmse: 1.0282\n",
      "Epoch 641/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.6319 - rmse: 1.1851 - val_loss: 1.6467 - val_rmse: 1.0064\n",
      "Epoch 642/1300\n",
      "16852/16852 [==============================] - 12s 707us/sample - loss: 1.6500 - rmse: 1.1899 - val_loss: 1.6360 - val_rmse: 0.9952\n",
      "Epoch 643/1300\n",
      "16852/16852 [==============================] - 12s 709us/sample - loss: 1.6162 - rmse: 1.1801 - val_loss: 1.6557 - val_rmse: 0.9982\n",
      "Epoch 644/1300\n",
      "16852/16852 [==============================] - 12s 707us/sample - loss: 1.6123 - rmse: 1.1811 - val_loss: 1.6404 - val_rmse: 0.9945\n",
      "Epoch 645/1300\n",
      "16852/16852 [==============================] - 12s 709us/sample - loss: 1.6351 - rmse: 1.1853 - val_loss: 1.7382 - val_rmse: 1.0276\n",
      "Epoch 646/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 1.6308 - rmse: 1.1844 - val_loss: 1.6623 - val_rmse: 0.9990\n",
      "Epoch 647/1300\n",
      "16852/16852 [==============================] - 12s 699us/sample - loss: 1.6304 - rmse: 1.1849 - val_loss: 1.7169 - val_rmse: 1.0193\n",
      "Epoch 648/1300\n",
      "16852/16852 [==============================] - 12s 711us/sample - loss: 1.6247 - rmse: 1.1838 - val_loss: 1.6997 - val_rmse: 1.0090\n",
      "Epoch 649/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.6259 - rmse: 1.1833 - val_loss: 1.6379 - val_rmse: 0.9946\n",
      "Epoch 650/1300\n",
      "16852/16852 [==============================] - 12s 706us/sample - loss: 1.6355 - rmse: 1.1855 - val_loss: 1.6298 - val_rmse: 0.9890\n",
      "Epoch 651/1300\n",
      "16852/16852 [==============================] - 12s 702us/sample - loss: 1.5992 - rmse: 1.1760 - val_loss: 1.6523 - val_rmse: 1.0020\n",
      "Epoch 652/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.6205 - rmse: 1.1812 - val_loss: 1.6799 - val_rmse: 1.0056\n",
      "Epoch 653/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.6259 - rmse: 1.1828 - val_loss: 1.6356 - val_rmse: 0.9969\n",
      "Epoch 654/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.6159 - rmse: 1.1822 - val_loss: 1.6895 - val_rmse: 1.0095\n",
      "Epoch 655/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.6079 - rmse: 1.1779 - val_loss: 1.6436 - val_rmse: 0.9929\n",
      "Epoch 656/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 1.6357 - rmse: 1.1843 - val_loss: 1.6194 - val_rmse: 0.9986\n",
      "Epoch 657/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.6053 - rmse: 1.1770 - val_loss: 1.6591 - val_rmse: 1.0002\n",
      "Epoch 658/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.5955 - rmse: 1.1740 - val_loss: 1.7221 - val_rmse: 1.0250\n",
      "Epoch 659/1300\n",
      "16852/16852 [==============================] - 12s 709us/sample - loss: 1.6160 - rmse: 1.1790 - val_loss: 1.6662 - val_rmse: 1.0010\n",
      "Epoch 660/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.6252 - rmse: 1.1829 - val_loss: 1.6868 - val_rmse: 1.0132\n",
      "Epoch 661/1300\n",
      "16852/16852 [==============================] - 12s 710us/sample - loss: 1.5910 - rmse: 1.1721 - val_loss: 1.6363 - val_rmse: 0.9984\n",
      "Epoch 662/1300\n",
      "16852/16852 [==============================] - 12s 712us/sample - loss: 1.5804 - rmse: 1.1671 - val_loss: 1.6732 - val_rmse: 0.9986\n",
      "Epoch 663/1300\n",
      "16852/16852 [==============================] - 12s 710us/sample - loss: 1.6186 - rmse: 1.1809 - val_loss: 1.6708 - val_rmse: 1.0103\n",
      "Epoch 664/1300\n",
      "16852/16852 [==============================] - 12s 705us/sample - loss: 1.6107 - rmse: 1.1774 - val_loss: 1.7111 - val_rmse: 1.0185\n",
      "Epoch 665/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 1.6096 - rmse: 1.1786 - val_loss: 1.7341 - val_rmse: 1.0178\n",
      "Epoch 666/1300\n",
      "16852/16852 [==============================] - 12s 716us/sample - loss: 1.6305 - rmse: 1.1831 - val_loss: 1.6259 - val_rmse: 0.9876\n",
      "Epoch 667/1300\n",
      "16852/16852 [==============================] - 12s 690us/sample - loss: 1.6169 - rmse: 1.1777 - val_loss: 1.6496 - val_rmse: 0.9964\n",
      "Epoch 668/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 1.6200 - rmse: 1.1806 - val_loss: 1.7128 - val_rmse: 1.0228\n",
      "Epoch 669/1300\n",
      "16852/16852 [==============================] - 12s 706us/sample - loss: 1.6271 - rmse: 1.1807 - val_loss: 1.6096 - val_rmse: 0.9844\n",
      "Epoch 670/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.6174 - rmse: 1.1791 - val_loss: 1.6279 - val_rmse: 0.9889\n",
      "Epoch 671/1300\n",
      "16852/16852 [==============================] - 12s 700us/sample - loss: 1.6014 - rmse: 1.1735 - val_loss: 1.7103 - val_rmse: 1.0137\n",
      "Epoch 672/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 1.5940 - rmse: 1.1719 - val_loss: 1.6379 - val_rmse: 0.9931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 673/1300\n",
      "16852/16852 [==============================] - 12s 705us/sample - loss: 1.6194 - rmse: 1.1796 - val_loss: 1.6674 - val_rmse: 1.0002\n",
      "Epoch 674/1300\n",
      "16852/16852 [==============================] - 12s 704us/sample - loss: 1.6273 - rmse: 1.1817 - val_loss: 1.6024 - val_rmse: 0.9824\n",
      "Epoch 675/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.6130 - rmse: 1.1777 - val_loss: 1.6221 - val_rmse: 0.9839\n",
      "Epoch 676/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.5978 - rmse: 1.1731 - val_loss: 1.6196 - val_rmse: 0.9854\n",
      "Epoch 677/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.6191 - rmse: 1.1786 - val_loss: 1.6422 - val_rmse: 0.9994\n",
      "Epoch 678/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.5986 - rmse: 1.1734 - val_loss: 1.6807 - val_rmse: 1.0101\n",
      "Epoch 679/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.6170 - rmse: 1.1790 - val_loss: 1.6634 - val_rmse: 1.0018\n",
      "Epoch 680/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 1.5958 - rmse: 1.1715 - val_loss: 1.6856 - val_rmse: 1.0194\n",
      "Epoch 681/1300\n",
      "16852/16852 [==============================] - 12s 700us/sample - loss: 1.5989 - rmse: 1.1732 - val_loss: 1.6465 - val_rmse: 0.9956\n",
      "Epoch 682/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.6051 - rmse: 1.1748 - val_loss: 1.6281 - val_rmse: 0.9913\n",
      "Epoch 683/1300\n",
      "16852/16852 [==============================] - 12s 706us/sample - loss: 1.5916 - rmse: 1.1705 - val_loss: 1.6527 - val_rmse: 0.9911\n",
      "Epoch 684/1300\n",
      "16852/16852 [==============================] - 12s 709us/sample - loss: 1.5894 - rmse: 1.1694 - val_loss: 1.6495 - val_rmse: 0.9978\n",
      "Epoch 685/1300\n",
      "16852/16852 [==============================] - 12s 707us/sample - loss: 1.6028 - rmse: 1.1739 - val_loss: 1.6359 - val_rmse: 0.9879\n",
      "Epoch 686/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.5930 - rmse: 1.1722 - val_loss: 1.6618 - val_rmse: 0.9949\n",
      "Epoch 687/1300\n",
      "16852/16852 [==============================] - 12s 702us/sample - loss: 1.5928 - rmse: 1.1694 - val_loss: 1.6673 - val_rmse: 0.9973\n",
      "Epoch 688/1300\n",
      "16852/16852 [==============================] - 12s 709us/sample - loss: 1.5925 - rmse: 1.1713 - val_loss: 1.6462 - val_rmse: 0.9951\n",
      "Epoch 689/1300\n",
      "16852/16852 [==============================] - 12s 711us/sample - loss: 1.6080 - rmse: 1.1762 - val_loss: 1.6893 - val_rmse: 1.0023\n",
      "Epoch 690/1300\n",
      "16852/16852 [==============================] - 12s 706us/sample - loss: 1.5971 - rmse: 1.1741 - val_loss: 1.6077 - val_rmse: 0.9903\n",
      "Epoch 691/1300\n",
      "16852/16852 [==============================] - 12s 710us/sample - loss: 1.5794 - rmse: 1.1655 - val_loss: 1.6280 - val_rmse: 0.9901\n",
      "Epoch 692/1300\n",
      "16852/16852 [==============================] - 12s 706us/sample - loss: 1.5871 - rmse: 1.1696 - val_loss: 1.6989 - val_rmse: 1.0096\n",
      "Epoch 693/1300\n",
      "16852/16852 [==============================] - 12s 708us/sample - loss: 1.5908 - rmse: 1.1701 - val_loss: 1.6858 - val_rmse: 1.0056\n",
      "Epoch 694/1300\n",
      "16852/16852 [==============================] - 12s 711us/sample - loss: 1.5982 - rmse: 1.1728 - val_loss: 1.6544 - val_rmse: 0.9955\n",
      "Epoch 695/1300\n",
      "16852/16852 [==============================] - 12s 700us/sample - loss: 1.5951 - rmse: 1.1715 - val_loss: 1.7204 - val_rmse: 1.0171\n",
      "Epoch 696/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.6076 - rmse: 1.1753 - val_loss: 1.6559 - val_rmse: 0.9928\n",
      "Epoch 697/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 1.5924 - rmse: 1.1696 - val_loss: 1.6232 - val_rmse: 0.9833\n",
      "Epoch 698/1300\n",
      "16852/16852 [==============================] - 12s 700us/sample - loss: 1.5995 - rmse: 1.1717 - val_loss: 1.6766 - val_rmse: 1.0047\n",
      "Epoch 699/1300\n",
      "16852/16852 [==============================] - 12s 704us/sample - loss: 1.6007 - rmse: 1.1730 - val_loss: 1.5877 - val_rmse: 0.9766\n",
      "Epoch 700/1300\n",
      "16852/16852 [==============================] - 12s 699us/sample - loss: 1.5827 - rmse: 1.1657 - val_loss: 1.6557 - val_rmse: 0.9918\n",
      "Epoch 701/1300\n",
      "16852/16852 [==============================] - 12s 720us/sample - loss: 1.5986 - rmse: 1.1728 - val_loss: 1.6369 - val_rmse: 0.9863\n",
      "Epoch 702/1300\n",
      "16852/16852 [==============================] - 12s 708us/sample - loss: 1.5940 - rmse: 1.1707 - val_loss: 1.6673 - val_rmse: 1.0055\n",
      "Epoch 703/1300\n",
      "16852/16852 [==============================] - 12s 709us/sample - loss: 1.5894 - rmse: 1.1693 - val_loss: 1.7230 - val_rmse: 1.0207\n",
      "Epoch 704/1300\n",
      "16852/16852 [==============================] - 12s 710us/sample - loss: 1.5969 - rmse: 1.1706 - val_loss: 1.6566 - val_rmse: 0.9959\n",
      "Epoch 705/1300\n",
      "16852/16852 [==============================] - 12s 712us/sample - loss: 1.5922 - rmse: 1.1708 - val_loss: 1.6748 - val_rmse: 0.9983\n",
      "Epoch 706/1300\n",
      "16852/16852 [==============================] - 12s 711us/sample - loss: 1.5941 - rmse: 1.1712 - val_loss: 1.6558 - val_rmse: 0.9949\n",
      "Epoch 707/1300\n",
      "16852/16852 [==============================] - 12s 713us/sample - loss: 1.5926 - rmse: 1.1694 - val_loss: 1.6250 - val_rmse: 0.9831\n",
      "Epoch 708/1300\n",
      "16852/16852 [==============================] - 12s 710us/sample - loss: 1.5813 - rmse: 1.1684 - val_loss: 1.6571 - val_rmse: 0.9889\n",
      "Epoch 709/1300\n",
      "16852/16852 [==============================] - 12s 709us/sample - loss: 1.5900 - rmse: 1.1696 - val_loss: 1.6722 - val_rmse: 0.9953\n",
      "Epoch 710/1300\n",
      "16852/16852 [==============================] - 12s 708us/sample - loss: 1.5973 - rmse: 1.1722 - val_loss: 1.7080 - val_rmse: 1.0142\n",
      "Epoch 711/1300\n",
      "16852/16852 [==============================] - 12s 709us/sample - loss: 1.5675 - rmse: 1.1624 - val_loss: 1.6301 - val_rmse: 0.9874\n",
      "Epoch 712/1300\n",
      "16852/16852 [==============================] - 12s 714us/sample - loss: 1.5980 - rmse: 1.1713 - val_loss: 1.6797 - val_rmse: 1.0059\n",
      "Epoch 713/1300\n",
      "16852/16852 [==============================] - 12s 709us/sample - loss: 1.5927 - rmse: 1.1691 - val_loss: 1.6675 - val_rmse: 1.0020\n",
      "Epoch 714/1300\n",
      "16852/16852 [==============================] - 12s 710us/sample - loss: 1.5883 - rmse: 1.1690 - val_loss: 1.7022 - val_rmse: 1.0059\n",
      "Epoch 715/1300\n",
      "16852/16852 [==============================] - 12s 704us/sample - loss: 1.5649 - rmse: 1.1601 - val_loss: 1.5850 - val_rmse: 0.9806\n",
      "Epoch 716/1300\n",
      "16852/16852 [==============================] - 12s 711us/sample - loss: 1.5736 - rmse: 1.1629 - val_loss: 1.6807 - val_rmse: 1.0066\n",
      "Epoch 717/1300\n",
      "16852/16852 [==============================] - 12s 708us/sample - loss: 1.5846 - rmse: 1.1681 - val_loss: 1.6293 - val_rmse: 0.9893\n",
      "Epoch 718/1300\n",
      "16852/16852 [==============================] - 12s 708us/sample - loss: 1.5656 - rmse: 1.1593 - val_loss: 1.6163 - val_rmse: 0.9853\n",
      "Epoch 719/1300\n",
      "16852/16852 [==============================] - 12s 708us/sample - loss: 1.5917 - rmse: 1.1716 - val_loss: 1.6732 - val_rmse: 1.0018\n",
      "Epoch 720/1300\n",
      "16852/16852 [==============================] - 12s 708us/sample - loss: 1.5870 - rmse: 1.1673 - val_loss: 1.7795 - val_rmse: 1.0462\n",
      "Epoch 721/1300\n",
      "16852/16852 [==============================] - 12s 711us/sample - loss: 1.6059 - rmse: 1.1730 - val_loss: 1.6460 - val_rmse: 0.9926\n",
      "Epoch 722/1300\n",
      "16852/16852 [==============================] - 12s 710us/sample - loss: 1.5873 - rmse: 1.1683 - val_loss: 1.6944 - val_rmse: 1.0245\n",
      "Epoch 723/1300\n",
      "16852/16852 [==============================] - 12s 709us/sample - loss: 1.5919 - rmse: 1.1696 - val_loss: 1.6333 - val_rmse: 0.9895\n",
      "Epoch 724/1300\n",
      "16852/16852 [==============================] - 12s 706us/sample - loss: 1.5637 - rmse: 1.1604 - val_loss: 1.6591 - val_rmse: 0.9949\n",
      "Epoch 725/1300\n",
      "16852/16852 [==============================] - 12s 705us/sample - loss: 1.6019 - rmse: 1.1738 - val_loss: 1.6985 - val_rmse: 1.0151\n",
      "Epoch 726/1300\n",
      "16852/16852 [==============================] - 12s 709us/sample - loss: 1.5836 - rmse: 1.1655 - val_loss: 1.7115 - val_rmse: 1.0153\n",
      "Epoch 727/1300\n",
      "16852/16852 [==============================] - 12s 710us/sample - loss: 1.5813 - rmse: 1.1650 - val_loss: 1.6324 - val_rmse: 0.9851\n",
      "Epoch 728/1300\n",
      "16852/16852 [==============================] - 12s 707us/sample - loss: 1.5852 - rmse: 1.1665 - val_loss: 1.7111 - val_rmse: 1.0062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 729/1300\n",
      "16852/16852 [==============================] - 12s 707us/sample - loss: 1.5834 - rmse: 1.1664 - val_loss: 1.6635 - val_rmse: 0.9905\n",
      "Epoch 730/1300\n",
      "16852/16852 [==============================] - 12s 705us/sample - loss: 1.5767 - rmse: 1.1634 - val_loss: 1.6422 - val_rmse: 1.0013\n",
      "Epoch 731/1300\n",
      "16852/16852 [==============================] - 12s 710us/sample - loss: 1.5784 - rmse: 1.1647 - val_loss: 1.6373 - val_rmse: 0.9858\n",
      "Epoch 732/1300\n",
      "16852/16852 [==============================] - 12s 709us/sample - loss: 1.5863 - rmse: 1.1672 - val_loss: 1.6371 - val_rmse: 0.9893\n",
      "Epoch 733/1300\n",
      "16852/16852 [==============================] - 12s 708us/sample - loss: 1.5929 - rmse: 1.1699 - val_loss: 1.6193 - val_rmse: 0.9862\n",
      "Epoch 734/1300\n",
      "16852/16852 [==============================] - 12s 713us/sample - loss: 1.5729 - rmse: 1.1626 - val_loss: 1.6027 - val_rmse: 0.9867\n",
      "Epoch 735/1300\n",
      "16852/16852 [==============================] - 12s 708us/sample - loss: 1.5897 - rmse: 1.1690 - val_loss: 1.6330 - val_rmse: 0.9841\n",
      "Epoch 736/1300\n",
      "16852/16852 [==============================] - 12s 703us/sample - loss: 1.5803 - rmse: 1.1645 - val_loss: 1.6628 - val_rmse: 0.9955\n",
      "Epoch 737/1300\n",
      "16852/16852 [==============================] - 12s 709us/sample - loss: 1.5664 - rmse: 1.1599 - val_loss: 1.6262 - val_rmse: 0.9881\n",
      "Epoch 738/1300\n",
      "16852/16852 [==============================] - 12s 707us/sample - loss: 1.5809 - rmse: 1.1652 - val_loss: 1.6526 - val_rmse: 0.9898\n",
      "Epoch 739/1300\n",
      "16852/16852 [==============================] - 12s 709us/sample - loss: 1.5714 - rmse: 1.1633 - val_loss: 1.6810 - val_rmse: 1.0004\n",
      "Epoch 740/1300\n",
      "16852/16852 [==============================] - 12s 710us/sample - loss: 1.6020 - rmse: 1.1732 - val_loss: 1.6074 - val_rmse: 0.9819\n",
      "Epoch 741/1300\n",
      "16852/16852 [==============================] - 12s 700us/sample - loss: 1.5563 - rmse: 1.1570 - val_loss: 1.6315 - val_rmse: 0.9837\n",
      "Epoch 742/1300\n",
      "16852/16852 [==============================] - 12s 710us/sample - loss: 1.5524 - rmse: 1.1552 - val_loss: 1.6855 - val_rmse: 1.0015\n",
      "Epoch 743/1300\n",
      "16852/16852 [==============================] - 12s 709us/sample - loss: 1.5677 - rmse: 1.1612 - val_loss: 1.6283 - val_rmse: 0.9845\n",
      "Epoch 744/1300\n",
      "16852/16852 [==============================] - 12s 710us/sample - loss: 1.5813 - rmse: 1.1668 - val_loss: 1.7671 - val_rmse: 1.0210\n",
      "Epoch 745/1300\n",
      "16852/16852 [==============================] - 12s 709us/sample - loss: 1.5799 - rmse: 1.1650 - val_loss: 1.6443 - val_rmse: 0.9914\n",
      "Epoch 746/1300\n",
      "16852/16852 [==============================] - 12s 710us/sample - loss: 1.5830 - rmse: 1.1641 - val_loss: 1.6096 - val_rmse: 0.9829\n",
      "Epoch 747/1300\n",
      "16852/16852 [==============================] - 12s 700us/sample - loss: 1.5768 - rmse: 1.1629 - val_loss: 1.6902 - val_rmse: 1.0170\n",
      "Epoch 748/1300\n",
      "16852/16852 [==============================] - 12s 706us/sample - loss: 1.5683 - rmse: 1.1612 - val_loss: 1.6592 - val_rmse: 0.9952\n",
      "Epoch 749/1300\n",
      "16852/16852 [==============================] - 12s 715us/sample - loss: 1.5773 - rmse: 1.1636 - val_loss: 1.6166 - val_rmse: 0.9886\n",
      "Epoch 750/1300\n",
      "16852/16852 [==============================] - 12s 708us/sample - loss: 1.5864 - rmse: 1.1657 - val_loss: 1.7341 - val_rmse: 1.0236\n",
      "Epoch 751/1300\n",
      "16852/16852 [==============================] - 12s 705us/sample - loss: 1.5632 - rmse: 1.1585 - val_loss: 1.6686 - val_rmse: 1.0076\n",
      "Epoch 752/1300\n",
      "16852/16852 [==============================] - 12s 708us/sample - loss: 1.5600 - rmse: 1.1576 - val_loss: 1.6959 - val_rmse: 1.0184\n",
      "Epoch 753/1300\n",
      "16852/16852 [==============================] - 12s 706us/sample - loss: 1.5766 - rmse: 1.1633 - val_loss: 1.6126 - val_rmse: 0.9809\n",
      "Epoch 754/1300\n",
      "16852/16852 [==============================] - 12s 711us/sample - loss: 1.5617 - rmse: 1.1578 - val_loss: 1.6380 - val_rmse: 0.9917\n",
      "Epoch 755/1300\n",
      "16852/16852 [==============================] - 12s 705us/sample - loss: 1.5732 - rmse: 1.1612 - val_loss: 1.6268 - val_rmse: 0.9857\n",
      "Epoch 756/1300\n",
      "16852/16852 [==============================] - 12s 710us/sample - loss: 1.5614 - rmse: 1.1586 - val_loss: 1.6690 - val_rmse: 0.9965\n",
      "Epoch 757/1300\n",
      "16852/16852 [==============================] - 12s 701us/sample - loss: 1.5815 - rmse: 1.1635 - val_loss: 1.6700 - val_rmse: 0.9958\n",
      "Epoch 758/1300\n",
      "16852/16852 [==============================] - 12s 706us/sample - loss: 1.5874 - rmse: 1.1659 - val_loss: 1.6130 - val_rmse: 0.9781\n",
      "Epoch 759/1300\n",
      "16852/16852 [==============================] - 12s 703us/sample - loss: 1.5784 - rmse: 1.1633 - val_loss: 1.6685 - val_rmse: 0.9989\n",
      "Epoch 760/1300\n",
      "16852/16852 [==============================] - 12s 706us/sample - loss: 1.5802 - rmse: 1.1630 - val_loss: 1.7465 - val_rmse: 1.0253\n",
      "Epoch 761/1300\n",
      "16852/16852 [==============================] - 12s 709us/sample - loss: 1.5729 - rmse: 1.1637 - val_loss: 1.6935 - val_rmse: 1.0100\n",
      "Epoch 762/1300\n",
      "16852/16852 [==============================] - 12s 707us/sample - loss: 1.5646 - rmse: 1.1584 - val_loss: 1.7276 - val_rmse: 1.0154\n",
      "Epoch 763/1300\n",
      "16852/16852 [==============================] - 12s 714us/sample - loss: 1.5451 - rmse: 1.1543 - val_loss: 1.6339 - val_rmse: 0.9835\n",
      "Epoch 764/1300\n",
      "16852/16852 [==============================] - 12s 710us/sample - loss: 1.5645 - rmse: 1.1597 - val_loss: 1.6246 - val_rmse: 0.9787\n",
      "Epoch 765/1300\n",
      "16852/16852 [==============================] - 12s 709us/sample - loss: 1.5736 - rmse: 1.1635 - val_loss: 1.6554 - val_rmse: 0.9946\n",
      "Epoch 766/1300\n",
      "16852/16852 [==============================] - 12s 709us/sample - loss: 1.5542 - rmse: 1.1551 - val_loss: 1.6701 - val_rmse: 1.0015\n",
      "Epoch 767/1300\n",
      "16852/16852 [==============================] - 12s 707us/sample - loss: 1.5720 - rmse: 1.1600 - val_loss: 1.6248 - val_rmse: 0.9842\n",
      "Epoch 768/1300\n",
      "16852/16852 [==============================] - 12s 702us/sample - loss: 1.5703 - rmse: 1.1609 - val_loss: 1.6721 - val_rmse: 0.9990\n",
      "Epoch 769/1300\n",
      "16852/16852 [==============================] - 12s 704us/sample - loss: 1.5695 - rmse: 1.1622 - val_loss: 1.6669 - val_rmse: 1.0015\n",
      "Epoch 770/1300\n",
      "16852/16852 [==============================] - 12s 711us/sample - loss: 1.5863 - rmse: 1.1668 - val_loss: 1.6896 - val_rmse: 1.0027\n",
      "Epoch 771/1300\n",
      "16852/16852 [==============================] - 12s 717us/sample - loss: 1.5648 - rmse: 1.1602 - val_loss: 1.6050 - val_rmse: 0.9760\n",
      "Epoch 772/1300\n",
      "16852/16852 [==============================] - 12s 717us/sample - loss: 1.5714 - rmse: 1.1625 - val_loss: 1.7420 - val_rmse: 1.0178\n",
      "Epoch 773/1300\n",
      "16852/16852 [==============================] - 12s 708us/sample - loss: 1.5776 - rmse: 1.1630 - val_loss: 1.7365 - val_rmse: 1.0189\n",
      "Epoch 774/1300\n",
      "16852/16852 [==============================] - 12s 700us/sample - loss: 1.5560 - rmse: 1.1555 - val_loss: 1.6607 - val_rmse: 0.9946\n",
      "Epoch 775/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 1.5435 - rmse: 1.1513 - val_loss: 1.6431 - val_rmse: 0.9890\n",
      "Epoch 776/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.5851 - rmse: 1.1654 - val_loss: 1.6742 - val_rmse: 0.9942\n",
      "Epoch 777/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.5655 - rmse: 1.1586 - val_loss: 1.6645 - val_rmse: 0.9896\n",
      "Epoch 778/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 1.5722 - rmse: 1.1616 - val_loss: 1.6871 - val_rmse: 1.0038\n",
      "Epoch 779/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 1.5373 - rmse: 1.1511 - val_loss: 1.6572 - val_rmse: 0.9976\n",
      "Epoch 780/1300\n",
      "16852/16852 [==============================] - 12s 707us/sample - loss: 1.5664 - rmse: 1.1588 - val_loss: 1.6181 - val_rmse: 0.9782\n",
      "Epoch 781/1300\n",
      "16852/16852 [==============================] - 12s 699us/sample - loss: 1.5547 - rmse: 1.1546 - val_loss: 1.6741 - val_rmse: 0.9978\n",
      "Epoch 782/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 1.5508 - rmse: 1.1543 - val_loss: 1.6183 - val_rmse: 0.9864\n",
      "Epoch 783/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.5509 - rmse: 1.1547 - val_loss: 1.6220 - val_rmse: 0.9814\n",
      "Epoch 784/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.5619 - rmse: 1.1578 - val_loss: 1.6690 - val_rmse: 0.9976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 785/1300\n",
      "16852/16852 [==============================] - 12s 704us/sample - loss: 1.5505 - rmse: 1.1528 - val_loss: 1.6190 - val_rmse: 0.9812\n",
      "Epoch 786/1300\n",
      "16852/16852 [==============================] - 12s 715us/sample - loss: 1.5535 - rmse: 1.1538 - val_loss: 1.6647 - val_rmse: 0.9914\n",
      "Epoch 787/1300\n",
      "16852/16852 [==============================] - 12s 711us/sample - loss: 1.5621 - rmse: 1.1587 - val_loss: 1.6931 - val_rmse: 1.0085\n",
      "Epoch 788/1300\n",
      "16852/16852 [==============================] - 12s 711us/sample - loss: 1.5808 - rmse: 1.1620 - val_loss: 1.6602 - val_rmse: 0.9933\n",
      "Epoch 789/1300\n",
      "16852/16852 [==============================] - 12s 710us/sample - loss: 1.5669 - rmse: 1.1595 - val_loss: 1.6525 - val_rmse: 0.9853\n",
      "Epoch 790/1300\n",
      "16852/16852 [==============================] - 12s 711us/sample - loss: 1.5499 - rmse: 1.1542 - val_loss: 1.6728 - val_rmse: 0.9930\n",
      "Epoch 791/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.5605 - rmse: 1.1569 - val_loss: 1.6910 - val_rmse: 1.0033\n",
      "Epoch 792/1300\n",
      "16852/16852 [==============================] - 12s 710us/sample - loss: 1.5550 - rmse: 1.1545 - val_loss: 1.6369 - val_rmse: 0.9853\n",
      "Epoch 793/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.5554 - rmse: 1.1541 - val_loss: 1.6963 - val_rmse: 1.0039\n",
      "Epoch 794/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 1.5665 - rmse: 1.1579 - val_loss: 1.6447 - val_rmse: 0.9858\n",
      "Epoch 795/1300\n",
      "16852/16852 [==============================] - 12s 708us/sample - loss: 1.5526 - rmse: 1.1546 - val_loss: 1.6311 - val_rmse: 0.9770\n",
      "Epoch 796/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.5604 - rmse: 1.1577 - val_loss: 1.6190 - val_rmse: 0.9823\n",
      "Epoch 797/1300\n",
      "16852/16852 [==============================] - 12s 700us/sample - loss: 1.5538 - rmse: 1.1538 - val_loss: 1.6959 - val_rmse: 1.0051\n",
      "Epoch 798/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 1.5316 - rmse: 1.1485 - val_loss: 1.6586 - val_rmse: 0.9864\n",
      "Epoch 799/1300\n",
      "16852/16852 [==============================] - 12s 705us/sample - loss: 1.5549 - rmse: 1.1554 - val_loss: 1.6471 - val_rmse: 0.9929\n",
      "Epoch 800/1300\n",
      "16852/16852 [==============================] - 12s 708us/sample - loss: 1.5534 - rmse: 1.1530 - val_loss: 1.6873 - val_rmse: 1.0032\n",
      "Epoch 801/1300\n",
      "16852/16852 [==============================] - 12s 709us/sample - loss: 1.5555 - rmse: 1.1538 - val_loss: 1.6164 - val_rmse: 0.9830\n",
      "Epoch 802/1300\n",
      "16852/16852 [==============================] - 12s 701us/sample - loss: 1.5694 - rmse: 1.1589 - val_loss: 1.6283 - val_rmse: 0.9868\n",
      "Epoch 803/1300\n",
      "16852/16852 [==============================] - 12s 703us/sample - loss: 1.5435 - rmse: 1.1516 - val_loss: 1.6332 - val_rmse: 0.9773\n",
      "Epoch 804/1300\n",
      "16852/16852 [==============================] - 12s 704us/sample - loss: 1.5645 - rmse: 1.1582 - val_loss: 1.7072 - val_rmse: 1.0024\n",
      "Epoch 805/1300\n",
      "16852/16852 [==============================] - 12s 699us/sample - loss: 1.5479 - rmse: 1.1514 - val_loss: 1.7121 - val_rmse: 1.0127\n",
      "Epoch 806/1300\n",
      "16852/16852 [==============================] - 12s 706us/sample - loss: 1.5465 - rmse: 1.1526 - val_loss: 1.6293 - val_rmse: 0.9930\n",
      "Epoch 807/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 1.5497 - rmse: 1.1521 - val_loss: 1.6158 - val_rmse: 0.9793\n",
      "Epoch 808/1300\n",
      "16852/16852 [==============================] - 12s 699us/sample - loss: 1.5377 - rmse: 1.1491 - val_loss: 1.6884 - val_rmse: 0.9985\n",
      "Epoch 809/1300\n",
      "16852/16852 [==============================] - 12s 702us/sample - loss: 1.5349 - rmse: 1.1491 - val_loss: 1.6918 - val_rmse: 1.0111\n",
      "Epoch 810/1300\n",
      "16852/16852 [==============================] - 12s 703us/sample - loss: 1.5336 - rmse: 1.1468 - val_loss: 1.5771 - val_rmse: 0.9655\n",
      "Epoch 811/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.5317 - rmse: 1.1472 - val_loss: 1.6198 - val_rmse: 0.9785\n",
      "Epoch 812/1300\n",
      "16852/16852 [==============================] - 12s 699us/sample - loss: 1.5554 - rmse: 1.1535 - val_loss: 1.5680 - val_rmse: 0.9709\n",
      "Epoch 813/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 1.5414 - rmse: 1.1498 - val_loss: 1.6723 - val_rmse: 0.9973\n",
      "Epoch 814/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.5415 - rmse: 1.1492 - val_loss: 1.6310 - val_rmse: 0.9760\n",
      "Epoch 815/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.5590 - rmse: 1.1562 - val_loss: 1.6341 - val_rmse: 0.9868\n",
      "Epoch 816/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.5493 - rmse: 1.1533 - val_loss: 1.6406 - val_rmse: 0.9822\n",
      "Epoch 817/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.5509 - rmse: 1.1535 - val_loss: 1.6767 - val_rmse: 1.0014\n",
      "Epoch 818/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.5386 - rmse: 1.1496 - val_loss: 1.6429 - val_rmse: 0.9821\n",
      "Epoch 819/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 1.5523 - rmse: 1.1528 - val_loss: 1.6644 - val_rmse: 0.9934\n",
      "Epoch 820/1300\n",
      "16852/16852 [==============================] - 12s 700us/sample - loss: 1.5430 - rmse: 1.1493 - val_loss: 1.6091 - val_rmse: 0.9757\n",
      "Epoch 821/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.5509 - rmse: 1.1524 - val_loss: 1.5925 - val_rmse: 0.9692\n",
      "Epoch 822/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.5366 - rmse: 1.1463 - val_loss: 1.6626 - val_rmse: 0.9881\n",
      "Epoch 823/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 1.5542 - rmse: 1.1536 - val_loss: 1.6659 - val_rmse: 0.9958\n",
      "Epoch 824/1300\n",
      "16852/16852 [==============================] - 12s 699us/sample - loss: 1.5589 - rmse: 1.1551 - val_loss: 1.6241 - val_rmse: 0.9793\n",
      "Epoch 825/1300\n",
      "16852/16852 [==============================] - 12s 701us/sample - loss: 1.5456 - rmse: 1.1509 - val_loss: 1.6512 - val_rmse: 0.9858\n",
      "Epoch 826/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 1.5508 - rmse: 1.1527 - val_loss: 1.6261 - val_rmse: 0.9781\n",
      "Epoch 827/1300\n",
      "16852/16852 [==============================] - 12s 699us/sample - loss: 1.5375 - rmse: 1.1489 - val_loss: 1.7182 - val_rmse: 1.0064\n",
      "Epoch 828/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.5358 - rmse: 1.1481 - val_loss: 1.6561 - val_rmse: 0.9845\n",
      "Epoch 829/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.5442 - rmse: 1.1503 - val_loss: 1.6435 - val_rmse: 0.9857\n",
      "Epoch 830/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.5402 - rmse: 1.1500 - val_loss: 1.6061 - val_rmse: 0.9763\n",
      "Epoch 831/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.5483 - rmse: 1.1515 - val_loss: 1.6891 - val_rmse: 0.9907\n",
      "Epoch 832/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.5559 - rmse: 1.1536 - val_loss: 1.6944 - val_rmse: 0.9979\n",
      "Epoch 833/1300\n",
      "16852/16852 [==============================] - 12s 699us/sample - loss: 1.5339 - rmse: 1.1464 - val_loss: 1.6519 - val_rmse: 0.9940\n",
      "Epoch 834/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 1.5555 - rmse: 1.1550 - val_loss: 1.5903 - val_rmse: 0.9715\n",
      "Epoch 835/1300\n",
      "16852/16852 [==============================] - 12s 706us/sample - loss: 1.5201 - rmse: 1.1420 - val_loss: 1.5790 - val_rmse: 0.9650\n",
      "Epoch 836/1300\n",
      "16852/16852 [==============================] - 12s 705us/sample - loss: 1.5476 - rmse: 1.1512 - val_loss: 1.6218 - val_rmse: 0.9793\n",
      "Epoch 837/1300\n",
      "16852/16852 [==============================] - 12s 708us/sample - loss: 1.5475 - rmse: 1.1514 - val_loss: 1.6229 - val_rmse: 0.9833\n",
      "Epoch 838/1300\n",
      "16852/16852 [==============================] - 12s 710us/sample - loss: 1.5405 - rmse: 1.1486 - val_loss: 1.6393 - val_rmse: 0.9868\n",
      "Epoch 839/1300\n",
      "16852/16852 [==============================] - 12s 710us/sample - loss: 1.5421 - rmse: 1.1488 - val_loss: 1.6608 - val_rmse: 0.9933\n",
      "Epoch 840/1300\n",
      "16852/16852 [==============================] - 12s 711us/sample - loss: 1.5599 - rmse: 1.1551 - val_loss: 1.6738 - val_rmse: 0.9910\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 841/1300\n",
      "16852/16852 [==============================] - 12s 709us/sample - loss: 1.5531 - rmse: 1.1534 - val_loss: 1.6393 - val_rmse: 0.9752\n",
      "Epoch 842/1300\n",
      "16852/16852 [==============================] - 12s 705us/sample - loss: 1.5383 - rmse: 1.1465 - val_loss: 1.6469 - val_rmse: 0.9943\n",
      "Epoch 843/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.5537 - rmse: 1.1536 - val_loss: 1.6156 - val_rmse: 0.9725\n",
      "Epoch 844/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.5285 - rmse: 1.1441 - val_loss: 1.6084 - val_rmse: 0.9735\n",
      "Epoch 845/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.5312 - rmse: 1.1457 - val_loss: 1.5953 - val_rmse: 0.9686\n",
      "Epoch 846/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.5478 - rmse: 1.1512 - val_loss: 1.6725 - val_rmse: 0.9978\n",
      "Epoch 847/1300\n",
      "16852/16852 [==============================] - 12s 707us/sample - loss: 1.5486 - rmse: 1.1515 - val_loss: 1.7315 - val_rmse: 1.0210\n",
      "Epoch 848/1300\n",
      "16852/16852 [==============================] - 12s 714us/sample - loss: 1.5302 - rmse: 1.1441 - val_loss: 1.6258 - val_rmse: 0.9829\n",
      "Epoch 849/1300\n",
      "16852/16852 [==============================] - 12s 707us/sample - loss: 1.5347 - rmse: 1.1481 - val_loss: 1.6318 - val_rmse: 0.9810\n",
      "Epoch 850/1300\n",
      "16852/16852 [==============================] - 12s 707us/sample - loss: 1.5402 - rmse: 1.1491 - val_loss: 1.6075 - val_rmse: 0.9696\n",
      "Epoch 851/1300\n",
      "16852/16852 [==============================] - 12s 711us/sample - loss: 1.5458 - rmse: 1.1503 - val_loss: 1.5856 - val_rmse: 0.9728\n",
      "Epoch 852/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.5395 - rmse: 1.1466 - val_loss: 1.6556 - val_rmse: 0.9844\n",
      "Epoch 853/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 1.5483 - rmse: 1.1521 - val_loss: 1.6478 - val_rmse: 0.9849\n",
      "Epoch 854/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.5263 - rmse: 1.1421 - val_loss: 1.6150 - val_rmse: 0.9833\n",
      "Epoch 855/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 1.5332 - rmse: 1.1452 - val_loss: 1.6176 - val_rmse: 0.9712\n",
      "Epoch 856/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.5435 - rmse: 1.1488 - val_loss: 1.6321 - val_rmse: 0.9987\n",
      "Epoch 857/1300\n",
      "16852/16852 [==============================] - 12s 700us/sample - loss: 1.5222 - rmse: 1.1432 - val_loss: 1.6972 - val_rmse: 0.9957\n",
      "Epoch 858/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 1.5442 - rmse: 1.1487 - val_loss: 1.6768 - val_rmse: 0.9961\n",
      "Epoch 859/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.5205 - rmse: 1.1426 - val_loss: 1.6672 - val_rmse: 0.9889\n",
      "Epoch 860/1300\n",
      "16852/16852 [==============================] - 12s 707us/sample - loss: 1.5243 - rmse: 1.1428 - val_loss: 1.7226 - val_rmse: 1.0003\n",
      "Epoch 861/1300\n",
      "16852/16852 [==============================] - 12s 709us/sample - loss: 1.5344 - rmse: 1.1479 - val_loss: 1.6958 - val_rmse: 1.0004\n",
      "Epoch 862/1300\n",
      "16852/16852 [==============================] - 12s 702us/sample - loss: 1.5445 - rmse: 1.1493 - val_loss: 1.6179 - val_rmse: 0.9745\n",
      "Epoch 863/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.5401 - rmse: 1.1496 - val_loss: 1.5991 - val_rmse: 0.9760\n",
      "Epoch 864/1300\n",
      "16852/16852 [==============================] - 12s 699us/sample - loss: 1.5481 - rmse: 1.1491 - val_loss: 1.6266 - val_rmse: 0.9797\n",
      "Epoch 865/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.5198 - rmse: 1.1407 - val_loss: 1.5920 - val_rmse: 0.9764\n",
      "Epoch 866/1300\n",
      "16852/16852 [==============================] - 12s 699us/sample - loss: 1.5300 - rmse: 1.1428 - val_loss: 1.6623 - val_rmse: 0.9883\n",
      "Epoch 867/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.5296 - rmse: 1.1446 - val_loss: 1.6730 - val_rmse: 1.0002\n",
      "Epoch 868/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.5191 - rmse: 1.1421 - val_loss: 1.6418 - val_rmse: 0.9838\n",
      "Epoch 869/1300\n",
      "16852/16852 [==============================] - 12s 693us/sample - loss: 1.5537 - rmse: 1.1511 - val_loss: 1.6389 - val_rmse: 0.9820\n",
      "Epoch 870/1300\n",
      "16852/16852 [==============================] - 12s 705us/sample - loss: 1.5299 - rmse: 1.1450 - val_loss: 1.7385 - val_rmse: 1.0064\n",
      "Epoch 871/1300\n",
      "16852/16852 [==============================] - 12s 711us/sample - loss: 1.5221 - rmse: 1.1404 - val_loss: 1.6082 - val_rmse: 0.9711\n",
      "Epoch 872/1300\n",
      "16852/16852 [==============================] - 12s 707us/sample - loss: 1.5325 - rmse: 1.1464 - val_loss: 1.6103 - val_rmse: 0.9729\n",
      "Epoch 873/1300\n",
      "16852/16852 [==============================] - 12s 715us/sample - loss: 1.5306 - rmse: 1.1457 - val_loss: 1.6319 - val_rmse: 0.9788\n",
      "Epoch 874/1300\n",
      "16852/16852 [==============================] - 12s 715us/sample - loss: 1.5059 - rmse: 1.1366 - val_loss: 1.5610 - val_rmse: 0.9564\n",
      "Epoch 875/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.5229 - rmse: 1.1434 - val_loss: 1.6964 - val_rmse: 1.0090\n",
      "Epoch 876/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 1.5272 - rmse: 1.1436 - val_loss: 1.6327 - val_rmse: 0.9814\n",
      "Epoch 877/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.5264 - rmse: 1.1435 - val_loss: 1.6085 - val_rmse: 0.9719\n",
      "Epoch 878/1300\n",
      "16852/16852 [==============================] - 12s 710us/sample - loss: 1.5341 - rmse: 1.1469 - val_loss: 1.6302 - val_rmse: 0.9789\n",
      "Epoch 879/1300\n",
      "16852/16852 [==============================] - 12s 713us/sample - loss: 1.5297 - rmse: 1.1442 - val_loss: 1.6370 - val_rmse: 0.9767\n",
      "Epoch 880/1300\n",
      "16852/16852 [==============================] - 12s 704us/sample - loss: 1.5356 - rmse: 1.1460 - val_loss: 1.6189 - val_rmse: 0.9809\n",
      "Epoch 881/1300\n",
      "16852/16852 [==============================] - 12s 709us/sample - loss: 1.5330 - rmse: 1.1442 - val_loss: 1.6558 - val_rmse: 0.9874\n",
      "Epoch 882/1300\n",
      "16852/16852 [==============================] - 12s 705us/sample - loss: 1.5319 - rmse: 1.1453 - val_loss: 1.6253 - val_rmse: 0.9814\n",
      "Epoch 883/1300\n",
      "16852/16852 [==============================] - 12s 713us/sample - loss: 1.5237 - rmse: 1.1408 - val_loss: 1.6309 - val_rmse: 0.9803\n",
      "Epoch 884/1300\n",
      "16852/16852 [==============================] - 12s 704us/sample - loss: 1.5239 - rmse: 1.1425 - val_loss: 1.6295 - val_rmse: 0.9820\n",
      "Epoch 885/1300\n",
      "16852/16852 [==============================] - 12s 714us/sample - loss: 1.5000 - rmse: 1.1336 - val_loss: 1.5989 - val_rmse: 0.9694\n",
      "Epoch 886/1300\n",
      "16852/16852 [==============================] - 12s 711us/sample - loss: 1.5363 - rmse: 1.1437 - val_loss: 1.6393 - val_rmse: 0.9853\n",
      "Epoch 887/1300\n",
      "16852/16852 [==============================] - 12s 706us/sample - loss: 1.5191 - rmse: 1.1405 - val_loss: 1.5928 - val_rmse: 0.9670\n",
      "Epoch 888/1300\n",
      "16852/16852 [==============================] - 12s 709us/sample - loss: 1.5443 - rmse: 1.1490 - val_loss: 1.5885 - val_rmse: 0.9639\n",
      "Epoch 889/1300\n",
      "16852/16852 [==============================] - 12s 711us/sample - loss: 1.5226 - rmse: 1.1413 - val_loss: 1.6281 - val_rmse: 0.9699\n",
      "Epoch 890/1300\n",
      "16852/16852 [==============================] - 12s 708us/sample - loss: 1.5261 - rmse: 1.1430 - val_loss: 1.5895 - val_rmse: 0.9660\n",
      "Epoch 891/1300\n",
      "16852/16852 [==============================] - 12s 708us/sample - loss: 1.5219 - rmse: 1.1413 - val_loss: 1.6950 - val_rmse: 0.9991\n",
      "Epoch 892/1300\n",
      "16852/16852 [==============================] - 12s 712us/sample - loss: 1.5246 - rmse: 1.1426 - val_loss: 1.6771 - val_rmse: 0.9935\n",
      "Epoch 893/1300\n",
      "16852/16852 [==============================] - 12s 706us/sample - loss: 1.5261 - rmse: 1.1427 - val_loss: 1.6816 - val_rmse: 0.9889\n",
      "Epoch 894/1300\n",
      "16852/16852 [==============================] - 12s 704us/sample - loss: 1.5312 - rmse: 1.1454 - val_loss: 1.6368 - val_rmse: 0.9791\n",
      "Epoch 895/1300\n",
      "16852/16852 [==============================] - 12s 708us/sample - loss: 1.5135 - rmse: 1.1383 - val_loss: 1.7094 - val_rmse: 1.0045\n",
      "Epoch 896/1300\n",
      "16852/16852 [==============================] - 12s 713us/sample - loss: 1.5220 - rmse: 1.1400 - val_loss: 1.7327 - val_rmse: 1.0206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 897/1300\n",
      "16852/16852 [==============================] - 12s 708us/sample - loss: 1.5174 - rmse: 1.1398 - val_loss: 1.7011 - val_rmse: 0.9943\n",
      "Epoch 898/1300\n",
      "16852/16852 [==============================] - 12s 708us/sample - loss: 1.5373 - rmse: 1.1460 - val_loss: 1.5809 - val_rmse: 0.9668\n",
      "Epoch 899/1300\n",
      "16852/16852 [==============================] - 12s 708us/sample - loss: 1.5251 - rmse: 1.1413 - val_loss: 1.7011 - val_rmse: 0.9991\n",
      "Epoch 900/1300\n",
      "16852/16852 [==============================] - 12s 711us/sample - loss: 1.5174 - rmse: 1.1393 - val_loss: 1.6617 - val_rmse: 0.9836\n",
      "Epoch 901/1300\n",
      "16852/16852 [==============================] - 12s 709us/sample - loss: 1.5453 - rmse: 1.1487 - val_loss: 1.6115 - val_rmse: 0.9750\n",
      "Epoch 902/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.5198 - rmse: 1.1409 - val_loss: 1.6894 - val_rmse: 0.9965\n",
      "Epoch 903/1300\n",
      "16852/16852 [==============================] - 12s 702us/sample - loss: 1.5230 - rmse: 1.1415 - val_loss: 1.6063 - val_rmse: 0.9706\n",
      "Epoch 904/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.5206 - rmse: 1.1383 - val_loss: 1.6239 - val_rmse: 0.9798\n",
      "Epoch 905/1300\n",
      "16852/16852 [==============================] - 12s 700us/sample - loss: 1.5034 - rmse: 1.1340 - val_loss: 1.6403 - val_rmse: 0.9784\n",
      "Epoch 906/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.5109 - rmse: 1.1354 - val_loss: 1.7908 - val_rmse: 1.0393\n",
      "Epoch 907/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.5087 - rmse: 1.1364 - val_loss: 1.6685 - val_rmse: 0.9831\n",
      "Epoch 908/1300\n",
      "16852/16852 [==============================] - 12s 703us/sample - loss: 1.5135 - rmse: 1.1379 - val_loss: 1.6134 - val_rmse: 0.9711\n",
      "Epoch 909/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.5311 - rmse: 1.1439 - val_loss: 1.6483 - val_rmse: 0.9847\n",
      "Epoch 910/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.5140 - rmse: 1.1358 - val_loss: 1.6642 - val_rmse: 0.9823\n",
      "Epoch 911/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 1.5132 - rmse: 1.1358 - val_loss: 1.5969 - val_rmse: 0.9686\n",
      "Epoch 912/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.5267 - rmse: 1.1428 - val_loss: 1.6161 - val_rmse: 0.9700\n",
      "Epoch 913/1300\n",
      "16852/16852 [==============================] - 12s 705us/sample - loss: 1.5159 - rmse: 1.1382 - val_loss: 1.6576 - val_rmse: 0.9911\n",
      "Epoch 914/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 1.5136 - rmse: 1.1364 - val_loss: 1.6494 - val_rmse: 0.9815\n",
      "Epoch 915/1300\n",
      "16852/16852 [==============================] - 12s 701us/sample - loss: 1.5094 - rmse: 1.1366 - val_loss: 1.6327 - val_rmse: 0.9773\n",
      "Epoch 916/1300\n",
      "16852/16852 [==============================] - 12s 711us/sample - loss: 1.5230 - rmse: 1.1424 - val_loss: 1.6789 - val_rmse: 0.9982\n",
      "Epoch 917/1300\n",
      "16852/16852 [==============================] - 12s 702us/sample - loss: 1.5047 - rmse: 1.1359 - val_loss: 1.5987 - val_rmse: 0.9709\n",
      "Epoch 918/1300\n",
      "16852/16852 [==============================] - 12s 708us/sample - loss: 1.5035 - rmse: 1.1341 - val_loss: 1.6330 - val_rmse: 0.9756\n",
      "Epoch 919/1300\n",
      "16852/16852 [==============================] - 12s 703us/sample - loss: 1.5251 - rmse: 1.1398 - val_loss: 1.6005 - val_rmse: 0.9708\n",
      "Epoch 920/1300\n",
      "16852/16852 [==============================] - 12s 708us/sample - loss: 1.5067 - rmse: 1.1356 - val_loss: 1.6343 - val_rmse: 0.9765\n",
      "Epoch 921/1300\n",
      "16852/16852 [==============================] - 12s 710us/sample - loss: 1.5310 - rmse: 1.1424 - val_loss: 1.6709 - val_rmse: 0.9865\n",
      "Epoch 922/1300\n",
      "16852/16852 [==============================] - 12s 710us/sample - loss: 1.5148 - rmse: 1.1368 - val_loss: 1.6194 - val_rmse: 0.9691\n",
      "Epoch 923/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.4972 - rmse: 1.1322 - val_loss: 1.6430 - val_rmse: 0.9779\n",
      "Epoch 924/1300\n",
      "16852/16852 [==============================] - 12s 710us/sample - loss: 1.5047 - rmse: 1.1347 - val_loss: 1.6449 - val_rmse: 0.9844\n",
      "Epoch 925/1300\n",
      "16852/16852 [==============================] - 12s 706us/sample - loss: 1.4937 - rmse: 1.1318 - val_loss: 1.5743 - val_rmse: 0.9654\n",
      "Epoch 926/1300\n",
      "16852/16852 [==============================] - 12s 712us/sample - loss: 1.5037 - rmse: 1.1344 - val_loss: 1.6385 - val_rmse: 0.9728\n",
      "Epoch 927/1300\n",
      "16852/16852 [==============================] - 12s 710us/sample - loss: 1.5027 - rmse: 1.1343 - val_loss: 1.6316 - val_rmse: 0.9735\n",
      "Epoch 928/1300\n",
      "16852/16852 [==============================] - 12s 707us/sample - loss: 1.4985 - rmse: 1.1319 - val_loss: 1.5831 - val_rmse: 0.9646\n",
      "Epoch 929/1300\n",
      "16852/16852 [==============================] - 12s 713us/sample - loss: 1.5019 - rmse: 1.1340 - val_loss: 1.6519 - val_rmse: 0.9823\n",
      "Epoch 930/1300\n",
      "16852/16852 [==============================] - 12s 709us/sample - loss: 1.5097 - rmse: 1.1361 - val_loss: 1.7118 - val_rmse: 0.9974\n",
      "Epoch 931/1300\n",
      "16852/16852 [==============================] - 12s 712us/sample - loss: 1.5237 - rmse: 1.1384 - val_loss: 1.5961 - val_rmse: 0.9615\n",
      "Epoch 932/1300\n",
      "16852/16852 [==============================] - 12s 711us/sample - loss: 1.4903 - rmse: 1.1291 - val_loss: 1.6560 - val_rmse: 0.9847\n",
      "Epoch 933/1300\n",
      "16852/16852 [==============================] - 12s 710us/sample - loss: 1.5063 - rmse: 1.1326 - val_loss: 1.5979 - val_rmse: 0.9782\n",
      "Epoch 934/1300\n",
      "16852/16852 [==============================] - 12s 708us/sample - loss: 1.5001 - rmse: 1.1315 - val_loss: 1.6348 - val_rmse: 0.9829\n",
      "Epoch 935/1300\n",
      "16852/16852 [==============================] - 12s 709us/sample - loss: 1.5070 - rmse: 1.1342 - val_loss: 1.6202 - val_rmse: 0.9728\n",
      "Epoch 936/1300\n",
      "16852/16852 [==============================] - 12s 699us/sample - loss: 1.5335 - rmse: 1.1421 - val_loss: 1.6151 - val_rmse: 0.9713\n",
      "Epoch 937/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 1.5078 - rmse: 1.1356 - val_loss: 1.6862 - val_rmse: 0.9903\n",
      "Epoch 938/1300\n",
      "16852/16852 [==============================] - 12s 706us/sample - loss: 1.5122 - rmse: 1.1358 - val_loss: 1.6498 - val_rmse: 0.9828\n",
      "Epoch 939/1300\n",
      "16852/16852 [==============================] - 12s 712us/sample - loss: 1.4950 - rmse: 1.1310 - val_loss: 1.5802 - val_rmse: 0.9678\n",
      "Epoch 940/1300\n",
      "16852/16852 [==============================] - 12s 719us/sample - loss: 1.5225 - rmse: 1.1396 - val_loss: 1.5878 - val_rmse: 0.9630\n",
      "Epoch 941/1300\n",
      "16852/16852 [==============================] - 12s 708us/sample - loss: 1.5000 - rmse: 1.1317 - val_loss: 1.5998 - val_rmse: 0.9647\n",
      "Epoch 942/1300\n",
      "16852/16852 [==============================] - 12s 708us/sample - loss: 1.5087 - rmse: 1.1353 - val_loss: 1.5899 - val_rmse: 0.9596\n",
      "Epoch 943/1300\n",
      "16852/16852 [==============================] - 12s 708us/sample - loss: 1.5305 - rmse: 1.1418 - val_loss: 1.6385 - val_rmse: 0.9832\n",
      "Epoch 944/1300\n",
      "16852/16852 [==============================] - 12s 712us/sample - loss: 1.5189 - rmse: 1.1388 - val_loss: 1.6028 - val_rmse: 0.9710\n",
      "Epoch 945/1300\n",
      "16852/16852 [==============================] - 12s 703us/sample - loss: 1.5025 - rmse: 1.1344 - val_loss: 1.7030 - val_rmse: 0.9959\n",
      "Epoch 946/1300\n",
      "16852/16852 [==============================] - 12s 708us/sample - loss: 1.4824 - rmse: 1.1257 - val_loss: 1.5844 - val_rmse: 0.9579\n",
      "Epoch 947/1300\n",
      "16852/16852 [==============================] - 12s 707us/sample - loss: 1.5070 - rmse: 1.1332 - val_loss: 1.6646 - val_rmse: 0.9819\n",
      "Epoch 948/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.5131 - rmse: 1.1348 - val_loss: 1.6409 - val_rmse: 0.9873\n",
      "Epoch 949/1300\n",
      "16852/16852 [==============================] - 12s 699us/sample - loss: 1.5026 - rmse: 1.1334 - val_loss: 1.6541 - val_rmse: 0.9837\n",
      "Epoch 950/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.4998 - rmse: 1.1334 - val_loss: 1.6527 - val_rmse: 0.9911\n",
      "Epoch 951/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.5050 - rmse: 1.1341 - val_loss: 1.6448 - val_rmse: 0.9844\n",
      "Epoch 952/1300\n",
      "16852/16852 [==============================] - 12s 701us/sample - loss: 1.5167 - rmse: 1.1364 - val_loss: 1.6314 - val_rmse: 0.9721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 953/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.5076 - rmse: 1.1353 - val_loss: 1.6280 - val_rmse: 0.9723\n",
      "Epoch 954/1300\n",
      "16852/16852 [==============================] - 12s 703us/sample - loss: 1.5094 - rmse: 1.1350 - val_loss: 1.5980 - val_rmse: 0.9785\n",
      "Epoch 955/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.4906 - rmse: 1.1293 - val_loss: 1.5984 - val_rmse: 0.9640\n",
      "Epoch 956/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.4963 - rmse: 1.1302 - val_loss: 1.6897 - val_rmse: 1.0001\n",
      "Epoch 957/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 1.4792 - rmse: 1.1256 - val_loss: 1.5988 - val_rmse: 0.9661\n",
      "Epoch 958/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.4937 - rmse: 1.1334 - val_loss: 1.6021 - val_rmse: 0.9669\n",
      "Epoch 959/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.5166 - rmse: 1.1377 - val_loss: 1.6153 - val_rmse: 0.9743\n",
      "Epoch 960/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 1.4946 - rmse: 1.1317 - val_loss: 1.7110 - val_rmse: 1.0071\n",
      "Epoch 961/1300\n",
      "16852/16852 [==============================] - 12s 699us/sample - loss: 1.4888 - rmse: 1.1272 - val_loss: 1.6752 - val_rmse: 0.9991\n",
      "Epoch 962/1300\n",
      "16852/16852 [==============================] - 12s 693us/sample - loss: 1.5048 - rmse: 1.1326 - val_loss: 1.6153 - val_rmse: 0.9706\n",
      "Epoch 963/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 1.5215 - rmse: 1.1398 - val_loss: 1.6090 - val_rmse: 0.9700\n",
      "Epoch 964/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.4954 - rmse: 1.1296 - val_loss: 1.6233 - val_rmse: 0.9707\n",
      "Epoch 965/1300\n",
      "16852/16852 [==============================] - 12s 699us/sample - loss: 1.5119 - rmse: 1.1359 - val_loss: 1.6378 - val_rmse: 0.9766\n",
      "Epoch 966/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.4883 - rmse: 1.1289 - val_loss: 1.6389 - val_rmse: 0.9744\n",
      "Epoch 967/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.4827 - rmse: 1.1251 - val_loss: 1.7383 - val_rmse: 1.0196\n",
      "Epoch 968/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.4927 - rmse: 1.1306 - val_loss: 1.6607 - val_rmse: 0.9840\n",
      "Epoch 969/1300\n",
      "16852/16852 [==============================] - 12s 700us/sample - loss: 1.4782 - rmse: 1.1267 - val_loss: 1.5779 - val_rmse: 0.9698\n",
      "Epoch 970/1300\n",
      "16852/16852 [==============================] - 12s 700us/sample - loss: 1.5034 - rmse: 1.1329 - val_loss: 1.6858 - val_rmse: 0.9909\n",
      "Epoch 971/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.4788 - rmse: 1.1245 - val_loss: 1.6077 - val_rmse: 0.9654\n",
      "Epoch 972/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.5219 - rmse: 1.1393 - val_loss: 1.6555 - val_rmse: 0.9823\n",
      "Epoch 973/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 1.4991 - rmse: 1.1319 - val_loss: 1.5921 - val_rmse: 0.9644\n",
      "Epoch 974/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.4913 - rmse: 1.1287 - val_loss: 1.7549 - val_rmse: 1.0142\n",
      "Epoch 975/1300\n",
      "16852/16852 [==============================] - 12s 705us/sample - loss: 1.4889 - rmse: 1.1284 - val_loss: 1.5990 - val_rmse: 0.9662\n",
      "Epoch 976/1300\n",
      "16852/16852 [==============================] - 12s 709us/sample - loss: 1.4955 - rmse: 1.1313 - val_loss: 1.5719 - val_rmse: 0.9565\n",
      "Epoch 977/1300\n",
      "16852/16852 [==============================] - 12s 712us/sample - loss: 1.4920 - rmse: 1.1294 - val_loss: 1.6076 - val_rmse: 0.9678\n",
      "Epoch 978/1300\n",
      "16852/16852 [==============================] - 12s 710us/sample - loss: 1.5000 - rmse: 1.1319 - val_loss: 1.6159 - val_rmse: 0.9651\n",
      "Epoch 979/1300\n",
      "16852/16852 [==============================] - 12s 712us/sample - loss: 1.4997 - rmse: 1.1313 - val_loss: 1.5929 - val_rmse: 0.9627\n",
      "Epoch 980/1300\n",
      "16852/16852 [==============================] - 12s 709us/sample - loss: 1.4907 - rmse: 1.1268 - val_loss: 1.6180 - val_rmse: 0.9698\n",
      "Epoch 981/1300\n",
      "16852/16852 [==============================] - 12s 707us/sample - loss: 1.5147 - rmse: 1.1368 - val_loss: 1.6395 - val_rmse: 0.9785\n",
      "Epoch 982/1300\n",
      "16852/16852 [==============================] - 12s 711us/sample - loss: 1.4997 - rmse: 1.1308 - val_loss: 1.6398 - val_rmse: 0.9736\n",
      "Epoch 983/1300\n",
      "16852/16852 [==============================] - 12s 709us/sample - loss: 1.5000 - rmse: 1.1323 - val_loss: 1.5716 - val_rmse: 0.9606\n",
      "Epoch 984/1300\n",
      "16852/16852 [==============================] - 12s 714us/sample - loss: 1.4960 - rmse: 1.1304 - val_loss: 1.6806 - val_rmse: 0.9982\n",
      "Epoch 985/1300\n",
      "16852/16852 [==============================] - 12s 709us/sample - loss: 1.5008 - rmse: 1.1314 - val_loss: 1.6412 - val_rmse: 0.9719\n",
      "Epoch 986/1300\n",
      "16852/16852 [==============================] - 12s 707us/sample - loss: 1.4932 - rmse: 1.1285 - val_loss: 1.5878 - val_rmse: 0.9617\n",
      "Epoch 987/1300\n",
      "16852/16852 [==============================] - 12s 708us/sample - loss: 1.4970 - rmse: 1.1317 - val_loss: 1.6185 - val_rmse: 0.9694\n",
      "Epoch 988/1300\n",
      "16852/16852 [==============================] - 12s 704us/sample - loss: 1.4763 - rmse: 1.1238 - val_loss: 1.6139 - val_rmse: 0.9729\n",
      "Epoch 989/1300\n",
      "16852/16852 [==============================] - 12s 712us/sample - loss: 1.4871 - rmse: 1.1275 - val_loss: 1.6246 - val_rmse: 0.9659\n",
      "Epoch 990/1300\n",
      "16852/16852 [==============================] - 12s 706us/sample - loss: 1.4779 - rmse: 1.1231 - val_loss: 1.6231 - val_rmse: 0.9733\n",
      "Epoch 991/1300\n",
      "16852/16852 [==============================] - 12s 705us/sample - loss: 1.5035 - rmse: 1.1318 - val_loss: 1.5816 - val_rmse: 0.9578\n",
      "Epoch 992/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 1.4856 - rmse: 1.1268 - val_loss: 1.7026 - val_rmse: 0.9963\n",
      "Epoch 993/1300\n",
      "16852/16852 [==============================] - 12s 703us/sample - loss: 1.4947 - rmse: 1.1297 - val_loss: 1.5519 - val_rmse: 0.9494\n",
      "Epoch 994/1300\n",
      "16852/16852 [==============================] - 12s 699us/sample - loss: 1.4842 - rmse: 1.1247 - val_loss: 1.6029 - val_rmse: 0.9672\n",
      "Epoch 995/1300\n",
      "16852/16852 [==============================] - 12s 700us/sample - loss: 1.5092 - rmse: 1.1328 - val_loss: 1.5622 - val_rmse: 0.9523\n",
      "Epoch 996/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 1.4671 - rmse: 1.1196 - val_loss: 1.5756 - val_rmse: 0.9616\n",
      "Epoch 997/1300\n",
      "16852/16852 [==============================] - 12s 709us/sample - loss: 1.4922 - rmse: 1.1287 - val_loss: 1.6063 - val_rmse: 0.9705\n",
      "Epoch 998/1300\n",
      "16852/16852 [==============================] - 12s 708us/sample - loss: 1.5043 - rmse: 1.1309 - val_loss: 1.6005 - val_rmse: 0.9665\n",
      "Epoch 999/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.4877 - rmse: 1.1279 - val_loss: 1.5722 - val_rmse: 0.9535\n",
      "Epoch 1000/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.5005 - rmse: 1.1304 - val_loss: 1.5850 - val_rmse: 0.9608\n",
      "Epoch 1001/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.4762 - rmse: 1.1238 - val_loss: 1.5968 - val_rmse: 0.9593\n",
      "Epoch 1002/1300\n",
      "16852/16852 [==============================] - 12s 699us/sample - loss: 1.4809 - rmse: 1.1239 - val_loss: 1.6106 - val_rmse: 0.9669\n",
      "Epoch 1003/1300\n",
      "16852/16852 [==============================] - 12s 710us/sample - loss: 1.4906 - rmse: 1.1270 - val_loss: 1.6457 - val_rmse: 0.9864\n",
      "Epoch 1004/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.4800 - rmse: 1.1224 - val_loss: 1.6261 - val_rmse: 0.9664\n",
      "Epoch 1005/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 1.4802 - rmse: 1.1250 - val_loss: 1.5783 - val_rmse: 0.9550\n",
      "Epoch 1006/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.4815 - rmse: 1.1233 - val_loss: 1.5699 - val_rmse: 0.9559\n",
      "Epoch 1007/1300\n",
      "16852/16852 [==============================] - 12s 699us/sample - loss: 1.5012 - rmse: 1.1318 - val_loss: 1.7277 - val_rmse: 1.0114\n",
      "Epoch 1008/1300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16852/16852 [==============================] - 12s 709us/sample - loss: 1.4735 - rmse: 1.1225 - val_loss: 1.6154 - val_rmse: 0.9710\n",
      "Epoch 1009/1300\n",
      "16852/16852 [==============================] - 12s 708us/sample - loss: 1.5051 - rmse: 1.1315 - val_loss: 1.6345 - val_rmse: 0.9701\n",
      "Epoch 1010/1300\n",
      "16852/16852 [==============================] - 12s 703us/sample - loss: 1.4875 - rmse: 1.1263 - val_loss: 1.6365 - val_rmse: 0.9681\n",
      "Epoch 1011/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.4645 - rmse: 1.1191 - val_loss: 1.7436 - val_rmse: 1.0178\n",
      "Epoch 1012/1300\n",
      "16852/16852 [==============================] - 12s 709us/sample - loss: 1.4783 - rmse: 1.1233 - val_loss: 1.6007 - val_rmse: 0.9602\n",
      "Epoch 1013/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.4819 - rmse: 1.1247 - val_loss: 1.6434 - val_rmse: 0.9753\n",
      "Epoch 1014/1300\n",
      "16852/16852 [==============================] - 12s 709us/sample - loss: 1.4950 - rmse: 1.1293 - val_loss: 1.6382 - val_rmse: 0.9700\n",
      "Epoch 1015/1300\n",
      "16852/16852 [==============================] - 12s 708us/sample - loss: 1.5023 - rmse: 1.1308 - val_loss: 1.6057 - val_rmse: 0.9563\n",
      "Epoch 1016/1300\n",
      "16852/16852 [==============================] - 12s 707us/sample - loss: 1.4835 - rmse: 1.1252 - val_loss: 1.5934 - val_rmse: 0.9587\n",
      "Epoch 1017/1300\n",
      "16852/16852 [==============================] - 12s 705us/sample - loss: 1.4739 - rmse: 1.1227 - val_loss: 1.5954 - val_rmse: 0.9593\n",
      "Epoch 1018/1300\n",
      "16852/16852 [==============================] - 12s 709us/sample - loss: 1.4969 - rmse: 1.1282 - val_loss: 1.5976 - val_rmse: 0.9587\n",
      "Epoch 1019/1300\n",
      "16852/16852 [==============================] - 12s 709us/sample - loss: 1.4803 - rmse: 1.1229 - val_loss: 1.5870 - val_rmse: 0.9531\n",
      "Epoch 1020/1300\n",
      "16852/16852 [==============================] - 12s 709us/sample - loss: 1.4746 - rmse: 1.1227 - val_loss: 1.6723 - val_rmse: 0.9826\n",
      "Epoch 1021/1300\n",
      "16852/16852 [==============================] - 12s 708us/sample - loss: 1.4830 - rmse: 1.1256 - val_loss: 1.6115 - val_rmse: 0.9649\n",
      "Epoch 1022/1300\n",
      "16852/16852 [==============================] - 12s 708us/sample - loss: 1.4753 - rmse: 1.1207 - val_loss: 1.6545 - val_rmse: 0.9768\n",
      "Epoch 1023/1300\n",
      "16852/16852 [==============================] - 12s 706us/sample - loss: 1.4538 - rmse: 1.1142 - val_loss: 1.5651 - val_rmse: 0.9524\n",
      "Epoch 1024/1300\n",
      "16852/16852 [==============================] - 12s 709us/sample - loss: 1.4858 - rmse: 1.1259 - val_loss: 1.5963 - val_rmse: 0.9589\n",
      "Epoch 1025/1300\n",
      "16852/16852 [==============================] - 12s 714us/sample - loss: 1.4733 - rmse: 1.1221 - val_loss: 1.6343 - val_rmse: 0.9690\n",
      "Epoch 1026/1300\n",
      "16852/16852 [==============================] - 12s 708us/sample - loss: 1.4754 - rmse: 1.1222 - val_loss: 1.5924 - val_rmse: 0.9548\n",
      "Epoch 1027/1300\n",
      "16852/16852 [==============================] - 12s 710us/sample - loss: 1.4948 - rmse: 1.1273 - val_loss: 1.5579 - val_rmse: 0.9581\n",
      "Epoch 1028/1300\n",
      "16852/16852 [==============================] - 12s 712us/sample - loss: 1.4757 - rmse: 1.1221 - val_loss: 1.5993 - val_rmse: 0.9583\n",
      "Epoch 1029/1300\n",
      "16852/16852 [==============================] - 12s 708us/sample - loss: 1.4831 - rmse: 1.1245 - val_loss: 1.6268 - val_rmse: 0.9654\n",
      "Epoch 1030/1300\n",
      "16852/16852 [==============================] - 12s 708us/sample - loss: 1.4782 - rmse: 1.1220 - val_loss: 1.6135 - val_rmse: 0.9629\n",
      "Epoch 1031/1300\n",
      "16852/16852 [==============================] - 12s 709us/sample - loss: 1.4919 - rmse: 1.1295 - val_loss: 1.5606 - val_rmse: 0.9529\n",
      "Epoch 1032/1300\n",
      "16852/16852 [==============================] - 12s 709us/sample - loss: 1.4757 - rmse: 1.1214 - val_loss: 1.5679 - val_rmse: 0.9503\n",
      "Epoch 1033/1300\n",
      "16852/16852 [==============================] - 12s 706us/sample - loss: 1.4722 - rmse: 1.1214 - val_loss: 1.6073 - val_rmse: 0.9592\n",
      "Epoch 1034/1300\n",
      "16852/16852 [==============================] - 12s 717us/sample - loss: 1.4928 - rmse: 1.1283 - val_loss: 1.5342 - val_rmse: 0.9409\n",
      "Epoch 1035/1300\n",
      "16852/16852 [==============================] - 12s 708us/sample - loss: 1.4803 - rmse: 1.1232 - val_loss: 1.6251 - val_rmse: 0.9673\n",
      "Epoch 1036/1300\n",
      "16852/16852 [==============================] - 12s 699us/sample - loss: 1.4733 - rmse: 1.1212 - val_loss: 1.6007 - val_rmse: 0.9582\n",
      "Epoch 1037/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.4640 - rmse: 1.1194 - val_loss: 1.6121 - val_rmse: 0.9623\n",
      "Epoch 1038/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.4837 - rmse: 1.1265 - val_loss: 1.5699 - val_rmse: 0.9568\n",
      "Epoch 1039/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.4785 - rmse: 1.1225 - val_loss: 1.5476 - val_rmse: 0.9503\n",
      "Epoch 1040/1300\n",
      "16852/16852 [==============================] - 12s 700us/sample - loss: 1.4777 - rmse: 1.1228 - val_loss: 1.6305 - val_rmse: 0.9651\n",
      "Epoch 1041/1300\n",
      "16852/16852 [==============================] - 12s 709us/sample - loss: 1.4840 - rmse: 1.1250 - val_loss: 1.5845 - val_rmse: 0.9558\n",
      "Epoch 1042/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.4831 - rmse: 1.1241 - val_loss: 1.6148 - val_rmse: 0.9617\n",
      "Epoch 1043/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.4637 - rmse: 1.1176 - val_loss: 1.6088 - val_rmse: 0.9607\n",
      "Epoch 1044/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.4762 - rmse: 1.1222 - val_loss: 1.6232 - val_rmse: 0.9693\n",
      "Epoch 1045/1300\n",
      "16852/16852 [==============================] - 12s 700us/sample - loss: 1.4743 - rmse: 1.1197 - val_loss: 1.5788 - val_rmse: 0.9544\n",
      "Epoch 1046/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.4750 - rmse: 1.1221 - val_loss: 1.6467 - val_rmse: 0.9754\n",
      "Epoch 1047/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 1.4648 - rmse: 1.1186 - val_loss: 1.6243 - val_rmse: 0.9745\n",
      "Epoch 1048/1300\n",
      "16852/16852 [==============================] - 12s 708us/sample - loss: 1.4799 - rmse: 1.1238 - val_loss: 1.6330 - val_rmse: 0.9732\n",
      "Epoch 1049/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.4538 - rmse: 1.1139 - val_loss: 1.5760 - val_rmse: 0.9552\n",
      "Epoch 1050/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.4567 - rmse: 1.1146 - val_loss: 1.5676 - val_rmse: 0.9539\n",
      "Epoch 1051/1300\n",
      "16852/16852 [==============================] - 12s 711us/sample - loss: 1.4864 - rmse: 1.1250 - val_loss: 1.5755 - val_rmse: 0.9544\n",
      "Epoch 1052/1300\n",
      "16852/16852 [==============================] - 12s 715us/sample - loss: 1.4608 - rmse: 1.1162 - val_loss: 1.6073 - val_rmse: 0.9631\n",
      "Epoch 1053/1300\n",
      "16852/16852 [==============================] - 12s 712us/sample - loss: 1.4762 - rmse: 1.1206 - val_loss: 1.6347 - val_rmse: 0.9830\n",
      "Epoch 1054/1300\n",
      "16852/16852 [==============================] - 12s 711us/sample - loss: 1.4615 - rmse: 1.1157 - val_loss: 1.5442 - val_rmse: 0.9435\n",
      "Epoch 1055/1300\n",
      "16852/16852 [==============================] - 12s 700us/sample - loss: 1.4818 - rmse: 1.1227 - val_loss: 1.5473 - val_rmse: 0.9473\n",
      "Epoch 1056/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.4609 - rmse: 1.1167 - val_loss: 1.6462 - val_rmse: 0.9719\n",
      "Epoch 1057/1300\n",
      "16852/16852 [==============================] - 12s 711us/sample - loss: 1.4566 - rmse: 1.1133 - val_loss: 1.5660 - val_rmse: 0.9528\n",
      "Epoch 1058/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 1.4608 - rmse: 1.1171 - val_loss: 1.6422 - val_rmse: 0.9780\n",
      "Epoch 1059/1300\n",
      "16852/16852 [==============================] - 12s 703us/sample - loss: 1.4797 - rmse: 1.1222 - val_loss: 1.6139 - val_rmse: 0.9673\n",
      "Epoch 1060/1300\n",
      "16852/16852 [==============================] - 12s 701us/sample - loss: 1.4828 - rmse: 1.1221 - val_loss: 1.5962 - val_rmse: 0.9610\n",
      "Epoch 1061/1300\n",
      "16852/16852 [==============================] - 12s 713us/sample - loss: 1.4613 - rmse: 1.1162 - val_loss: 1.6089 - val_rmse: 0.9560\n",
      "Epoch 1062/1300\n",
      "16852/16852 [==============================] - 12s 700us/sample - loss: 1.4764 - rmse: 1.1207 - val_loss: 1.6545 - val_rmse: 0.9748\n",
      "Epoch 1063/1300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16852/16852 [==============================] - 12s 694us/sample - loss: 1.4703 - rmse: 1.1195 - val_loss: 1.5494 - val_rmse: 0.9466\n",
      "Epoch 1064/1300\n",
      "16852/16852 [==============================] - 12s 700us/sample - loss: 1.4868 - rmse: 1.1244 - val_loss: 1.5593 - val_rmse: 0.9607\n",
      "Epoch 1065/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.4544 - rmse: 1.1142 - val_loss: 1.6451 - val_rmse: 0.9742\n",
      "Epoch 1066/1300\n",
      "16852/16852 [==============================] - 12s 701us/sample - loss: 1.4670 - rmse: 1.1186 - val_loss: 1.6654 - val_rmse: 0.9830\n",
      "Epoch 1067/1300\n",
      "16852/16852 [==============================] - 12s 709us/sample - loss: 1.4605 - rmse: 1.1160 - val_loss: 1.6068 - val_rmse: 0.9618\n",
      "Epoch 1068/1300\n",
      "16852/16852 [==============================] - 12s 712us/sample - loss: 1.4810 - rmse: 1.1217 - val_loss: 1.5935 - val_rmse: 0.9571\n",
      "Epoch 1069/1300\n",
      "16852/16852 [==============================] - 12s 706us/sample - loss: 1.4719 - rmse: 1.1202 - val_loss: 1.6301 - val_rmse: 0.9696\n",
      "Epoch 1070/1300\n",
      "16852/16852 [==============================] - 12s 707us/sample - loss: 1.4597 - rmse: 1.1153 - val_loss: 1.5733 - val_rmse: 0.9505\n",
      "Epoch 1071/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 1.4720 - rmse: 1.1209 - val_loss: 1.6074 - val_rmse: 0.9575\n",
      "Epoch 1072/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.4701 - rmse: 1.1189 - val_loss: 1.6202 - val_rmse: 0.9682\n",
      "Epoch 1073/1300\n",
      "16852/16852 [==============================] - 12s 699us/sample - loss: 1.4609 - rmse: 1.1162 - val_loss: 1.5921 - val_rmse: 0.9590\n",
      "Epoch 1074/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.4767 - rmse: 1.1220 - val_loss: 1.5581 - val_rmse: 0.9476\n",
      "Epoch 1075/1300\n",
      "16852/16852 [==============================] - 12s 699us/sample - loss: 1.4614 - rmse: 1.1170 - val_loss: 1.6203 - val_rmse: 0.9657\n",
      "Epoch 1076/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.4821 - rmse: 1.1221 - val_loss: 1.6476 - val_rmse: 0.9853\n",
      "Epoch 1077/1300\n",
      "16852/16852 [==============================] - 12s 696us/sample - loss: 1.4576 - rmse: 1.1150 - val_loss: 1.6047 - val_rmse: 0.9597\n",
      "Epoch 1078/1300\n",
      "16852/16852 [==============================] - 12s 699us/sample - loss: 1.4754 - rmse: 1.1196 - val_loss: 1.5688 - val_rmse: 0.9565\n",
      "Epoch 1079/1300\n",
      "16852/16852 [==============================] - 12s 711us/sample - loss: 1.4774 - rmse: 1.1217 - val_loss: 1.6032 - val_rmse: 0.9623\n",
      "Epoch 1080/1300\n",
      "16852/16852 [==============================] - 12s 694us/sample - loss: 1.4678 - rmse: 1.1194 - val_loss: 1.5769 - val_rmse: 0.9512\n",
      "Epoch 1081/1300\n",
      "16852/16852 [==============================] - 12s 708us/sample - loss: 1.4478 - rmse: 1.1096 - val_loss: 1.6211 - val_rmse: 0.9748\n",
      "Epoch 1082/1300\n",
      "16852/16852 [==============================] - 12s 702us/sample - loss: 1.4549 - rmse: 1.1141 - val_loss: 1.5750 - val_rmse: 0.9510\n",
      "Epoch 1083/1300\n",
      "16852/16852 [==============================] - 12s 711us/sample - loss: 1.4691 - rmse: 1.1172 - val_loss: 1.6396 - val_rmse: 0.9686\n",
      "Epoch 1084/1300\n",
      "16852/16852 [==============================] - 12s 707us/sample - loss: 1.4666 - rmse: 1.1186 - val_loss: 1.6134 - val_rmse: 0.9649\n",
      "Epoch 1085/1300\n",
      "16852/16852 [==============================] - 12s 699us/sample - loss: 1.4655 - rmse: 1.1180 - val_loss: 1.6314 - val_rmse: 0.9656\n",
      "Epoch 1086/1300\n",
      "16852/16852 [==============================] - 12s 695us/sample - loss: 1.4621 - rmse: 1.1162 - val_loss: 1.6488 - val_rmse: 0.9760\n",
      "Epoch 1087/1300\n",
      "16852/16852 [==============================] - 12s 699us/sample - loss: 1.4580 - rmse: 1.1158 - val_loss: 1.6111 - val_rmse: 0.9602\n",
      "Epoch 1088/1300\n",
      "16852/16852 [==============================] - 12s 697us/sample - loss: 1.4707 - rmse: 1.1189 - val_loss: 1.6177 - val_rmse: 0.9592\n",
      "Epoch 1089/1300\n",
      "16852/16852 [==============================] - 12s 705us/sample - loss: 1.4456 - rmse: 1.1104 - val_loss: 1.5987 - val_rmse: 0.9574\n",
      "Epoch 1090/1300\n",
      "16852/16852 [==============================] - 12s 702us/sample - loss: 1.4634 - rmse: 1.1167 - val_loss: 1.5941 - val_rmse: 0.9602\n",
      "Epoch 1091/1300\n",
      "16852/16852 [==============================] - 12s 709us/sample - loss: 1.4721 - rmse: 1.1175 - val_loss: 1.5663 - val_rmse: 0.9492\n",
      "Epoch 1092/1300\n",
      "16852/16852 [==============================] - 12s 698us/sample - loss: 1.4734 - rmse: 1.1208 - val_loss: 1.6082 - val_rmse: 0.9603\n",
      "Epoch 1093/1300\n",
      "16852/16852 [==============================] - 12s 700us/sample - loss: 1.4724 - rmse: 1.1207 - val_loss: 1.6149 - val_rmse: 0.9614\n",
      "Epoch 1094/1300\n",
      "16852/16852 [==============================] - 12s 709us/sample - loss: 1.4595 - rmse: 1.1155 - val_loss: 1.5856 - val_rmse: 0.9536\n",
      "Epoch 1095/1300\n",
      "16852/16852 [==============================] - 12s 710us/sample - loss: 1.4570 - rmse: 1.1152 - val_loss: 1.5959 - val_rmse: 0.9635\n",
      "Epoch 1096/1300\n",
      "16852/16852 [==============================] - 12s 711us/sample - loss: 1.4728 - rmse: 1.1194 - val_loss: 1.5629 - val_rmse: 0.9487\n",
      "Epoch 1097/1300\n",
      "16852/16852 [==============================] - 12s 708us/sample - loss: 1.4590 - rmse: 1.1151 - val_loss: 1.6018 - val_rmse: 0.9606\n",
      "Epoch 1098/1300\n",
      "16852/16852 [==============================] - 12s 710us/sample - loss: 1.4547 - rmse: 1.1135 - val_loss: 1.6172 - val_rmse: 0.9594\n",
      "Epoch 1099/1300\n",
      "16852/16852 [==============================] - 12s 708us/sample - loss: 1.4509 - rmse: 1.1128 - val_loss: 1.6288 - val_rmse: 0.9642\n",
      "Epoch 1100/1300\n",
      "16852/16852 [==============================] - 12s 710us/sample - loss: 1.4658 - rmse: 1.1165 - val_loss: 1.5693 - val_rmse: 0.9495\n",
      "Epoch 1101/1300\n",
      "16852/16852 [==============================] - 13s 768us/sample - loss: 1.4589 - rmse: 1.1139 - val_loss: 1.5611 - val_rmse: 0.9521\n",
      "Epoch 1102/1300\n",
      "16640/16852 [============================>.] - ETA: 0s - loss: 1.4893 - rmse: 1.1249"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-a5e6cb07af3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m                     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                     \u001b[0mvalidation_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m#data = (x_test, y_test),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m                     )\n\u001b[1;32m     11\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'seaborn-darkgrid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    878\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[1;32m    362\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m           \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m           validation_in_fit=True)\n\u001b[0m\u001b[1;32m    365\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0mval_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mval_results\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3076\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "USE_SAVED_MODEL = False\n",
    "\n",
    "if USE_SAVED_MODEL == False:\n",
    "    history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs = 1300,\n",
    "                    batch_size = 256,\n",
    "                    validation_split = 0.2, #data = (x_test, y_test),\n",
    "                    callbacks = callbacks\n",
    "                    )\n",
    "    plt.style.use('seaborn-darkgrid')\n",
    "    plt.plot(history.history['rmse'])\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Root Mean Square Error')\n",
    "    plt.title('Model Training Error')\n",
    "    plt.show() \n",
    "    \n",
    "else:\n",
    "    model.load_weights(model_dir+\"final_model_weights.h5\")\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookid_dir = '../input/IdLookupTable.csv'\n",
    "lookid_data = pd.read_csv(lookid_dir)\n",
    "test_data = pd.read_csv(test_file)\n",
    "\n",
    "x_test = []\n",
    "for i in range(0,len(test_data)):\n",
    "    img = test_data['Image'][i].split(' ')\n",
    "    x_test.append(img)\n",
    "    \n",
    "x_test = np.array(x_test,dtype = 'float')\n",
    "x_test = x_test/255.0\n",
    "x_test = x_test.reshape(-1,96,96,1)    \n",
    "\n",
    "y_test = model.predict(x_test)\n",
    "y_test = np.clip(y_test,0,96)\n",
    "\n",
    "lookid_list = list(lookid_data['FeatureName'])\n",
    "imageID = list(lookid_data['ImageId']-1)\n",
    "pred_list = list(y_test)\n",
    "\n",
    "rowid = list(lookid_data['RowId'])\n",
    "\n",
    "feature = []\n",
    "for f in list(lookid_data['FeatureName']):\n",
    "    feature.append(lookid_list.index(f))\n",
    "    \n",
    "    \n",
    "submit_data = []\n",
    "for x,y in zip(imageID,feature):\n",
    "    submit_data.append(pred_list[x][y])\n",
    "rowid = pd.Series(rowid,name = 'RowId')\n",
    "loc = pd.Series(submit_data,name = 'Location')\n",
    "submission = pd.concat([rowid,loc],axis = 1)\n",
    "submission.to_csv('../output/w207_temp_submission.csv',index = False)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "    sess = get_session()\n",
    "    clear_session()\n",
    "    sess.close()\n",
    " \n",
    "    try:\n",
    "        del model # this is from global space - change this as you need\n",
    "    except:\n",
    "        print(\"Model clear Failed\")\n",
    "    print(gc.collect())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
