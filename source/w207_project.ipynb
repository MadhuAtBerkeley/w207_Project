{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4dbced162440c393a0a5b7e5aee344711a30e994"
   },
   "source": [
    "# Facial Keypoint Detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "369fa247a546e39d82bdfdc5b7d4ed58baa40e4f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/user1/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/user1/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/user1/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/user1/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/user1/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/user1/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.backend.tensorflow_backend import clear_session\n",
    "from keras.backend.tensorflow_backend import get_session\n",
    "from tensorflow.keras.layers import LeakyReLU, ReLU\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Activation, Convolution2D, BatchNormalization, Flatten\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, MaxPool2D\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "from keras import backend\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import cv2\n",
    "import os, gc, json, math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n",
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print(keras.__version__)\n",
    "import tensorflow\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pixels(data):\n",
    "    \"\"\"\n",
    "    Convert pixels to the right intensity 0-1 and in a square matrix.\n",
    "    \"\"\"\n",
    "    data = np.array([row.split(' ') for row in data['Image']],dtype='float') / 255.0\n",
    "    data = data.reshape(-1,96,96,1)\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_img(sample_img,coord=None):\n",
    "    \"\"\"\n",
    "    Display an image. For debugging, display a coordinate on the image.\n",
    "    input:\n",
    "        - sample_img: numpy array. image to be displayed\n",
    "        - coord: lst. of coordinates in form [[x_coordinate,y_coordinate],[x_coordinate,y_coordinate]]\n",
    "    TODO handle multiple coordinates. Work out bugs with multiple coordinates\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.imshow(sample_img.reshape(96,96),cmap='gray')\n",
    "    if coord is not None:\n",
    "        plt.scatter(coord[0],coord[1],marker = '*',c='r')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_facial_keypoints(data,ind):\n",
    "    \"\"\"\n",
    "    Structure the coordinates for all facial keypoints for a single image.\n",
    "    inputs:\n",
    "        - data: numpy array containing rows as each image sample and columns as facial keypoint coordinates\n",
    "        - ind: index of the image\n",
    "    output:\n",
    "        - numpy array with format [[list of x-coordinates],[list of y-coordinates]]\n",
    "    \"\"\"\n",
    "    data[ind]\n",
    "    it = iter(data[ind])\n",
    "    x_coord = []\n",
    "    y_coord = []\n",
    "\n",
    "    for x in it:\n",
    "        x_coord.append(x)\n",
    "        y_coord.append(next(it))\n",
    "    \n",
    "    return(np.array([x_coord,y_coord]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Reset Keras Session\n",
    "def reset_keras():\n",
    "    sess = get_session()\n",
    "    clear_session()\n",
    "    sess.close()\n",
    "    sess = get_session()\n",
    "\n",
    "    try:\n",
    "        del model # this is from global space - change this as you need\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    print(gc.collect()) # if it's done something you should see a number being outputted\n",
    "\n",
    "    # use the same config as you used to create the session\n",
    "    config = tensorflow.ConfigProto()\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = 1\n",
    "    config.gpu_options.visible_device_list = \"0\"\n",
    "    set_session(tensorflow.Session(config=config))\n",
    "    \n",
    "reset_keras()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "fa1b76273d02502e3fd668dddf74ecf522044524"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('../output/'):\n",
    "    os.makedirs('../output/model')\n",
    "    os.makedirs('../output/history')\n",
    "    \n",
    "    \n",
    "model_dir = \"../output/model/\"\n",
    "history_dir = \"../output/history/\"\n",
    "\n",
    "train_file = '../input/training/training.csv'\n",
    "test_file = '../input/test/test.csv'\n",
    "train_data = pd.read_csv(train_file)  \n",
    "#test_data = pd.read_csv(test_file)\n",
    "\n",
    "\n",
    "bad_samples = [1747, 1731, 1877, 1881, 1979, 2199, 2289, 2321, 2453, 3173, 3296, 3447, 4180, 6859,\n",
    "              2090, 2175, 1907, 2562, 2818, 3296, 3447, 4263, 4482, 4490, 4636, 5059, 6493, 6585, 6906]\n",
    "\n",
    "train_data = train_data.drop(bad_samples).reset_index(drop=True)\n",
    "train_clean = train_data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "cfd045f7166f9cce2e2075b3ead83813d07012c8",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2140, 31)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_clean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e1b88f1528838c0a8fec61f9a02a70b5077312e9"
   },
   "source": [
    "Create training vector with images and normalize thee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = convert_pixels(train_data)\n",
    "x_clean = convert_pixels(train_clean)\n",
    "y_train = train_data[[col for col in train_data.columns if col != 'Image']].to_numpy()\n",
    "y_clean = train_clean[[col for col in train_clean.columns if col != 'Image']].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a833f4cc5e559774d3a310fd09d40d31e49e71da"
   },
   "source": [
    "Generate labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "e9d804a035809cdf8ffda19f41ce3feb278a38fb"
   },
   "outputs": [],
   "source": [
    "\n",
    "#imputer = KNNImputer(n_neighbors=3, weights='distance')\n",
    "\n",
    "imputer = IterativeImputer(max_iter=1000, tol=0.01, random_state=1)\n",
    "y_train = imputer.fit_transform(y_train)\n",
    "\n",
    "#imputer = IterativeImputer(max_iter=1000, tol=0.01, random_state=2)\n",
    "#y_clean = imputer.fit_transform(y_clean)\n",
    "\n",
    "bad_bottom_lip = [210, 350, 499, 512, 810, 839, 895, 1058, 1194,1230, 1245, 1546, 1548]\n",
    "for sample in bad_bottom_lip:\n",
    "    y_train[sample][29] = 94\n",
    "    y_train[sample][28] = y_train[sample][26]\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set feature engineering parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_na = False\n",
    "add_flip_horiz = True\n",
    "add_blur_img = False\n",
    "add_rotate_img = False\n",
    "orig_x_train = x_clean.copy()\n",
    "#orig_y_train = y_clean.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill NA in the training labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if fill_na:\n",
    "    # https://stackoverflow.com/questions/18689235/numpy-array-replace-nan-values-with-average-of-columns\n",
    "    # get column means\n",
    "    col_mean = np.nanmean(y_train,axis=0)\n",
    "\n",
    "    # find the x,y indices that are missing from y_train\n",
    "    inds = np.where(np.isnan(y_train))\n",
    "\n",
    "    # fill in missing values in y_train with the column means. \"take\" is much more efficient than fancy indexing\n",
    "    y_train[inds] = np.take(col_mean, inds[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flip images horizontally and add to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_img_horiz(train_data):\n",
    "    \"\"\"\n",
    "    Flip images horizontally for all training images\n",
    "    \"\"\"\n",
    "    # Flip images\n",
    "    x_train = convert_pixels(train_data)\n",
    "    flip_img = np.array([np.fliplr(x_train[[ind]][0]) for ind in range(x_train.shape[0])])\n",
    "    \n",
    "    # Flip coordinates\n",
    "    train_data_flip = train_data.copy()\n",
    "    x_columns = [col for col in train_data.columns if '_x' in col]\n",
    "    train_data_flip[x_columns] = train_data[x_columns].applymap(lambda x: 96-x)\n",
    "    \n",
    "    #left and right are swapped so undo\n",
    "    left_columns = [col for col in train_data.columns if 'left' in col]\n",
    "    right_columns = [col for col in train_data.columns if 'right' in col]\n",
    "    train_data_flip[left_columns+right_columns] = train_data_flip[right_columns+left_columns]\n",
    "    \n",
    "    flip_coord = train_data_flip[[col for col in train_data if col != 'Image']].to_numpy()\n",
    "    return(flip_img,flip_coord)\n",
    "\n",
    "if add_flip_horiz:\n",
    "    # Apply the augmentation and add the new data to the training set\n",
    "    flipped_img,flipped_coord = flip_img_horiz(train_clean)\n",
    "    \n",
    "   \n",
    "    x_train = np.append(x_train,flipped_img,axis=0)\n",
    "    y_train = np.append(y_train,flipped_coord,axis=0)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Gaussian blurring with a 5x5 filter with $\\sigma$ = 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9162, 30)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blur_img():\n",
    "    \"\"\"\n",
    "    Add Gaussian blurring to the images\n",
    "    \"\"\"\n",
    "    # https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_filtering/py_filtering.html\n",
    "    blur_img = np.array([cv2.GaussianBlur(orig_x_train[[ind]][0],(5,5),2).reshape(96,96,1) for ind in range(orig_x_train.shape[0])])\n",
    "    \n",
    "    return(blur_img)\n",
    "\n",
    "if add_blur_img:\n",
    "    x_train = np.append(x_train,blur_img(),axis=0)\n",
    "    y_train = np.append(y_train,orig_y_train,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_img(x_train, y_train):\n",
    "    \"\"\"\"\n",
    "    Rotate images by angles between [5, 10, 14 degrees]\n",
    "    \"\"\"\n",
    "    angles = [5, -5, 10, -10, 14, -14]\n",
    "    b = np.ones((1,3))\n",
    "    rows,cols = (96,96)\n",
    "    x_train_rot = []\n",
    "    y_train_rot = y_train.copy()\n",
    "    M_angles = [cv2.getRotationMatrix2D((cols/2,rows/2),angle,1) for angle in angles]\n",
    "    \n",
    "    for i in range(x_train.shape[0]):\n",
    "        #M = cv2.getRotationMatrix2D((cols/2,rows/2),np.random.choice(angles,1),1)\n",
    "        M = M_angles[np.random.choice(len(M_angles))]\n",
    "        x_train_rot.append((cv2.warpAffine(x_train[[i]].reshape(rows,cols,1),M,(cols,rows)).reshape(96,96,1)))\n",
    "       \n",
    "        #apply affine transformation to (x,y) labels\n",
    "        for j in range(int(y_train.shape[1]/2)):\n",
    "            b[:,0:2] = y_train[i,2*j:2*j+2]\n",
    "            y_train_rot[i,2*j:2*j+2] = np.dot(b,M.transpose()) \n",
    "    \n",
    "    x_train_rot = np.array(x_train_rot)\n",
    "    return x_train_rot, y_train_rot\n",
    "\n",
    "if add_rotate_img:\n",
    "    \n",
    "    x_rotate, y_rotate = rotate_img(x_train,y_train)\n",
    "    x_train = np.append(x_train,x_rotate,axis=0)\n",
    "    y_train = np.append(y_train,y_rotate,axis=0)   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callback function if detailed log required\n",
    "class History(tensorflow.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.train_loss = []\n",
    "        self.train_rmse = []\n",
    "        self.val_rmse = []\n",
    "        self.val_loss = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.train_loss.append(logs.get('loss'))\n",
    "        self.train_rmse.append(logs.get('rmse'))\n",
    "        \n",
    "    def on_epoch_end(self, batch, logs={}):    \n",
    "        self.val_rmse.append(logs.get('val_rmse'))\n",
    "        self.val_loss.append(logs.get('val_loss'))\n",
    "        \n",
    "# Implement ModelCheckPoint callback function to save CNN model\n",
    "class CNN_ModelCheckpoint(tensorflow.keras.callbacks.Callback):\n",
    "\n",
    "    def __init__(self, model, filename):\n",
    "        self.filename = filename\n",
    "        self.cnn_model = model\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.max_val_rmse = math.inf\n",
    "        \n",
    " \n",
    "    def on_epoch_end(self, batch, logs={}):    \n",
    "        val_rmse = logs.get('val_rmse')\n",
    "        if(val_rmse < self.max_val_rmse):\n",
    "           self.max_val_rmse = val_rmse\n",
    "           self.cnn_model.save_weights(self.filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 96, 96, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 96, 96, 32)        288       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 96, 96, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_12 (B (None, 96, 96, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 48, 48, 64)        18432     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 48, 48, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_13 (B (None, 48, 48, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 48, 48, 64)        36864     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 48, 48, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_14 (B (None, 48, 48, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 24, 24, 96)        55296     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 24, 24, 96)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_15 (B (None, 24, 24, 96)        384       \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 24, 24, 96)        82944     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 24, 24, 96)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_16 (B (None, 24, 24, 96)        384       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 12, 12, 96)        0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 12, 12, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 12, 12, 128)       110592    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)   (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_17 (B (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 12, 12, 128)       147456    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)   (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_18 (B (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 6, 6, 256)         294912    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)   (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_19 (B (None, 6, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 6, 6, 256)         589824    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)   (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_20 (B (None, 6, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 3, 3, 512)         1179648   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)   (None, 3, 3, 512)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_21 (B (None, 3, 3, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 3, 3, 512)         2359296   \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_22 (B (None, 3, 3, 512)         2048      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               2359808   \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 30)                15390     \n",
      "=================================================================\n",
      "Total params: 7,259,326\n",
      "Trainable params: 7,255,038\n",
      "Non-trainable params: 4,288\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def base_model():\n",
    "    model_input = Input(shape=(96,96,1))\n",
    "\n",
    "    x = Convolution2D(32, (3,3), padding='same', use_bias=False)(model_input)\n",
    "    x = LeakyReLU(alpha = 0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Convolution2D(32, (3,3), padding='same', use_bias=False)(model_input)\n",
    "    x = LeakyReLU(alpha = 0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Convolution2D(64, (3,3), padding='same', use_bias=False)(x)\n",
    "    x = LeakyReLU(alpha = 0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Convolution2D(64, (3,3), padding='same', use_bias=False)(x)\n",
    "    x = LeakyReLU(alpha = 0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Convolution2D(96, (3,3), padding='same', use_bias=False)(x)\n",
    "    x = LeakyReLU(alpha = 0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Convolution2D(96, (3,3), padding='same', use_bias=False)(x)\n",
    "    x = LeakyReLU(alpha = 0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Convolution2D(128, (3,3),padding='same', use_bias=False)(x)\n",
    "    x = LeakyReLU(alpha = 0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Convolution2D(128, (3,3),padding='same', use_bias=False)(x)\n",
    "    x = LeakyReLU(alpha = 0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    #x = Dropout(0.1)(x)\n",
    "    x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Convolution2D(256, (3,3),padding='same',use_bias=False)(x)\n",
    "    x = LeakyReLU(alpha = 0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Convolution2D(256, (3,3),padding='same',use_bias=False)(x)\n",
    "    x = LeakyReLU(alpha = 0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Convolution2D(512, (3,3), padding='same', use_bias=False)(x)\n",
    "    x = LeakyReLU(alpha = 0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Convolution2D(512, (3,3), activation='relu', padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(512,activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    model_output = Dense(30)(x)\n",
    "    model = Model(model_input, model_output, name=\"base_model\")\n",
    "    return model\n",
    "\n",
    "model = base_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom RMSE metric\n",
    "def rmse(y_true, y_pred):\n",
    "    return backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))\n",
    "\n",
    "#from tensorflow.keras.optimizers.schedules import InverseTimeDecay\n",
    "\n",
    "# Use Nadam optimizer with variable learning rate\n",
    "optimizer = Nadam(lr=0.00001,\n",
    "                  beta_1=0.9,\n",
    "                  beta_2=0.999,\n",
    "                  epsilon=1e-08,\n",
    "                  schedule_decay=0.004)\n",
    "\n",
    "\n",
    "# Loss: MSE and Metric = RMSE\n",
    "model.compile(optimizer= optimizer, \n",
    "              loss='mean_squared_error',\n",
    "              metrics=[rmse])\n",
    "\n",
    "#Callback to save the best model\n",
    "saveBase_Model = CNN_ModelCheckpoint(model, model_dir+\"base_model_weights_1.h5\")\n",
    "\n",
    "#define callback functions\n",
    "callbacks = [#EarlyStopping(monitor='val_rmse', patience=3, verbose=2),\n",
    "             saveBase_Model]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e4cf4686b410841f2e34dbb081f3429d1b0f67e9"
   },
   "source": [
    "Run for 1000 epochs and keeping 20% train-valid split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "894af9cbfcf2dca50e7407946cad318157b77d0a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7329 samples, validate on 1833 samples\n",
      "Epoch 1/1000\n",
      "7329/7329 [==============================] - 11s 2ms/sample - loss: 2528.6959 - rmse: 50.2460 - val_loss: 2631.5144 - val_rmse: 51.2785\n",
      "Epoch 2/1000\n",
      "7329/7329 [==============================] - 6s 770us/sample - loss: 2038.0228 - rmse: 45.0622 - val_loss: 2624.6580 - val_rmse: 51.2116\n",
      "Epoch 3/1000\n",
      "7329/7329 [==============================] - 6s 775us/sample - loss: 1476.1080 - rmse: 38.3198 - val_loss: 2593.1172 - val_rmse: 50.9027\n",
      "Epoch 4/1000\n",
      "7329/7329 [==============================] - 6s 781us/sample - loss: 1060.6232 - rmse: 32.4605 - val_loss: 2500.1241 - val_rmse: 49.9810\n",
      "Epoch 5/1000\n",
      "7329/7329 [==============================] - 6s 879us/sample - loss: 757.2951 - rmse: 27.4008 - val_loss: 2329.3429 - val_rmse: 48.2425\n",
      "Epoch 6/1000\n",
      "7329/7329 [==============================] - 6s 801us/sample - loss: 531.9613 - rmse: 22.9364 - val_loss: 2100.3646 - val_rmse: 45.8079\n",
      "Epoch 7/1000\n",
      "7329/7329 [==============================] - 5s 746us/sample - loss: 366.7871 - rmse: 19.0011 - val_loss: 1838.0256 - val_rmse: 42.8488\n",
      "Epoch 8/1000\n",
      "7329/7329 [==============================] - 5s 737us/sample - loss: 249.6259 - rmse: 15.6345 - val_loss: 1579.2584 - val_rmse: 39.7136\n",
      "Epoch 9/1000\n",
      "7329/7329 [==============================] - 6s 866us/sample - loss: 168.9713 - rmse: 12.8194 - val_loss: 1306.4659 - val_rmse: 36.1119\n",
      "Epoch 10/1000\n",
      "7329/7329 [==============================] - 5s 702us/sample - loss: 116.3728 - rmse: 10.5860 - val_loss: 1074.4655 - val_rmse: 32.7294\n",
      "Epoch 11/1000\n",
      "7329/7329 [==============================] - 6s 828us/sample - loss: 84.2844 - rmse: 8.9785 - val_loss: 830.4056 - val_rmse: 28.7319\n",
      "Epoch 12/1000\n",
      "7329/7329 [==============================] - 6s 869us/sample - loss: 65.1744 - rmse: 7.8745 - val_loss: 622.8889 - val_rmse: 24.8319\n",
      "Epoch 13/1000\n",
      "7329/7329 [==============================] - 5s 737us/sample - loss: 54.7940 - rmse: 7.2123 - val_loss: 445.8911 - val_rmse: 20.9383\n",
      "Epoch 14/1000\n",
      "7329/7329 [==============================] - 5s 699us/sample - loss: 49.0080 - rmse: 6.8146 - val_loss: 342.1470 - val_rmse: 18.2775\n",
      "Epoch 15/1000\n",
      "7329/7329 [==============================] - 5s 743us/sample - loss: 45.8260 - rmse: 6.5987 - val_loss: 235.8553 - val_rmse: 15.0996\n",
      "Epoch 16/1000\n",
      "7329/7329 [==============================] - 7s 893us/sample - loss: 44.4244 - rmse: 6.4998 - val_loss: 171.7910 - val_rmse: 12.8190\n",
      "Epoch 17/1000\n",
      "7329/7329 [==============================] - 5s 718us/sample - loss: 43.3519 - rmse: 6.4182 - val_loss: 132.4332 - val_rmse: 11.2038\n",
      "Epoch 18/1000\n",
      "7329/7329 [==============================] - 5s 698us/sample - loss: 42.6928 - rmse: 6.3716 - val_loss: 95.3467 - val_rmse: 9.4646\n",
      "Epoch 19/1000\n",
      "7329/7329 [==============================] - 5s 703us/sample - loss: 42.6479 - rmse: 6.3734 - val_loss: 71.5244 - val_rmse: 8.1668\n",
      "Epoch 20/1000\n",
      "7329/7329 [==============================] - 6s 781us/sample - loss: 42.2296 - rmse: 6.3399 - val_loss: 55.8909 - val_rmse: 7.2004\n",
      "Epoch 21/1000\n",
      "7329/7329 [==============================] - 6s 768us/sample - loss: 42.0107 - rmse: 6.3192 - val_loss: 43.0345 - val_rmse: 6.2962\n",
      "Epoch 22/1000\n",
      "7329/7329 [==============================] - 7s 957us/sample - loss: 41.6256 - rmse: 6.2857 - val_loss: 33.1521 - val_rmse: 5.5117\n",
      "Epoch 23/1000\n",
      "7329/7329 [==============================] - 7s 907us/sample - loss: 41.3239 - rmse: 6.2688 - val_loss: 27.5230 - val_rmse: 4.9974\n",
      "Epoch 24/1000\n",
      "7329/7329 [==============================] - 6s 789us/sample - loss: 41.0285 - rmse: 6.2456 - val_loss: 23.7979 - val_rmse: 4.6404\n",
      "Epoch 25/1000\n",
      "7329/7329 [==============================] - 5s 750us/sample - loss: 41.0209 - rmse: 6.2445 - val_loss: 21.2859 - val_rmse: 4.3821\n",
      "Epoch 26/1000\n",
      "7329/7329 [==============================] - 5s 746us/sample - loss: 40.6017 - rmse: 6.2106 - val_loss: 19.0129 - val_rmse: 4.1377\n",
      "Epoch 27/1000\n",
      "7329/7329 [==============================] - 5s 747us/sample - loss: 40.2888 - rmse: 6.1878 - val_loss: 17.5920 - val_rmse: 3.9760\n",
      "Epoch 28/1000\n",
      "7329/7329 [==============================] - 5s 743us/sample - loss: 40.1603 - rmse: 6.1734 - val_loss: 16.3198 - val_rmse: 3.8296\n",
      "Epoch 29/1000\n",
      "7329/7329 [==============================] - 5s 745us/sample - loss: 39.9555 - rmse: 6.1608 - val_loss: 15.4157 - val_rmse: 3.7192\n",
      "Epoch 30/1000\n",
      "7329/7329 [==============================] - 5s 743us/sample - loss: 39.7159 - rmse: 6.1401 - val_loss: 14.9398 - val_rmse: 3.6597\n",
      "Epoch 31/1000\n",
      "7329/7329 [==============================] - 5s 744us/sample - loss: 39.5121 - rmse: 6.1222 - val_loss: 14.4600 - val_rmse: 3.6012\n",
      "Epoch 32/1000\n",
      "7329/7329 [==============================] - 5s 749us/sample - loss: 39.1251 - rmse: 6.0960 - val_loss: 14.1299 - val_rmse: 3.5590\n",
      "Epoch 33/1000\n",
      "7329/7329 [==============================] - 5s 747us/sample - loss: 39.2140 - rmse: 6.0958 - val_loss: 13.9666 - val_rmse: 3.5367\n",
      "Epoch 34/1000\n",
      "7329/7329 [==============================] - 5s 747us/sample - loss: 38.7650 - rmse: 6.0656 - val_loss: 13.7306 - val_rmse: 3.5069\n",
      "Epoch 35/1000\n",
      "7329/7329 [==============================] - 5s 742us/sample - loss: 38.5752 - rmse: 6.0493 - val_loss: 13.4866 - val_rmse: 3.4735\n",
      "Epoch 36/1000\n",
      "7329/7329 [==============================] - 5s 743us/sample - loss: 38.3630 - rmse: 6.0344 - val_loss: 13.0973 - val_rmse: 3.4222\n",
      "Epoch 37/1000\n",
      "7329/7329 [==============================] - 5s 726us/sample - loss: 38.2075 - rmse: 6.0292 - val_loss: 13.3259 - val_rmse: 3.4542\n",
      "Epoch 38/1000\n",
      "7329/7329 [==============================] - 5s 729us/sample - loss: 38.0923 - rmse: 6.0116 - val_loss: 13.1964 - val_rmse: 3.4363\n",
      "Epoch 39/1000\n",
      "7329/7329 [==============================] - 5s 744us/sample - loss: 37.6146 - rmse: 5.9760 - val_loss: 12.9832 - val_rmse: 3.4083\n",
      "Epoch 40/1000\n",
      "7329/7329 [==============================] - 5s 743us/sample - loss: 37.4834 - rmse: 5.9628 - val_loss: 12.8162 - val_rmse: 3.3882\n",
      "Epoch 41/1000\n",
      "7329/7329 [==============================] - 5s 744us/sample - loss: 37.3452 - rmse: 5.9518 - val_loss: 12.5794 - val_rmse: 3.3561\n",
      "Epoch 42/1000\n",
      "7329/7329 [==============================] - 5s 732us/sample - loss: 37.1895 - rmse: 5.9348 - val_loss: 12.6088 - val_rmse: 3.3600\n",
      "Epoch 43/1000\n",
      "7329/7329 [==============================] - 5s 727us/sample - loss: 36.9462 - rmse: 5.9148 - val_loss: 12.6275 - val_rmse: 3.3634\n",
      "Epoch 44/1000\n",
      "7329/7329 [==============================] - 6s 755us/sample - loss: 37.1059 - rmse: 5.9267 - val_loss: 12.4957 - val_rmse: 3.3440\n",
      "Epoch 45/1000\n",
      "7329/7329 [==============================] - 5s 742us/sample - loss: 36.8179 - rmse: 5.9026 - val_loss: 12.4126 - val_rmse: 3.3352\n",
      "Epoch 46/1000\n",
      "7329/7329 [==============================] - 6s 754us/sample - loss: 36.4790 - rmse: 5.8748 - val_loss: 12.3950 - val_rmse: 3.3294\n",
      "Epoch 47/1000\n",
      "7329/7329 [==============================] - 5s 744us/sample - loss: 36.2842 - rmse: 5.8571 - val_loss: 12.1129 - val_rmse: 3.2938\n",
      "Epoch 48/1000\n",
      "7329/7329 [==============================] - 5s 732us/sample - loss: 36.3924 - rmse: 5.8660 - val_loss: 12.1283 - val_rmse: 3.2968\n",
      "Epoch 49/1000\n",
      "7329/7329 [==============================] - 5s 738us/sample - loss: 35.7435 - rmse: 5.8180 - val_loss: 12.2834 - val_rmse: 3.3165\n",
      "Epoch 50/1000\n",
      "7329/7329 [==============================] - 5s 727us/sample - loss: 35.6476 - rmse: 5.8062 - val_loss: 12.1443 - val_rmse: 3.2968\n",
      "Epoch 51/1000\n",
      "7329/7329 [==============================] - 5s 744us/sample - loss: 35.5049 - rmse: 5.7944 - val_loss: 12.0364 - val_rmse: 3.2806\n",
      "Epoch 52/1000\n",
      "7329/7329 [==============================] - 5s 746us/sample - loss: 35.3249 - rmse: 5.7788 - val_loss: 12.0207 - val_rmse: 3.2789\n",
      "Epoch 53/1000\n",
      "7329/7329 [==============================] - 5s 746us/sample - loss: 35.2835 - rmse: 5.7765 - val_loss: 11.9568 - val_rmse: 3.2714\n",
      "Epoch 54/1000\n",
      "7329/7329 [==============================] - 5s 745us/sample - loss: 35.1704 - rmse: 5.7666 - val_loss: 11.9076 - val_rmse: 3.2641\n",
      "Epoch 55/1000\n",
      "7329/7329 [==============================] - 5s 727us/sample - loss: 35.0276 - rmse: 5.7508 - val_loss: 11.9689 - val_rmse: 3.2719\n",
      "Epoch 56/1000\n",
      "7329/7329 [==============================] - 5s 730us/sample - loss: 34.5861 - rmse: 5.7166 - val_loss: 11.9518 - val_rmse: 3.2702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/1000\n",
      "7329/7329 [==============================] - 5s 744us/sample - loss: 34.4765 - rmse: 5.7053 - val_loss: 11.8499 - val_rmse: 3.2537\n",
      "Epoch 58/1000\n",
      "7329/7329 [==============================] - 5s 744us/sample - loss: 34.5173 - rmse: 5.7095 - val_loss: 11.7431 - val_rmse: 3.2411\n",
      "Epoch 59/1000\n",
      "7329/7329 [==============================] - 5s 745us/sample - loss: 34.1198 - rmse: 5.6766 - val_loss: 11.7307 - val_rmse: 3.2385\n",
      "Epoch 60/1000\n",
      "7329/7329 [==============================] - 5s 750us/sample - loss: 33.9526 - rmse: 5.6625 - val_loss: 11.7162 - val_rmse: 3.2359\n",
      "Epoch 61/1000\n",
      "7329/7329 [==============================] - 5s 748us/sample - loss: 33.9819 - rmse: 5.6646 - val_loss: 11.6828 - val_rmse: 3.2299\n",
      "Epoch 62/1000\n",
      "7329/7329 [==============================] - 5s 727us/sample - loss: 33.8219 - rmse: 5.6533 - val_loss: 11.7425 - val_rmse: 3.2384\n",
      "Epoch 63/1000\n",
      "7329/7329 [==============================] - 5s 741us/sample - loss: 33.5163 - rmse: 5.6247 - val_loss: 11.6048 - val_rmse: 3.2197\n",
      "Epoch 64/1000\n",
      "7329/7329 [==============================] - 5s 746us/sample - loss: 33.3150 - rmse: 5.6054 - val_loss: 11.5249 - val_rmse: 3.2076\n",
      "Epoch 65/1000\n",
      "7329/7329 [==============================] - 5s 742us/sample - loss: 33.2230 - rmse: 5.6019 - val_loss: 11.4957 - val_rmse: 3.2032\n",
      "Epoch 66/1000\n",
      "7329/7329 [==============================] - 5s 745us/sample - loss: 32.9222 - rmse: 5.5712 - val_loss: 11.4326 - val_rmse: 3.1935\n",
      "Epoch 67/1000\n",
      "7329/7329 [==============================] - 5s 744us/sample - loss: 32.7921 - rmse: 5.5603 - val_loss: 11.2345 - val_rmse: 3.1654\n",
      "Epoch 68/1000\n",
      "7329/7329 [==============================] - 5s 738us/sample - loss: 32.7031 - rmse: 5.5536 - val_loss: 11.3692 - val_rmse: 3.1853\n",
      "Epoch 69/1000\n",
      "7329/7329 [==============================] - 5s 727us/sample - loss: 32.3397 - rmse: 5.5179 - val_loss: 11.3397 - val_rmse: 3.1813\n",
      "Epoch 70/1000\n",
      "7329/7329 [==============================] - 5s 742us/sample - loss: 32.3400 - rmse: 5.5225 - val_loss: 11.1840 - val_rmse: 3.1574\n",
      "Epoch 71/1000\n",
      "7329/7329 [==============================] - 5s 730us/sample - loss: 32.1820 - rmse: 5.5079 - val_loss: 11.2478 - val_rmse: 3.1647\n",
      "Epoch 72/1000\n",
      "7329/7329 [==============================] - 5s 742us/sample - loss: 31.7427 - rmse: 5.4669 - val_loss: 11.0952 - val_rmse: 3.1433\n",
      "Epoch 73/1000\n",
      "7329/7329 [==============================] - 5s 746us/sample - loss: 31.7967 - rmse: 5.4702 - val_loss: 10.9828 - val_rmse: 3.1259\n",
      "Epoch 74/1000\n",
      "7329/7329 [==============================] - 5s 730us/sample - loss: 31.4581 - rmse: 5.4438 - val_loss: 11.0529 - val_rmse: 3.1360\n",
      "Epoch 75/1000\n",
      "7329/7329 [==============================] - 5s 729us/sample - loss: 31.0790 - rmse: 5.4093 - val_loss: 11.0816 - val_rmse: 3.1409\n",
      "Epoch 76/1000\n",
      "7329/7329 [==============================] - 5s 744us/sample - loss: 30.9714 - rmse: 5.3984 - val_loss: 10.8987 - val_rmse: 3.1125\n",
      "Epoch 77/1000\n",
      "7329/7329 [==============================] - 5s 742us/sample - loss: 30.8202 - rmse: 5.3829 - val_loss: 10.7912 - val_rmse: 3.0976\n",
      "Epoch 78/1000\n",
      "7329/7329 [==============================] - 5s 747us/sample - loss: 30.4419 - rmse: 5.3531 - val_loss: 10.7528 - val_rmse: 3.0908\n",
      "Epoch 79/1000\n",
      "7329/7329 [==============================] - 6s 764us/sample - loss: 30.5037 - rmse: 5.3550 - val_loss: 10.7930 - val_rmse: 3.0951\n",
      "Epoch 80/1000\n",
      "7329/7329 [==============================] - 6s 799us/sample - loss: 30.1644 - rmse: 5.3252 - val_loss: 10.8956 - val_rmse: 3.1122\n",
      "Epoch 81/1000\n",
      "7329/7329 [==============================] - 5s 746us/sample - loss: 30.0492 - rmse: 5.3120 - val_loss: 10.6709 - val_rmse: 3.0781\n",
      "Epoch 82/1000\n",
      "7329/7329 [==============================] - 5s 730us/sample - loss: 29.6003 - rmse: 5.2728 - val_loss: 10.7625 - val_rmse: 3.0919\n",
      "Epoch 83/1000\n",
      "7329/7329 [==============================] - 5s 728us/sample - loss: 29.2396 - rmse: 5.2461 - val_loss: 10.3846 - val_rmse: 3.0354\n",
      "Epoch 84/1000\n",
      "7329/7329 [==============================] - 5s 731us/sample - loss: 29.2176 - rmse: 5.2354 - val_loss: 10.3352 - val_rmse: 3.0288\n",
      "Epoch 85/1000\n",
      "7329/7329 [==============================] - 5s 731us/sample - loss: 28.7759 - rmse: 5.1981 - val_loss: 10.2113 - val_rmse: 3.0114\n",
      "Epoch 86/1000\n",
      "7329/7329 [==============================] - 5s 732us/sample - loss: 28.6515 - rmse: 5.1837 - val_loss: 10.1811 - val_rmse: 3.0043\n",
      "Epoch 87/1000\n",
      "7329/7329 [==============================] - 5s 714us/sample - loss: 28.3067 - rmse: 5.1574 - val_loss: 10.1636 - val_rmse: 3.0045\n",
      "Epoch 88/1000\n",
      "7329/7329 [==============================] - 5s 729us/sample - loss: 28.0119 - rmse: 5.1264 - val_loss: 10.1253 - val_rmse: 2.9994\n",
      "Epoch 89/1000\n",
      "7329/7329 [==============================] - 5s 729us/sample - loss: 27.7662 - rmse: 5.1086 - val_loss: 9.8068 - val_rmse: 2.9446\n",
      "Epoch 90/1000\n",
      "7329/7329 [==============================] - 5s 718us/sample - loss: 27.4661 - rmse: 5.0778 - val_loss: 9.8443 - val_rmse: 2.9541\n",
      "Epoch 91/1000\n",
      "7329/7329 [==============================] - 5s 733us/sample - loss: 27.1137 - rmse: 5.0468 - val_loss: 9.2813 - val_rmse: 2.8645\n",
      "Epoch 92/1000\n",
      "7329/7329 [==============================] - 5s 735us/sample - loss: 26.9346 - rmse: 5.0307 - val_loss: 9.0602 - val_rmse: 2.8280\n",
      "Epoch 93/1000\n",
      "7329/7329 [==============================] - 5s 733us/sample - loss: 26.8051 - rmse: 5.0136 - val_loss: 8.9283 - val_rmse: 2.8071\n",
      "Epoch 94/1000\n",
      "7329/7329 [==============================] - 5s 716us/sample - loss: 26.5270 - rmse: 4.9925 - val_loss: 8.9692 - val_rmse: 2.8185\n",
      "Epoch 95/1000\n",
      "7329/7329 [==============================] - 5s 731us/sample - loss: 26.4360 - rmse: 4.9804 - val_loss: 8.6566 - val_rmse: 2.7595\n",
      "Epoch 96/1000\n",
      "7329/7329 [==============================] - 5s 731us/sample - loss: 26.1204 - rmse: 4.9547 - val_loss: 8.2623 - val_rmse: 2.6928\n",
      "Epoch 97/1000\n",
      "7329/7329 [==============================] - 5s 719us/sample - loss: 25.9022 - rmse: 4.9354 - val_loss: 8.3673 - val_rmse: 2.7148\n",
      "Epoch 98/1000\n",
      "7329/7329 [==============================] - 5s 731us/sample - loss: 25.9518 - rmse: 4.9370 - val_loss: 8.1780 - val_rmse: 2.6858\n",
      "Epoch 99/1000\n",
      "7329/7329 [==============================] - 5s 731us/sample - loss: 25.4221 - rmse: 4.8902 - val_loss: 7.9065 - val_rmse: 2.6381\n",
      "Epoch 100/1000\n",
      "7329/7329 [==============================] - 5s 714us/sample - loss: 25.4581 - rmse: 4.8968 - val_loss: 7.8952 - val_rmse: 2.6384\n",
      "Epoch 101/1000\n",
      "7329/7329 [==============================] - 5s 729us/sample - loss: 25.3507 - rmse: 4.8830 - val_loss: 7.7576 - val_rmse: 2.6148\n",
      "Epoch 102/1000\n",
      "7329/7329 [==============================] - 5s 731us/sample - loss: 25.0696 - rmse: 4.8564 - val_loss: 7.7515 - val_rmse: 2.6135\n",
      "Epoch 103/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 24.8042 - rmse: 4.8304 - val_loss: 7.8534 - val_rmse: 2.6331\n",
      "Epoch 104/1000\n",
      "7329/7329 [==============================] - 5s 737us/sample - loss: 24.6268 - rmse: 4.8087 - val_loss: 7.6987 - val_rmse: 2.6059\n",
      "Epoch 105/1000\n",
      "7329/7329 [==============================] - 5s 735us/sample - loss: 24.5538 - rmse: 4.8060 - val_loss: 7.5410 - val_rmse: 2.5774\n",
      "Epoch 106/1000\n",
      "7329/7329 [==============================] - 5s 732us/sample - loss: 24.3275 - rmse: 4.7858 - val_loss: 7.4514 - val_rmse: 2.5624\n",
      "Epoch 107/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 23.9793 - rmse: 4.7491 - val_loss: 7.4731 - val_rmse: 2.5652\n",
      "Epoch 108/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 24.1551 - rmse: 4.7658 - val_loss: 7.5441 - val_rmse: 2.5823\n",
      "Epoch 109/1000\n",
      "7329/7329 [==============================] - 5s 729us/sample - loss: 24.0606 - rmse: 4.7584 - val_loss: 7.2528 - val_rmse: 2.5284\n",
      "Epoch 110/1000\n",
      "7329/7329 [==============================] - 5s 714us/sample - loss: 23.6430 - rmse: 4.7185 - val_loss: 7.3750 - val_rmse: 2.5532\n",
      "Epoch 111/1000\n",
      "7329/7329 [==============================] - 5s 729us/sample - loss: 23.5488 - rmse: 4.7087 - val_loss: 7.1716 - val_rmse: 2.5144\n",
      "Epoch 112/1000\n",
      "7329/7329 [==============================] - 5s 731us/sample - loss: 23.3171 - rmse: 4.6840 - val_loss: 6.9841 - val_rmse: 2.4770\n",
      "Epoch 113/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7329/7329 [==============================] - 5s 719us/sample - loss: 23.2870 - rmse: 4.6800 - val_loss: 7.0146 - val_rmse: 2.4843\n",
      "Epoch 114/1000\n",
      "7329/7329 [==============================] - 5s 711us/sample - loss: 23.1511 - rmse: 4.6707 - val_loss: 7.1024 - val_rmse: 2.5056\n",
      "Epoch 115/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 22.6989 - rmse: 4.6228 - val_loss: 6.9988 - val_rmse: 2.4848\n",
      "Epoch 116/1000\n",
      "7329/7329 [==============================] - 5s 726us/sample - loss: 22.8171 - rmse: 4.6353 - val_loss: 6.8524 - val_rmse: 2.4563\n",
      "Epoch 117/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 22.6186 - rmse: 4.6165 - val_loss: 6.9827 - val_rmse: 2.4842\n",
      "Epoch 118/1000\n",
      "7329/7329 [==============================] - 5s 711us/sample - loss: 22.5538 - rmse: 4.6097 - val_loss: 6.8955 - val_rmse: 2.4678\n",
      "Epoch 119/1000\n",
      "7329/7329 [==============================] - 5s 729us/sample - loss: 22.2975 - rmse: 4.5870 - val_loss: 6.7316 - val_rmse: 2.4316\n",
      "Epoch 120/1000\n",
      "7329/7329 [==============================] - 5s 716us/sample - loss: 22.2039 - rmse: 4.5734 - val_loss: 6.7895 - val_rmse: 2.4470\n",
      "Epoch 121/1000\n",
      "7329/7329 [==============================] - 5s 727us/sample - loss: 21.9755 - rmse: 4.5497 - val_loss: 6.5837 - val_rmse: 2.4091\n",
      "Epoch 122/1000\n",
      "7329/7329 [==============================] - 5s 715us/sample - loss: 21.7158 - rmse: 4.5283 - val_loss: 6.6056 - val_rmse: 2.4153\n",
      "Epoch 123/1000\n",
      "7329/7329 [==============================] - 5s 710us/sample - loss: 21.7460 - rmse: 4.5322 - val_loss: 6.5924 - val_rmse: 2.4095\n",
      "Epoch 124/1000\n",
      "7329/7329 [==============================] - 5s 711us/sample - loss: 21.4837 - rmse: 4.4982 - val_loss: 6.6114 - val_rmse: 2.4134\n",
      "Epoch 125/1000\n",
      "7329/7329 [==============================] - 5s 728us/sample - loss: 21.4742 - rmse: 4.5011 - val_loss: 6.4703 - val_rmse: 2.3900\n",
      "Epoch 126/1000\n",
      "7329/7329 [==============================] - 5s 730us/sample - loss: 21.1656 - rmse: 4.4687 - val_loss: 6.2424 - val_rmse: 2.3449\n",
      "Epoch 127/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 21.0771 - rmse: 4.4579 - val_loss: 6.3920 - val_rmse: 2.3734\n",
      "Epoch 128/1000\n",
      "7329/7329 [==============================] - 5s 711us/sample - loss: 21.0825 - rmse: 4.4660 - val_loss: 6.3924 - val_rmse: 2.3717\n",
      "Epoch 129/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 21.0330 - rmse: 4.4525 - val_loss: 6.3078 - val_rmse: 2.3514\n",
      "Epoch 130/1000\n",
      "7329/7329 [==============================] - 5s 714us/sample - loss: 20.7486 - rmse: 4.4244 - val_loss: 6.3704 - val_rmse: 2.3628\n",
      "Epoch 131/1000\n",
      "7329/7329 [==============================] - 5s 734us/sample - loss: 20.5722 - rmse: 4.4101 - val_loss: 6.2561 - val_rmse: 2.3410\n",
      "Epoch 132/1000\n",
      "7329/7329 [==============================] - 5s 732us/sample - loss: 20.3974 - rmse: 4.3931 - val_loss: 6.0413 - val_rmse: 2.2996\n",
      "Epoch 133/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 20.3255 - rmse: 4.3835 - val_loss: 6.1184 - val_rmse: 2.3140\n",
      "Epoch 134/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 20.1018 - rmse: 4.3592 - val_loss: 6.2499 - val_rmse: 2.3398\n",
      "Epoch 135/1000\n",
      "7329/7329 [==============================] - 5s 730us/sample - loss: 19.8758 - rmse: 4.3369 - val_loss: 5.8503 - val_rmse: 2.2626\n",
      "Epoch 136/1000\n",
      "7329/7329 [==============================] - 5s 721us/sample - loss: 19.8886 - rmse: 4.3374 - val_loss: 5.9560 - val_rmse: 2.2798\n",
      "Epoch 137/1000\n",
      "7329/7329 [==============================] - 5s 729us/sample - loss: 19.7881 - rmse: 4.3285 - val_loss: 5.7902 - val_rmse: 2.2454\n",
      "Epoch 138/1000\n",
      "7329/7329 [==============================] - 5s 719us/sample - loss: 19.6450 - rmse: 4.3112 - val_loss: 5.8732 - val_rmse: 2.2658\n",
      "Epoch 139/1000\n",
      "7329/7329 [==============================] - 5s 729us/sample - loss: 19.4533 - rmse: 4.2910 - val_loss: 5.6641 - val_rmse: 2.2191\n",
      "Epoch 140/1000\n",
      "7329/7329 [==============================] - 5s 714us/sample - loss: 19.3356 - rmse: 4.2743 - val_loss: 5.8601 - val_rmse: 2.2589\n",
      "Epoch 141/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 19.2107 - rmse: 4.2639 - val_loss: 5.7870 - val_rmse: 2.2441\n",
      "Epoch 142/1000\n",
      "7329/7329 [==============================] - 5s 727us/sample - loss: 19.2565 - rmse: 4.2678 - val_loss: 5.6367 - val_rmse: 2.2136\n",
      "Epoch 143/1000\n",
      "7329/7329 [==============================] - 5s 730us/sample - loss: 18.8714 - rmse: 4.2304 - val_loss: 5.5256 - val_rmse: 2.1911\n",
      "Epoch 144/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 18.9398 - rmse: 4.2334 - val_loss: 5.7405 - val_rmse: 2.2329\n",
      "Epoch 145/1000\n",
      "7329/7329 [==============================] - 5s 710us/sample - loss: 18.8161 - rmse: 4.2210 - val_loss: 5.5843 - val_rmse: 2.2033\n",
      "Epoch 146/1000\n",
      "7329/7329 [==============================] - 5s 731us/sample - loss: 18.6218 - rmse: 4.1979 - val_loss: 5.5210 - val_rmse: 2.1900\n",
      "Epoch 147/1000\n",
      "7329/7329 [==============================] - 5s 711us/sample - loss: 18.4827 - rmse: 4.1882 - val_loss: 5.8764 - val_rmse: 2.2605\n",
      "Epoch 148/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 18.3164 - rmse: 4.1659 - val_loss: 5.5163 - val_rmse: 2.1923\n",
      "Epoch 149/1000\n",
      "7329/7329 [==============================] - 5s 730us/sample - loss: 18.1685 - rmse: 4.1461 - val_loss: 5.3565 - val_rmse: 2.1618\n",
      "Epoch 150/1000\n",
      "7329/7329 [==============================] - 5s 732us/sample - loss: 18.1660 - rmse: 4.1456 - val_loss: 5.3156 - val_rmse: 2.1423\n",
      "Epoch 151/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 17.8895 - rmse: 4.1203 - val_loss: 5.4350 - val_rmse: 2.1675\n",
      "Epoch 152/1000\n",
      "7329/7329 [==============================] - 5s 730us/sample - loss: 18.0361 - rmse: 4.1351 - val_loss: 5.2912 - val_rmse: 2.1419\n",
      "Epoch 153/1000\n",
      "7329/7329 [==============================] - 5s 718us/sample - loss: 17.9287 - rmse: 4.1195 - val_loss: 5.5546 - val_rmse: 2.1929\n",
      "Epoch 154/1000\n",
      "7329/7329 [==============================] - 5s 714us/sample - loss: 17.7747 - rmse: 4.1033 - val_loss: 5.3198 - val_rmse: 2.1454\n",
      "Epoch 155/1000\n",
      "7329/7329 [==============================] - 5s 730us/sample - loss: 17.6837 - rmse: 4.0909 - val_loss: 5.2819 - val_rmse: 2.1365\n",
      "Epoch 156/1000\n",
      "7329/7329 [==============================] - 5s 732us/sample - loss: 17.6884 - rmse: 4.0939 - val_loss: 4.9007 - val_rmse: 2.0564\n",
      "Epoch 157/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 17.4526 - rmse: 4.0693 - val_loss: 4.9391 - val_rmse: 2.0637\n",
      "Epoch 158/1000\n",
      "7329/7329 [==============================] - 5s 714us/sample - loss: 17.2851 - rmse: 4.0489 - val_loss: 5.0602 - val_rmse: 2.0883\n",
      "Epoch 159/1000\n",
      "7329/7329 [==============================] - 5s 717us/sample - loss: 17.2952 - rmse: 4.0499 - val_loss: 4.9339 - val_rmse: 2.0602\n",
      "Epoch 160/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 16.9926 - rmse: 4.0127 - val_loss: 5.1963 - val_rmse: 2.1141\n",
      "Epoch 161/1000\n",
      "7329/7329 [==============================] - 5s 719us/sample - loss: 17.1136 - rmse: 4.0271 - val_loss: 4.9282 - val_rmse: 2.0658\n",
      "Epoch 162/1000\n",
      "7329/7329 [==============================] - 5s 727us/sample - loss: 16.9463 - rmse: 4.0093 - val_loss: 4.8905 - val_rmse: 2.0507\n",
      "Epoch 163/1000\n",
      "7329/7329 [==============================] - 5s 731us/sample - loss: 16.7314 - rmse: 3.9837 - val_loss: 4.8415 - val_rmse: 2.0389\n",
      "Epoch 164/1000\n",
      "7329/7329 [==============================] - 5s 715us/sample - loss: 16.7115 - rmse: 3.9821 - val_loss: 5.0501 - val_rmse: 2.0845\n",
      "Epoch 165/1000\n",
      "7329/7329 [==============================] - 5s 714us/sample - loss: 16.5202 - rmse: 3.9612 - val_loss: 4.8749 - val_rmse: 2.0512\n",
      "Epoch 166/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 16.4635 - rmse: 3.9553 - val_loss: 4.8718 - val_rmse: 2.0432\n",
      "Epoch 167/1000\n",
      "7329/7329 [==============================] - 5s 728us/sample - loss: 16.3815 - rmse: 3.9468 - val_loss: 4.7861 - val_rmse: 2.0263\n",
      "Epoch 168/1000\n",
      "7329/7329 [==============================] - 5s 730us/sample - loss: 16.1971 - rmse: 3.9205 - val_loss: 4.7747 - val_rmse: 2.0252\n",
      "Epoch 169/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7329/7329 [==============================] - 5s 714us/sample - loss: 16.1100 - rmse: 3.9119 - val_loss: 4.8692 - val_rmse: 2.0448\n",
      "Epoch 170/1000\n",
      "7329/7329 [==============================] - 5s 726us/sample - loss: 16.0177 - rmse: 3.8969 - val_loss: 4.7562 - val_rmse: 2.0218\n",
      "Epoch 171/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 15.9239 - rmse: 3.8909 - val_loss: 5.0731 - val_rmse: 2.0891\n",
      "Epoch 172/1000\n",
      "7329/7329 [==============================] - 5s 729us/sample - loss: 15.8477 - rmse: 3.8814 - val_loss: 4.5771 - val_rmse: 1.9823\n",
      "Epoch 173/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 15.8505 - rmse: 3.8846 - val_loss: 4.7182 - val_rmse: 2.0090\n",
      "Epoch 174/1000\n",
      "7329/7329 [==============================] - 5s 727us/sample - loss: 15.7613 - rmse: 3.8680 - val_loss: 4.5230 - val_rmse: 1.9679\n",
      "Epoch 175/1000\n",
      "7329/7329 [==============================] - 5s 711us/sample - loss: 15.6252 - rmse: 3.8513 - val_loss: 4.7800 - val_rmse: 2.0238\n",
      "Epoch 176/1000\n",
      "7329/7329 [==============================] - 5s 710us/sample - loss: 15.4121 - rmse: 3.8282 - val_loss: 4.7952 - val_rmse: 2.0302\n",
      "Epoch 177/1000\n",
      "7329/7329 [==============================] - 5s 711us/sample - loss: 15.4706 - rmse: 3.8341 - val_loss: 4.4702 - val_rmse: 1.9689\n",
      "Epoch 178/1000\n",
      "7329/7329 [==============================] - 5s 711us/sample - loss: 15.4467 - rmse: 3.8344 - val_loss: 4.5867 - val_rmse: 1.9800\n",
      "Epoch 179/1000\n",
      "7329/7329 [==============================] - 5s 710us/sample - loss: 15.3713 - rmse: 3.8226 - val_loss: 4.6254 - val_rmse: 1.9899\n",
      "Epoch 180/1000\n",
      "7329/7329 [==============================] - 5s 709us/sample - loss: 15.1351 - rmse: 3.7954 - val_loss: 4.5624 - val_rmse: 1.9794\n",
      "Epoch 181/1000\n",
      "7329/7329 [==============================] - 5s 719us/sample - loss: 15.0448 - rmse: 3.7848 - val_loss: 4.6123 - val_rmse: 1.9873\n",
      "Epoch 182/1000\n",
      "7329/7329 [==============================] - 5s 715us/sample - loss: 14.9623 - rmse: 3.7711 - val_loss: 4.8773 - val_rmse: 2.0445\n",
      "Epoch 183/1000\n",
      "7329/7329 [==============================] - 5s 728us/sample - loss: 14.9262 - rmse: 3.7674 - val_loss: 4.5277 - val_rmse: 1.9675\n",
      "Epoch 184/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 14.8770 - rmse: 3.7620 - val_loss: 4.5938 - val_rmse: 1.9835\n",
      "Epoch 185/1000\n",
      "7329/7329 [==============================] - 5s 727us/sample - loss: 14.8769 - rmse: 3.7606 - val_loss: 4.4369 - val_rmse: 1.9515\n",
      "Epoch 186/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 14.6930 - rmse: 3.7395 - val_loss: 5.0614 - val_rmse: 2.0914\n",
      "Epoch 187/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 14.6094 - rmse: 3.7265 - val_loss: 4.6328 - val_rmse: 1.9906\n",
      "Epoch 188/1000\n",
      "7329/7329 [==============================] - 5s 732us/sample - loss: 14.5289 - rmse: 3.7197 - val_loss: 4.4327 - val_rmse: 1.9450\n",
      "Epoch 189/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 14.4400 - rmse: 3.7054 - val_loss: 4.6778 - val_rmse: 2.0004\n",
      "Epoch 190/1000\n",
      "7329/7329 [==============================] - 5s 711us/sample - loss: 14.4196 - rmse: 3.7034 - val_loss: 4.4709 - val_rmse: 1.9501\n",
      "Epoch 191/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 14.2665 - rmse: 3.6872 - val_loss: 4.5340 - val_rmse: 1.9676\n",
      "Epoch 192/1000\n",
      "7329/7329 [==============================] - 5s 728us/sample - loss: 14.1311 - rmse: 3.6677 - val_loss: 4.2863 - val_rmse: 1.9124\n",
      "Epoch 193/1000\n",
      "7329/7329 [==============================] - 5s 727us/sample - loss: 14.0350 - rmse: 3.6567 - val_loss: 4.2536 - val_rmse: 1.9095\n",
      "Epoch 194/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 14.1485 - rmse: 3.6701 - val_loss: 4.4283 - val_rmse: 1.9417\n",
      "Epoch 195/1000\n",
      "7329/7329 [==============================] - 5s 730us/sample - loss: 13.9681 - rmse: 3.6463 - val_loss: 4.2688 - val_rmse: 1.9078\n",
      "Epoch 196/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 13.8580 - rmse: 3.6365 - val_loss: 4.5132 - val_rmse: 1.9638\n",
      "Epoch 197/1000\n",
      "7329/7329 [==============================] - 5s 711us/sample - loss: 13.9235 - rmse: 3.6429 - val_loss: 4.5513 - val_rmse: 1.9709\n",
      "Epoch 198/1000\n",
      "7329/7329 [==============================] - 5s 728us/sample - loss: 13.6969 - rmse: 3.6147 - val_loss: 4.1552 - val_rmse: 1.8871\n",
      "Epoch 199/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 13.6640 - rmse: 3.6067 - val_loss: 4.6176 - val_rmse: 1.9901\n",
      "Epoch 200/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 13.5312 - rmse: 3.5933 - val_loss: 4.3951 - val_rmse: 1.9350\n",
      "Epoch 201/1000\n",
      "7329/7329 [==============================] - 5s 709us/sample - loss: 13.4737 - rmse: 3.5848 - val_loss: 4.3386 - val_rmse: 1.9237\n",
      "Epoch 202/1000\n",
      "7329/7329 [==============================] - 5s 710us/sample - loss: 13.5082 - rmse: 3.5882 - val_loss: 4.3141 - val_rmse: 1.9174\n",
      "Epoch 203/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 13.3538 - rmse: 3.5694 - val_loss: 4.2427 - val_rmse: 1.9036\n",
      "Epoch 204/1000\n",
      "7329/7329 [==============================] - 5s 719us/sample - loss: 13.2411 - rmse: 3.5533 - val_loss: 4.3466 - val_rmse: 1.9265\n",
      "Epoch 205/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 13.2239 - rmse: 3.5550 - val_loss: 4.2949 - val_rmse: 1.9122\n",
      "Epoch 206/1000\n",
      "7329/7329 [==============================] - 5s 727us/sample - loss: 13.0657 - rmse: 3.5300 - val_loss: 4.1191 - val_rmse: 1.8780\n",
      "Epoch 207/1000\n",
      "7329/7329 [==============================] - 5s 715us/sample - loss: 13.0977 - rmse: 3.5360 - val_loss: 4.2418 - val_rmse: 1.9046\n",
      "Epoch 208/1000\n",
      "7329/7329 [==============================] - 5s 730us/sample - loss: 12.9932 - rmse: 3.5206 - val_loss: 4.0906 - val_rmse: 1.8708\n",
      "Epoch 209/1000\n",
      "7329/7329 [==============================] - 5s 730us/sample - loss: 12.8425 - rmse: 3.5015 - val_loss: 3.9626 - val_rmse: 1.8395\n",
      "Epoch 210/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 12.7430 - rmse: 3.4876 - val_loss: 4.1629 - val_rmse: 1.8832\n",
      "Epoch 211/1000\n",
      "7329/7329 [==============================] - 5s 714us/sample - loss: 12.7572 - rmse: 3.4890 - val_loss: 4.1666 - val_rmse: 1.8841\n",
      "Epoch 212/1000\n",
      "7329/7329 [==============================] - 5s 711us/sample - loss: 12.7217 - rmse: 3.4859 - val_loss: 4.0656 - val_rmse: 1.8606\n",
      "Epoch 213/1000\n",
      "7329/7329 [==============================] - 5s 711us/sample - loss: 12.5915 - rmse: 3.4674 - val_loss: 4.0952 - val_rmse: 1.8697\n",
      "Epoch 214/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 12.6758 - rmse: 3.4792 - val_loss: 3.9716 - val_rmse: 1.8476\n",
      "Epoch 215/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 12.5853 - rmse: 3.4648 - val_loss: 4.1993 - val_rmse: 1.8905\n",
      "Epoch 216/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 12.5065 - rmse: 3.4567 - val_loss: 4.0463 - val_rmse: 1.8558\n",
      "Epoch 217/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 12.3444 - rmse: 3.4317 - val_loss: 4.4092 - val_rmse: 1.9438\n",
      "Epoch 218/1000\n",
      "7329/7329 [==============================] - 5s 721us/sample - loss: 12.4153 - rmse: 3.4449 - val_loss: 4.4165 - val_rmse: 1.9432\n",
      "Epoch 219/1000\n",
      "7329/7329 [==============================] - 5s 729us/sample - loss: 12.1433 - rmse: 3.4066 - val_loss: 3.9921 - val_rmse: 1.8394\n",
      "Epoch 220/1000\n",
      "7329/7329 [==============================] - 5s 714us/sample - loss: 12.1851 - rmse: 3.4120 - val_loss: 4.0126 - val_rmse: 1.8483\n",
      "Epoch 221/1000\n",
      "7329/7329 [==============================] - 5s 729us/sample - loss: 12.1947 - rmse: 3.4113 - val_loss: 3.9793 - val_rmse: 1.8372\n",
      "Epoch 222/1000\n",
      "7329/7329 [==============================] - 5s 729us/sample - loss: 12.0111 - rmse: 3.3854 - val_loss: 3.9656 - val_rmse: 1.8349\n",
      "Epoch 223/1000\n",
      "7329/7329 [==============================] - 5s 714us/sample - loss: 12.0732 - rmse: 3.3935 - val_loss: 3.9744 - val_rmse: 1.8363\n",
      "Epoch 224/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 12.0237 - rmse: 3.3889 - val_loss: 4.0180 - val_rmse: 1.8504\n",
      "Epoch 225/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7329/7329 [==============================] - 5s 710us/sample - loss: 11.8723 - rmse: 3.3666 - val_loss: 4.0564 - val_rmse: 1.8575\n",
      "Epoch 226/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 11.8193 - rmse: 3.3591 - val_loss: 4.2048 - val_rmse: 1.8921\n",
      "Epoch 227/1000\n",
      "7329/7329 [==============================] - 5s 716us/sample - loss: 11.6767 - rmse: 3.3369 - val_loss: 4.0797 - val_rmse: 1.8625\n",
      "Epoch 228/1000\n",
      "7329/7329 [==============================] - 5s 729us/sample - loss: 11.7023 - rmse: 3.3442 - val_loss: 3.8179 - val_rmse: 1.8010\n",
      "Epoch 229/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 11.6640 - rmse: 3.3368 - val_loss: 4.0408 - val_rmse: 1.8487\n",
      "Epoch 230/1000\n",
      "7329/7329 [==============================] - 5s 715us/sample - loss: 11.5199 - rmse: 3.3189 - val_loss: 3.8859 - val_rmse: 1.8162\n",
      "Epoch 231/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 11.5712 - rmse: 3.3276 - val_loss: 4.2831 - val_rmse: 1.9103\n",
      "Epoch 232/1000\n",
      "7329/7329 [==============================] - 5s 710us/sample - loss: 11.3859 - rmse: 3.2985 - val_loss: 3.8487 - val_rmse: 1.8080\n",
      "Epoch 233/1000\n",
      "7329/7329 [==============================] - 5s 728us/sample - loss: 11.4530 - rmse: 3.3088 - val_loss: 3.7086 - val_rmse: 1.7727\n",
      "Epoch 234/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 11.3842 - rmse: 3.2979 - val_loss: 3.8827 - val_rmse: 1.8143\n",
      "Epoch 235/1000\n",
      "7329/7329 [==============================] - 5s 711us/sample - loss: 11.3167 - rmse: 3.2867 - val_loss: 3.8113 - val_rmse: 1.7977\n",
      "Epoch 236/1000\n",
      "7329/7329 [==============================] - 5s 709us/sample - loss: 11.2348 - rmse: 3.2767 - val_loss: 3.7672 - val_rmse: 1.7862\n",
      "Epoch 237/1000\n",
      "7329/7329 [==============================] - 5s 726us/sample - loss: 11.1904 - rmse: 3.2714 - val_loss: 3.6737 - val_rmse: 1.7686\n",
      "Epoch 238/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 11.1836 - rmse: 3.2717 - val_loss: 3.7967 - val_rmse: 1.7932\n",
      "Epoch 239/1000\n",
      "7329/7329 [==============================] - 5s 715us/sample - loss: 11.0790 - rmse: 3.2529 - val_loss: 4.1780 - val_rmse: 1.8881\n",
      "Epoch 240/1000\n",
      "7329/7329 [==============================] - 5s 711us/sample - loss: 10.9861 - rmse: 3.2409 - val_loss: 3.8495 - val_rmse: 1.8076\n",
      "Epoch 241/1000\n",
      "7329/7329 [==============================] - 5s 730us/sample - loss: 10.8952 - rmse: 3.2262 - val_loss: 3.6559 - val_rmse: 1.7600\n",
      "Epoch 242/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 10.9305 - rmse: 3.2339 - val_loss: 4.1735 - val_rmse: 1.8868\n",
      "Epoch 243/1000\n",
      "7329/7329 [==============================] - 5s 711us/sample - loss: 10.8515 - rmse: 3.2204 - val_loss: 3.8212 - val_rmse: 1.7964\n",
      "Epoch 244/1000\n",
      "7329/7329 [==============================] - 5s 711us/sample - loss: 10.8629 - rmse: 3.2227 - val_loss: 3.8159 - val_rmse: 1.8006\n",
      "Epoch 245/1000\n",
      "7329/7329 [==============================] - 5s 715us/sample - loss: 10.6587 - rmse: 3.1928 - val_loss: 4.0900 - val_rmse: 1.8658\n",
      "Epoch 246/1000\n",
      "7329/7329 [==============================] - 5s 714us/sample - loss: 10.7441 - rmse: 3.2045 - val_loss: 4.0331 - val_rmse: 1.8458\n",
      "Epoch 247/1000\n",
      "7329/7329 [==============================] - 5s 709us/sample - loss: 10.7062 - rmse: 3.2010 - val_loss: 3.8550 - val_rmse: 1.8073\n",
      "Epoch 248/1000\n",
      "7329/7329 [==============================] - 5s 710us/sample - loss: 10.5987 - rmse: 3.1863 - val_loss: 3.9758 - val_rmse: 1.8318\n",
      "Epoch 249/1000\n",
      "7329/7329 [==============================] - 5s 730us/sample - loss: 10.5527 - rmse: 3.1769 - val_loss: 3.6253 - val_rmse: 1.7530\n",
      "Epoch 250/1000\n",
      "7329/7329 [==============================] - 5s 720us/sample - loss: 10.5381 - rmse: 3.1727 - val_loss: 3.6621 - val_rmse: 1.7614\n",
      "Epoch 251/1000\n",
      "7329/7329 [==============================] - 5s 711us/sample - loss: 10.4986 - rmse: 3.1674 - val_loss: 3.9083 - val_rmse: 1.8169\n",
      "Epoch 252/1000\n",
      "7329/7329 [==============================] - 5s 715us/sample - loss: 10.4691 - rmse: 3.1655 - val_loss: 3.8589 - val_rmse: 1.8089\n",
      "Epoch 253/1000\n",
      "7329/7329 [==============================] - 5s 727us/sample - loss: 10.3797 - rmse: 3.1517 - val_loss: 3.6195 - val_rmse: 1.7506\n",
      "Epoch 254/1000\n",
      "7329/7329 [==============================] - 5s 714us/sample - loss: 10.3485 - rmse: 3.1439 - val_loss: 3.8692 - val_rmse: 1.8103\n",
      "Epoch 255/1000\n",
      "7329/7329 [==============================] - 5s 729us/sample - loss: 10.3011 - rmse: 3.1389 - val_loss: 3.6022 - val_rmse: 1.7461\n",
      "Epoch 256/1000\n",
      "7329/7329 [==============================] - 5s 732us/sample - loss: 10.2142 - rmse: 3.1260 - val_loss: 3.5745 - val_rmse: 1.7456\n",
      "Epoch 257/1000\n",
      "7329/7329 [==============================] - 5s 716us/sample - loss: 10.2190 - rmse: 3.1255 - val_loss: 3.6178 - val_rmse: 1.7566\n",
      "Epoch 258/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 10.1108 - rmse: 3.1073 - val_loss: 3.6238 - val_rmse: 1.7563\n",
      "Epoch 259/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 10.0397 - rmse: 3.0975 - val_loss: 3.8634 - val_rmse: 1.8095\n",
      "Epoch 260/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 10.0392 - rmse: 3.0980 - val_loss: 3.7336 - val_rmse: 1.7758\n",
      "Epoch 261/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 9.9488 - rmse: 3.0843 - val_loss: 3.7072 - val_rmse: 1.7692\n",
      "Epoch 262/1000\n",
      "7329/7329 [==============================] - 5s 711us/sample - loss: 9.9802 - rmse: 3.0891 - val_loss: 3.6200 - val_rmse: 1.7481\n",
      "Epoch 263/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 9.9063 - rmse: 3.0772 - val_loss: 3.8245 - val_rmse: 1.7970\n",
      "Epoch 264/1000\n",
      "7329/7329 [==============================] - 5s 715us/sample - loss: 9.8832 - rmse: 3.0739 - val_loss: 3.6766 - val_rmse: 1.7615\n",
      "Epoch 265/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 9.7647 - rmse: 3.0585 - val_loss: 3.6238 - val_rmse: 1.7482\n",
      "Epoch 266/1000\n",
      "7329/7329 [==============================] - 5s 714us/sample - loss: 9.7661 - rmse: 3.0538 - val_loss: 3.6369 - val_rmse: 1.7515\n",
      "Epoch 267/1000\n",
      "7329/7329 [==============================] - 5s 728us/sample - loss: 9.6963 - rmse: 3.0479 - val_loss: 3.4916 - val_rmse: 1.7200\n",
      "Epoch 268/1000\n",
      "7329/7329 [==============================] - 5s 715us/sample - loss: 9.6376 - rmse: 3.0361 - val_loss: 3.4903 - val_rmse: 1.7318\n",
      "Epoch 269/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 9.6782 - rmse: 3.0406 - val_loss: 3.7485 - val_rmse: 1.7805\n",
      "Epoch 270/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 9.5931 - rmse: 3.0277 - val_loss: 3.8867 - val_rmse: 1.8158\n",
      "Epoch 271/1000\n",
      "7329/7329 [==============================] - 5s 731us/sample - loss: 9.6031 - rmse: 3.0291 - val_loss: 3.4759 - val_rmse: 1.7150\n",
      "Epoch 272/1000\n",
      "7329/7329 [==============================] - 5s 733us/sample - loss: 9.4828 - rmse: 3.0106 - val_loss: 3.4322 - val_rmse: 1.7053\n",
      "Epoch 273/1000\n",
      "7329/7329 [==============================] - 5s 718us/sample - loss: 9.4702 - rmse: 3.0092 - val_loss: 3.8436 - val_rmse: 1.8084\n",
      "Epoch 274/1000\n",
      "7329/7329 [==============================] - 5s 729us/sample - loss: 9.4135 - rmse: 2.9988 - val_loss: 3.4357 - val_rmse: 1.7040\n",
      "Epoch 275/1000\n",
      "7329/7329 [==============================] - 5s 715us/sample - loss: 9.3810 - rmse: 2.9959 - val_loss: 3.5008 - val_rmse: 1.7176\n",
      "Epoch 276/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 9.2676 - rmse: 2.9802 - val_loss: 3.7171 - val_rmse: 1.7734\n",
      "Epoch 277/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 9.2763 - rmse: 2.9793 - val_loss: 3.6260 - val_rmse: 1.7490\n",
      "Epoch 278/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 9.2125 - rmse: 2.9678 - val_loss: 3.7030 - val_rmse: 1.7665\n",
      "Epoch 279/1000\n",
      "7329/7329 [==============================] - 5s 716us/sample - loss: 9.2681 - rmse: 2.9761 - val_loss: 3.7042 - val_rmse: 1.7687\n",
      "Epoch 280/1000\n",
      "7329/7329 [==============================] - 5s 716us/sample - loss: 9.1490 - rmse: 2.9588 - val_loss: 3.4841 - val_rmse: 1.7184\n",
      "Epoch 281/1000\n",
      "7329/7329 [==============================] - 5s 714us/sample - loss: 9.1787 - rmse: 2.9589 - val_loss: 3.6747 - val_rmse: 1.7625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 282/1000\n",
      "7329/7329 [==============================] - 5s 711us/sample - loss: 9.1306 - rmse: 2.9529 - val_loss: 3.6935 - val_rmse: 1.7729\n",
      "Epoch 283/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 9.1351 - rmse: 2.9553 - val_loss: 3.4864 - val_rmse: 1.7136\n",
      "Epoch 284/1000\n",
      "7329/7329 [==============================] - 5s 728us/sample - loss: 9.0857 - rmse: 2.9472 - val_loss: 3.4253 - val_rmse: 1.6983\n",
      "Epoch 285/1000\n",
      "7329/7329 [==============================] - 5s 729us/sample - loss: 8.9166 - rmse: 2.9205 - val_loss: 3.4110 - val_rmse: 1.6952\n",
      "Epoch 286/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 8.9255 - rmse: 2.9200 - val_loss: 3.7088 - val_rmse: 1.7662\n",
      "Epoch 287/1000\n",
      "7329/7329 [==============================] - 5s 716us/sample - loss: 8.8617 - rmse: 2.9112 - val_loss: 3.8951 - val_rmse: 1.8177\n",
      "Epoch 288/1000\n",
      "7329/7329 [==============================] - 5s 727us/sample - loss: 8.9114 - rmse: 2.9169 - val_loss: 3.3617 - val_rmse: 1.6880\n",
      "Epoch 289/1000\n",
      "7329/7329 [==============================] - 5s 711us/sample - loss: 8.8284 - rmse: 2.9046 - val_loss: 3.6905 - val_rmse: 1.7598\n",
      "Epoch 290/1000\n",
      "7329/7329 [==============================] - 5s 714us/sample - loss: 8.7964 - rmse: 2.9002 - val_loss: 3.4166 - val_rmse: 1.6937\n",
      "Epoch 291/1000\n",
      "7329/7329 [==============================] - 5s 720us/sample - loss: 8.8306 - rmse: 2.9067 - val_loss: 3.5199 - val_rmse: 1.7200\n",
      "Epoch 292/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 8.7444 - rmse: 2.8904 - val_loss: 3.5150 - val_rmse: 1.7187\n",
      "Epoch 293/1000\n",
      "7329/7329 [==============================] - 5s 726us/sample - loss: 8.7063 - rmse: 2.8856 - val_loss: 3.3757 - val_rmse: 1.6849\n",
      "Epoch 294/1000\n",
      "7329/7329 [==============================] - 5s 729us/sample - loss: 8.6425 - rmse: 2.8764 - val_loss: 3.3237 - val_rmse: 1.6737\n",
      "Epoch 295/1000\n",
      "7329/7329 [==============================] - 5s 719us/sample - loss: 8.5735 - rmse: 2.8623 - val_loss: 3.3627 - val_rmse: 1.6798\n",
      "Epoch 296/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 8.6548 - rmse: 2.8747 - val_loss: 3.4915 - val_rmse: 1.7138\n",
      "Epoch 297/1000\n",
      "7329/7329 [==============================] - 5s 726us/sample - loss: 8.6116 - rmse: 2.8689 - val_loss: 3.3126 - val_rmse: 1.6701\n",
      "Epoch 298/1000\n",
      "7329/7329 [==============================] - 5s 717us/sample - loss: 8.5406 - rmse: 2.8575 - val_loss: 3.3389 - val_rmse: 1.6789\n",
      "Epoch 299/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 8.5142 - rmse: 2.8502 - val_loss: 3.5146 - val_rmse: 1.7167\n",
      "Epoch 300/1000\n",
      "7329/7329 [==============================] - 5s 711us/sample - loss: 8.4935 - rmse: 2.8486 - val_loss: 3.6750 - val_rmse: 1.7595\n",
      "Epoch 301/1000\n",
      "7329/7329 [==============================] - 5s 711us/sample - loss: 8.4500 - rmse: 2.8417 - val_loss: 3.4858 - val_rmse: 1.7102\n",
      "Epoch 302/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 8.3723 - rmse: 2.8296 - val_loss: 3.5490 - val_rmse: 1.7258\n",
      "Epoch 303/1000\n",
      "7329/7329 [==============================] - 5s 716us/sample - loss: 8.3658 - rmse: 2.8279 - val_loss: 3.4599 - val_rmse: 1.7046\n",
      "Epoch 304/1000\n",
      "7329/7329 [==============================] - 5s 711us/sample - loss: 8.3891 - rmse: 2.8301 - val_loss: 3.4236 - val_rmse: 1.6942\n",
      "Epoch 305/1000\n",
      "7329/7329 [==============================] - 5s 711us/sample - loss: 8.2570 - rmse: 2.8076 - val_loss: 3.3774 - val_rmse: 1.6832\n",
      "Epoch 306/1000\n",
      "7329/7329 [==============================] - 5s 711us/sample - loss: 8.2898 - rmse: 2.8149 - val_loss: 3.5362 - val_rmse: 1.7197\n",
      "Epoch 307/1000\n",
      "7329/7329 [==============================] - 5s 711us/sample - loss: 8.2722 - rmse: 2.8109 - val_loss: 3.3627 - val_rmse: 1.6770\n",
      "Epoch 308/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 8.1860 - rmse: 2.7997 - val_loss: 3.3360 - val_rmse: 1.6737\n",
      "Epoch 309/1000\n",
      "7329/7329 [==============================] - 5s 716us/sample - loss: 8.1873 - rmse: 2.7933 - val_loss: 3.5593 - val_rmse: 1.7241\n",
      "Epoch 310/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 8.1298 - rmse: 2.7871 - val_loss: 3.4378 - val_rmse: 1.6992\n",
      "Epoch 311/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 8.1169 - rmse: 2.7832 - val_loss: 3.6815 - val_rmse: 1.7665\n",
      "Epoch 312/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 8.0152 - rmse: 2.7696 - val_loss: 3.4876 - val_rmse: 1.7127\n",
      "Epoch 313/1000\n",
      "7329/7329 [==============================] - 5s 727us/sample - loss: 8.0662 - rmse: 2.7739 - val_loss: 3.1879 - val_rmse: 1.6484\n",
      "Epoch 314/1000\n",
      "7329/7329 [==============================] - 5s 732us/sample - loss: 7.9872 - rmse: 2.7599 - val_loss: 3.1805 - val_rmse: 1.6373\n",
      "Epoch 315/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 8.0386 - rmse: 2.7686 - val_loss: 3.2469 - val_rmse: 1.6490\n",
      "Epoch 316/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 7.9335 - rmse: 2.7519 - val_loss: 3.1733 - val_rmse: 1.6375\n",
      "Epoch 317/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 7.9119 - rmse: 2.7493 - val_loss: 3.3103 - val_rmse: 1.6638\n",
      "Epoch 318/1000\n",
      "7329/7329 [==============================] - 5s 711us/sample - loss: 7.8799 - rmse: 2.7426 - val_loss: 3.2700 - val_rmse: 1.6552\n",
      "Epoch 319/1000\n",
      "7329/7329 [==============================] - 5s 718us/sample - loss: 7.8633 - rmse: 2.7398 - val_loss: 3.6222 - val_rmse: 1.7463\n",
      "Epoch 320/1000\n",
      "7329/7329 [==============================] - 5s 714us/sample - loss: 7.7590 - rmse: 2.7229 - val_loss: 3.3594 - val_rmse: 1.6790\n",
      "Epoch 321/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 7.8044 - rmse: 2.7301 - val_loss: 3.2028 - val_rmse: 1.6381\n",
      "Epoch 322/1000\n",
      "7329/7329 [==============================] - 5s 711us/sample - loss: 7.7967 - rmse: 2.7285 - val_loss: 3.2423 - val_rmse: 1.6475\n",
      "Epoch 323/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 7.7461 - rmse: 2.7198 - val_loss: 3.2106 - val_rmse: 1.6380\n",
      "Epoch 324/1000\n",
      "7329/7329 [==============================] - 5s 728us/sample - loss: 7.6690 - rmse: 2.7057 - val_loss: 3.1618 - val_rmse: 1.6290\n",
      "Epoch 325/1000\n",
      "7329/7329 [==============================] - 5s 716us/sample - loss: 7.7533 - rmse: 2.7212 - val_loss: 3.2816 - val_rmse: 1.6570\n",
      "Epoch 326/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 7.6173 - rmse: 2.6958 - val_loss: 3.2096 - val_rmse: 1.6391\n",
      "Epoch 327/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 7.6162 - rmse: 2.6943 - val_loss: 3.4846 - val_rmse: 1.7139\n",
      "Epoch 328/1000\n",
      "7329/7329 [==============================] - 5s 714us/sample - loss: 7.6448 - rmse: 2.7027 - val_loss: 3.2640 - val_rmse: 1.6526\n",
      "Epoch 329/1000\n",
      "7329/7329 [==============================] - 5s 711us/sample - loss: 7.5435 - rmse: 2.6824 - val_loss: 3.3118 - val_rmse: 1.6687\n",
      "Epoch 330/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 7.5101 - rmse: 2.6767 - val_loss: 3.4893 - val_rmse: 1.7095\n",
      "Epoch 331/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 7.5028 - rmse: 2.6758 - val_loss: 3.2709 - val_rmse: 1.6520\n",
      "Epoch 332/1000\n",
      "7329/7329 [==============================] - 5s 721us/sample - loss: 7.5028 - rmse: 2.6745 - val_loss: 3.4969 - val_rmse: 1.7127\n",
      "Epoch 333/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 7.4773 - rmse: 2.6711 - val_loss: 3.2691 - val_rmse: 1.6518\n",
      "Epoch 334/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 7.3895 - rmse: 2.6536 - val_loss: 3.2137 - val_rmse: 1.6380\n",
      "Epoch 335/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 7.4385 - rmse: 2.6634 - val_loss: 3.3642 - val_rmse: 1.6759\n",
      "Epoch 336/1000\n",
      "7329/7329 [==============================] - 5s 742us/sample - loss: 7.4017 - rmse: 2.6566 - val_loss: 3.1508 - val_rmse: 1.6217\n",
      "Epoch 337/1000\n",
      "7329/7329 [==============================] - 5s 720us/sample - loss: 7.3847 - rmse: 2.6528 - val_loss: 3.4728 - val_rmse: 1.7099\n",
      "Epoch 338/1000\n",
      "7329/7329 [==============================] - 5s 729us/sample - loss: 7.3218 - rmse: 2.6425 - val_loss: 3.1108 - val_rmse: 1.6145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 339/1000\n",
      "7329/7329 [==============================] - 5s 711us/sample - loss: 7.3592 - rmse: 2.6465 - val_loss: 3.2252 - val_rmse: 1.6423\n",
      "Epoch 340/1000\n",
      "7329/7329 [==============================] - 5s 711us/sample - loss: 7.3320 - rmse: 2.6416 - val_loss: 3.2798 - val_rmse: 1.6534\n",
      "Epoch 341/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 7.2410 - rmse: 2.6269 - val_loss: 3.1535 - val_rmse: 1.6207\n",
      "Epoch 342/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 7.2594 - rmse: 2.6300 - val_loss: 3.4774 - val_rmse: 1.7114\n",
      "Epoch 343/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 7.2135 - rmse: 2.6218 - val_loss: 3.2682 - val_rmse: 1.6549\n",
      "Epoch 344/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 7.1736 - rmse: 2.6178 - val_loss: 3.1223 - val_rmse: 1.6174\n",
      "Epoch 345/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 7.0947 - rmse: 2.6007 - val_loss: 3.3135 - val_rmse: 1.6639\n",
      "Epoch 346/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 7.0938 - rmse: 2.6006 - val_loss: 3.2190 - val_rmse: 1.6374\n",
      "Epoch 347/1000\n",
      "7329/7329 [==============================] - 5s 728us/sample - loss: 7.0827 - rmse: 2.5998 - val_loss: 3.1057 - val_rmse: 1.6131\n",
      "Epoch 348/1000\n",
      "7329/7329 [==============================] - 5s 745us/sample - loss: 7.0689 - rmse: 2.5950 - val_loss: 3.3003 - val_rmse: 1.6567\n",
      "Epoch 349/1000\n",
      "7329/7329 [==============================] - 5s 717us/sample - loss: 7.0065 - rmse: 2.5844 - val_loss: 3.1724 - val_rmse: 1.6274\n",
      "Epoch 350/1000\n",
      "7329/7329 [==============================] - 5s 710us/sample - loss: 7.0026 - rmse: 2.5842 - val_loss: 3.1823 - val_rmse: 1.6283\n",
      "Epoch 351/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 6.9767 - rmse: 2.5801 - val_loss: 3.1890 - val_rmse: 1.6269\n",
      "Epoch 352/1000\n",
      "7329/7329 [==============================] - 5s 727us/sample - loss: 6.9738 - rmse: 2.5769 - val_loss: 3.0662 - val_rmse: 1.6006\n",
      "Epoch 353/1000\n",
      "7329/7329 [==============================] - 5s 711us/sample - loss: 6.9869 - rmse: 2.5787 - val_loss: 3.1256 - val_rmse: 1.6140\n",
      "Epoch 354/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 6.9994 - rmse: 2.5819 - val_loss: 3.1870 - val_rmse: 1.6253\n",
      "Epoch 355/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 6.8805 - rmse: 2.5611 - val_loss: 3.2482 - val_rmse: 1.6447\n",
      "Epoch 356/1000\n",
      "7329/7329 [==============================] - 5s 711us/sample - loss: 6.9299 - rmse: 2.5700 - val_loss: 3.1733 - val_rmse: 1.6251\n",
      "Epoch 357/1000\n",
      "7329/7329 [==============================] - 5s 729us/sample - loss: 6.8007 - rmse: 2.5459 - val_loss: 3.0344 - val_rmse: 1.5923\n",
      "Epoch 358/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 6.8718 - rmse: 2.5564 - val_loss: 3.0279 - val_rmse: 1.5963\n",
      "Epoch 359/1000\n",
      "7329/7329 [==============================] - 5s 711us/sample - loss: 6.8649 - rmse: 2.5555 - val_loss: 3.2296 - val_rmse: 1.6380\n",
      "Epoch 360/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 6.7622 - rmse: 2.5371 - val_loss: 3.3323 - val_rmse: 1.6642\n",
      "Epoch 361/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 6.7759 - rmse: 2.5412 - val_loss: 3.2678 - val_rmse: 1.6511\n",
      "Epoch 362/1000\n",
      "7329/7329 [==============================] - 5s 711us/sample - loss: 6.7250 - rmse: 2.5315 - val_loss: 3.1339 - val_rmse: 1.6126\n",
      "Epoch 363/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 6.7528 - rmse: 2.5375 - val_loss: 3.1039 - val_rmse: 1.6054\n",
      "Epoch 364/1000\n",
      "7329/7329 [==============================] - 5s 720us/sample - loss: 6.7295 - rmse: 2.5309 - val_loss: 3.3270 - val_rmse: 1.6631\n",
      "Epoch 365/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 6.6964 - rmse: 2.5237 - val_loss: 3.3448 - val_rmse: 1.6725\n",
      "Epoch 366/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 6.6671 - rmse: 2.5182 - val_loss: 3.0484 - val_rmse: 1.5955\n",
      "Epoch 367/1000\n",
      "7329/7329 [==============================] - 5s 716us/sample - loss: 6.6732 - rmse: 2.5201 - val_loss: 3.0447 - val_rmse: 1.5942\n",
      "Epoch 368/1000\n",
      "7329/7329 [==============================] - 5s 711us/sample - loss: 6.6409 - rmse: 2.5144 - val_loss: 3.1300 - val_rmse: 1.6147\n",
      "Epoch 369/1000\n",
      "7329/7329 [==============================] - 5s 710us/sample - loss: 6.6554 - rmse: 2.5161 - val_loss: 3.0856 - val_rmse: 1.6014\n",
      "Epoch 370/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 6.6314 - rmse: 2.5125 - val_loss: 3.1545 - val_rmse: 1.6168\n",
      "Epoch 371/1000\n",
      "7329/7329 [==============================] - 5s 729us/sample - loss: 6.5608 - rmse: 2.4990 - val_loss: 2.9802 - val_rmse: 1.5761\n",
      "Epoch 372/1000\n",
      "7329/7329 [==============================] - 5s 718us/sample - loss: 6.4912 - rmse: 2.4852 - val_loss: 3.0184 - val_rmse: 1.5850\n",
      "Epoch 373/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 6.5348 - rmse: 2.4913 - val_loss: 3.3078 - val_rmse: 1.6589\n",
      "Epoch 374/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 6.4784 - rmse: 2.4823 - val_loss: 3.1077 - val_rmse: 1.6068\n",
      "Epoch 375/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 6.4297 - rmse: 2.4722 - val_loss: 3.1402 - val_rmse: 1.6175\n",
      "Epoch 376/1000\n",
      "7329/7329 [==============================] - 5s 714us/sample - loss: 6.4681 - rmse: 2.4812 - val_loss: 3.2859 - val_rmse: 1.6517\n",
      "Epoch 377/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 6.4474 - rmse: 2.4755 - val_loss: 3.2923 - val_rmse: 1.6554\n",
      "Epoch 378/1000\n",
      "7329/7329 [==============================] - 5s 715us/sample - loss: 6.4421 - rmse: 2.4762 - val_loss: 3.0162 - val_rmse: 1.5848\n",
      "Epoch 379/1000\n",
      "7329/7329 [==============================] - 5s 711us/sample - loss: 6.4191 - rmse: 2.4701 - val_loss: 3.0482 - val_rmse: 1.5918\n",
      "Epoch 380/1000\n",
      "7329/7329 [==============================] - 5s 711us/sample - loss: 6.3445 - rmse: 2.4570 - val_loss: 3.1525 - val_rmse: 1.6243\n",
      "Epoch 381/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 6.4097 - rmse: 2.4674 - val_loss: 2.9960 - val_rmse: 1.5788\n",
      "Epoch 382/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 6.3935 - rmse: 2.4658 - val_loss: 3.2258 - val_rmse: 1.6375\n",
      "Epoch 383/1000\n",
      "7329/7329 [==============================] - 5s 715us/sample - loss: 6.3540 - rmse: 2.4585 - val_loss: 3.0721 - val_rmse: 1.5978\n",
      "Epoch 384/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 6.3603 - rmse: 2.4569 - val_loss: 3.0026 - val_rmse: 1.5820\n",
      "Epoch 385/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 6.2592 - rmse: 2.4377 - val_loss: 3.0282 - val_rmse: 1.5856\n",
      "Epoch 386/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 6.2861 - rmse: 2.4436 - val_loss: 3.5410 - val_rmse: 1.7287\n",
      "Epoch 387/1000\n",
      "7329/7329 [==============================] - 5s 717us/sample - loss: 6.2939 - rmse: 2.4456 - val_loss: 2.9856 - val_rmse: 1.5761\n",
      "Epoch 388/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 6.2279 - rmse: 2.4345 - val_loss: 3.0624 - val_rmse: 1.5982\n",
      "Epoch 389/1000\n",
      "7329/7329 [==============================] - 5s 742us/sample - loss: 6.3031 - rmse: 2.4455 - val_loss: 2.9600 - val_rmse: 1.5682\n",
      "Epoch 390/1000\n",
      "7329/7329 [==============================] - 5s 720us/sample - loss: 6.1817 - rmse: 2.4241 - val_loss: 2.9525 - val_rmse: 1.5757\n",
      "Epoch 391/1000\n",
      "7329/7329 [==============================] - 5s 714us/sample - loss: 6.1609 - rmse: 2.4190 - val_loss: 2.9980 - val_rmse: 1.5819\n",
      "Epoch 392/1000\n",
      "7329/7329 [==============================] - 5s 728us/sample - loss: 6.1743 - rmse: 2.4225 - val_loss: 2.9177 - val_rmse: 1.5610\n",
      "Epoch 393/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 6.1831 - rmse: 2.4231 - val_loss: 3.0460 - val_rmse: 1.5876\n",
      "Epoch 394/1000\n",
      "7329/7329 [==============================] - 5s 714us/sample - loss: 6.1161 - rmse: 2.4092 - val_loss: 3.0163 - val_rmse: 1.5842\n",
      "Epoch 395/1000\n",
      "7329/7329 [==============================] - 5s 716us/sample - loss: 6.1252 - rmse: 2.4120 - val_loss: 3.0547 - val_rmse: 1.5942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 396/1000\n",
      "7329/7329 [==============================] - 5s 710us/sample - loss: 6.1393 - rmse: 2.4157 - val_loss: 2.9948 - val_rmse: 1.5880\n",
      "Epoch 397/1000\n",
      "7329/7329 [==============================] - 5s 714us/sample - loss: 6.0437 - rmse: 2.3969 - val_loss: 3.3995 - val_rmse: 1.6831\n",
      "Epoch 398/1000\n",
      "7329/7329 [==============================] - 5s 711us/sample - loss: 6.0274 - rmse: 2.3918 - val_loss: 3.1101 - val_rmse: 1.6037\n",
      "Epoch 399/1000\n",
      "7329/7329 [==============================] - 5s 711us/sample - loss: 6.0865 - rmse: 2.4033 - val_loss: 2.9879 - val_rmse: 1.5760\n",
      "Epoch 400/1000\n",
      "7329/7329 [==============================] - 5s 711us/sample - loss: 6.0660 - rmse: 2.3995 - val_loss: 2.9394 - val_rmse: 1.5655\n",
      "Epoch 401/1000\n",
      "7329/7329 [==============================] - 5s 710us/sample - loss: 6.0783 - rmse: 2.3987 - val_loss: 2.9359 - val_rmse: 1.5709\n",
      "Epoch 402/1000\n",
      "7329/7329 [==============================] - 5s 709us/sample - loss: 6.0007 - rmse: 2.3871 - val_loss: 3.0680 - val_rmse: 1.5923\n",
      "Epoch 403/1000\n",
      "7329/7329 [==============================] - 5s 714us/sample - loss: 5.9988 - rmse: 2.3859 - val_loss: 2.9073 - val_rmse: 1.5625\n",
      "Epoch 404/1000\n",
      "7329/7329 [==============================] - 5s 709us/sample - loss: 5.9473 - rmse: 2.3756 - val_loss: 2.9121 - val_rmse: 1.5611\n",
      "Epoch 405/1000\n",
      "7329/7329 [==============================] - 5s 714us/sample - loss: 5.9593 - rmse: 2.3803 - val_loss: 3.0829 - val_rmse: 1.5983\n",
      "Epoch 406/1000\n",
      "7329/7329 [==============================] - 5s 716us/sample - loss: 5.9301 - rmse: 2.3721 - val_loss: 3.0496 - val_rmse: 1.5917\n",
      "Epoch 407/1000\n",
      "7329/7329 [==============================] - 5s 726us/sample - loss: 5.9388 - rmse: 2.3721 - val_loss: 2.9198 - val_rmse: 1.5553\n",
      "Epoch 408/1000\n",
      "7329/7329 [==============================] - 5s 711us/sample - loss: 5.9332 - rmse: 2.3747 - val_loss: 2.9902 - val_rmse: 1.5724\n",
      "Epoch 409/1000\n",
      "7329/7329 [==============================] - 5s 718us/sample - loss: 5.9269 - rmse: 2.3712 - val_loss: 2.9984 - val_rmse: 1.5740\n",
      "Epoch 410/1000\n",
      "7329/7329 [==============================] - 5s 716us/sample - loss: 5.8937 - rmse: 2.3635 - val_loss: 3.0559 - val_rmse: 1.5907\n",
      "Epoch 411/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 5.8351 - rmse: 2.3524 - val_loss: 3.0830 - val_rmse: 1.5956\n",
      "Epoch 412/1000\n",
      "7329/7329 [==============================] - 5s 711us/sample - loss: 5.8179 - rmse: 2.3494 - val_loss: 2.9999 - val_rmse: 1.5818\n",
      "Epoch 413/1000\n",
      "7329/7329 [==============================] - 5s 711us/sample - loss: 5.7919 - rmse: 2.3448 - val_loss: 2.9558 - val_rmse: 1.5753\n",
      "Epoch 414/1000\n",
      "7329/7329 [==============================] - 5s 710us/sample - loss: 5.7857 - rmse: 2.3435 - val_loss: 2.9881 - val_rmse: 1.5744\n",
      "Epoch 415/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 5.8289 - rmse: 2.3511 - val_loss: 3.2077 - val_rmse: 1.6362\n",
      "Epoch 416/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 5.8031 - rmse: 2.3451 - val_loss: 3.0045 - val_rmse: 1.5808\n",
      "Epoch 417/1000\n",
      "7329/7329 [==============================] - 5s 710us/sample - loss: 5.7826 - rmse: 2.3401 - val_loss: 2.9611 - val_rmse: 1.5631\n",
      "Epoch 418/1000\n",
      "7329/7329 [==============================] - 5s 715us/sample - loss: 5.7647 - rmse: 2.3379 - val_loss: 3.0379 - val_rmse: 1.5846\n",
      "Epoch 419/1000\n",
      "7329/7329 [==============================] - 5s 727us/sample - loss: 5.7486 - rmse: 2.3348 - val_loss: 2.8551 - val_rmse: 1.5472\n",
      "Epoch 420/1000\n",
      "7329/7329 [==============================] - 5s 714us/sample - loss: 5.6658 - rmse: 2.3179 - val_loss: 2.8561 - val_rmse: 1.5531\n",
      "Epoch 421/1000\n",
      "7329/7329 [==============================] - 5s 711us/sample - loss: 5.6504 - rmse: 2.3148 - val_loss: 3.2136 - val_rmse: 1.6335\n",
      "Epoch 422/1000\n",
      "7329/7329 [==============================] - 5s 728us/sample - loss: 5.6345 - rmse: 2.3115 - val_loss: 2.8844 - val_rmse: 1.5455\n",
      "Epoch 423/1000\n",
      "7329/7329 [==============================] - 5s 711us/sample - loss: 5.6948 - rmse: 2.3252 - val_loss: 2.9480 - val_rmse: 1.5593\n",
      "Epoch 424/1000\n",
      "7329/7329 [==============================] - 5s 716us/sample - loss: 5.6397 - rmse: 2.3116 - val_loss: 2.9323 - val_rmse: 1.5557\n",
      "Epoch 425/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 5.7167 - rmse: 2.3251 - val_loss: 3.1451 - val_rmse: 1.6151\n",
      "Epoch 426/1000\n",
      "7329/7329 [==============================] - 5s 711us/sample - loss: 5.6471 - rmse: 2.3135 - val_loss: 2.9539 - val_rmse: 1.5641\n",
      "Epoch 427/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 5.5900 - rmse: 2.3018 - val_loss: 3.2300 - val_rmse: 1.6402\n",
      "Epoch 428/1000\n",
      "7329/7329 [==============================] - 5s 711us/sample - loss: 5.6212 - rmse: 2.3072 - val_loss: 3.0516 - val_rmse: 1.5920\n",
      "Epoch 429/1000\n",
      "7329/7329 [==============================] - 5s 716us/sample - loss: 5.5999 - rmse: 2.3045 - val_loss: 2.9030 - val_rmse: 1.5485\n",
      "Epoch 430/1000\n",
      "7329/7329 [==============================] - 5s 727us/sample - loss: 5.6169 - rmse: 2.3074 - val_loss: 2.8840 - val_rmse: 1.5412\n",
      "Epoch 431/1000\n",
      "7329/7329 [==============================] - 5s 730us/sample - loss: 5.6015 - rmse: 2.3029 - val_loss: 2.8430 - val_rmse: 1.5319\n",
      "Epoch 432/1000\n",
      "7329/7329 [==============================] - 5s 717us/sample - loss: 5.5147 - rmse: 2.2846 - val_loss: 2.8257 - val_rmse: 1.5320\n",
      "Epoch 433/1000\n",
      "7329/7329 [==============================] - 5s 729us/sample - loss: 5.5143 - rmse: 2.2871 - val_loss: 2.7998 - val_rmse: 1.5250\n",
      "Epoch 434/1000\n",
      "7329/7329 [==============================] - 5s 714us/sample - loss: 5.5106 - rmse: 2.2839 - val_loss: 2.9940 - val_rmse: 1.5726\n",
      "Epoch 435/1000\n",
      "7329/7329 [==============================] - 5s 714us/sample - loss: 5.5235 - rmse: 2.2862 - val_loss: 2.8862 - val_rmse: 1.5433\n",
      "Epoch 436/1000\n",
      "7329/7329 [==============================] - 5s 728us/sample - loss: 5.4606 - rmse: 2.2740 - val_loss: 2.7762 - val_rmse: 1.5194\n",
      "Epoch 437/1000\n",
      "7329/7329 [==============================] - 5s 714us/sample - loss: 5.4949 - rmse: 2.2800 - val_loss: 2.8673 - val_rmse: 1.5392\n",
      "Epoch 438/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 5.5384 - rmse: 2.2894 - val_loss: 2.9355 - val_rmse: 1.5622\n",
      "Epoch 439/1000\n",
      "7329/7329 [==============================] - 5s 714us/sample - loss: 5.3969 - rmse: 2.2616 - val_loss: 2.8295 - val_rmse: 1.5319\n",
      "Epoch 440/1000\n",
      "7329/7329 [==============================] - 5s 720us/sample - loss: 5.4126 - rmse: 2.2631 - val_loss: 2.8003 - val_rmse: 1.5255\n",
      "Epoch 441/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 5.4041 - rmse: 2.2639 - val_loss: 2.8759 - val_rmse: 1.5413\n",
      "Epoch 442/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 5.4050 - rmse: 2.2601 - val_loss: 2.9263 - val_rmse: 1.5533\n",
      "Epoch 443/1000\n",
      "7329/7329 [==============================] - 5s 714us/sample - loss: 5.4490 - rmse: 2.2729 - val_loss: 2.9110 - val_rmse: 1.5492\n",
      "Epoch 444/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 5.3201 - rmse: 2.2454 - val_loss: 2.8921 - val_rmse: 1.5458\n",
      "Epoch 445/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 5.4166 - rmse: 2.2608 - val_loss: 2.8688 - val_rmse: 1.5406\n",
      "Epoch 446/1000\n",
      "7329/7329 [==============================] - 5s 716us/sample - loss: 5.3749 - rmse: 2.2556 - val_loss: 3.0128 - val_rmse: 1.5722\n",
      "Epoch 447/1000\n",
      "7329/7329 [==============================] - 5s 716us/sample - loss: 5.3789 - rmse: 2.2563 - val_loss: 2.8122 - val_rmse: 1.5259\n",
      "Epoch 448/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 5.3473 - rmse: 2.2488 - val_loss: 2.9463 - val_rmse: 1.5552\n",
      "Epoch 449/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 5.2678 - rmse: 2.2327 - val_loss: 2.8436 - val_rmse: 1.5317\n",
      "Epoch 450/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 5.3229 - rmse: 2.2442 - val_loss: 2.8546 - val_rmse: 1.5380\n",
      "Epoch 451/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 5.3291 - rmse: 2.2442 - val_loss: 2.8499 - val_rmse: 1.5344\n",
      "Epoch 452/1000\n",
      "7329/7329 [==============================] - 5s 714us/sample - loss: 5.3339 - rmse: 2.2442 - val_loss: 2.9709 - val_rmse: 1.5683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 453/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 5.2130 - rmse: 2.2212 - val_loss: 2.9287 - val_rmse: 1.5586\n",
      "Epoch 454/1000\n",
      "7329/7329 [==============================] - 5s 710us/sample - loss: 5.2615 - rmse: 2.2309 - val_loss: 3.0072 - val_rmse: 1.5777\n",
      "Epoch 455/1000\n",
      "7329/7329 [==============================] - 5s 710us/sample - loss: 5.2481 - rmse: 2.2281 - val_loss: 2.8757 - val_rmse: 1.5403\n",
      "Epoch 456/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 5.2353 - rmse: 2.2247 - val_loss: 2.9285 - val_rmse: 1.5551\n",
      "Epoch 457/1000\n",
      "7329/7329 [==============================] - 5s 710us/sample - loss: 5.2303 - rmse: 2.2250 - val_loss: 2.8356 - val_rmse: 1.5314\n",
      "Epoch 458/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 5.2349 - rmse: 2.2240 - val_loss: 2.9422 - val_rmse: 1.5569\n",
      "Epoch 459/1000\n",
      "7329/7329 [==============================] - 5s 730us/sample - loss: 5.2423 - rmse: 2.2264 - val_loss: 2.7599 - val_rmse: 1.5161\n",
      "Epoch 460/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 5.1677 - rmse: 2.2102 - val_loss: 2.7768 - val_rmse: 1.5171\n",
      "Epoch 461/1000\n",
      "7329/7329 [==============================] - 5s 708us/sample - loss: 5.1672 - rmse: 2.2100 - val_loss: 2.8545 - val_rmse: 1.5314\n",
      "Epoch 462/1000\n",
      "7329/7329 [==============================] - 5s 716us/sample - loss: 5.1773 - rmse: 2.2128 - val_loss: 2.8965 - val_rmse: 1.5433\n",
      "Epoch 463/1000\n",
      "7329/7329 [==============================] - 5s 716us/sample - loss: 5.0994 - rmse: 2.1946 - val_loss: 2.8016 - val_rmse: 1.5210\n",
      "Epoch 464/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 5.1024 - rmse: 2.1954 - val_loss: 2.9059 - val_rmse: 1.5455\n",
      "Epoch 465/1000\n",
      "7329/7329 [==============================] - 5s 727us/sample - loss: 5.1992 - rmse: 2.2155 - val_loss: 2.7700 - val_rmse: 1.5150\n",
      "Epoch 466/1000\n",
      "7329/7329 [==============================] - 5s 714us/sample - loss: 5.1181 - rmse: 2.1990 - val_loss: 2.8569 - val_rmse: 1.5409\n",
      "Epoch 467/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 5.1349 - rmse: 2.2022 - val_loss: 2.8038 - val_rmse: 1.5276\n",
      "Epoch 468/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 5.0529 - rmse: 2.1841 - val_loss: 2.9584 - val_rmse: 1.5620\n",
      "Epoch 469/1000\n",
      "7329/7329 [==============================] - 5s 731us/sample - loss: 5.0207 - rmse: 2.1805 - val_loss: 2.7459 - val_rmse: 1.5102\n",
      "Epoch 470/1000\n",
      "7329/7329 [==============================] - 5s 710us/sample - loss: 5.0026 - rmse: 2.1753 - val_loss: 3.1076 - val_rmse: 1.6043\n",
      "Epoch 471/1000\n",
      "7329/7329 [==============================] - 5s 709us/sample - loss: 5.0094 - rmse: 2.1772 - val_loss: 3.0205 - val_rmse: 1.5785\n",
      "Epoch 472/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 4.9874 - rmse: 2.1712 - val_loss: 2.8742 - val_rmse: 1.5415\n",
      "Epoch 473/1000\n",
      "7329/7329 [==============================] - 5s 726us/sample - loss: 5.0309 - rmse: 2.1811 - val_loss: 2.7242 - val_rmse: 1.5015\n",
      "Epoch 474/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 5.0433 - rmse: 2.1816 - val_loss: 2.7770 - val_rmse: 1.5247\n",
      "Epoch 475/1000\n",
      "7329/7329 [==============================] - 5s 716us/sample - loss: 5.0173 - rmse: 2.1766 - val_loss: 2.7675 - val_rmse: 1.5149\n",
      "Epoch 476/1000\n",
      "7329/7329 [==============================] - 5s 710us/sample - loss: 5.0264 - rmse: 2.1770 - val_loss: 2.7607 - val_rmse: 1.5129\n",
      "Epoch 477/1000\n",
      "7329/7329 [==============================] - 5s 710us/sample - loss: 5.0066 - rmse: 2.1740 - val_loss: 2.8185 - val_rmse: 1.5228\n",
      "Epoch 478/1000\n",
      "7329/7329 [==============================] - 5s 715us/sample - loss: 5.0158 - rmse: 2.1741 - val_loss: 2.8243 - val_rmse: 1.5277\n",
      "Epoch 479/1000\n",
      "7329/7329 [==============================] - 5s 711us/sample - loss: 4.9977 - rmse: 2.1704 - val_loss: 2.8816 - val_rmse: 1.5411\n",
      "Epoch 480/1000\n",
      "7329/7329 [==============================] - 5s 711us/sample - loss: 4.9596 - rmse: 2.1642 - val_loss: 3.1920 - val_rmse: 1.6208\n",
      "Epoch 481/1000\n",
      "7329/7329 [==============================] - 5s 723us/sample - loss: 4.9511 - rmse: 2.1628 - val_loss: 3.0072 - val_rmse: 1.5824\n",
      "Epoch 482/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 4.9659 - rmse: 2.1640 - val_loss: 2.7186 - val_rmse: 1.5047\n",
      "Epoch 483/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 4.9393 - rmse: 2.1596 - val_loss: 2.7263 - val_rmse: 1.5127\n",
      "Epoch 484/1000\n",
      "7329/7329 [==============================] - 5s 711us/sample - loss: 4.9187 - rmse: 2.1550 - val_loss: 2.8196 - val_rmse: 1.5271\n",
      "Epoch 485/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 4.9536 - rmse: 2.1599 - val_loss: 2.7276 - val_rmse: 1.5033\n",
      "Epoch 486/1000\n",
      "7329/7329 [==============================] - 5s 720us/sample - loss: 4.8513 - rmse: 2.1404 - val_loss: 2.7298 - val_rmse: 1.5033\n",
      "Epoch 487/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 4.8612 - rmse: 2.1417 - val_loss: 2.8235 - val_rmse: 1.5247\n",
      "Epoch 488/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 4.9393 - rmse: 2.1575 - val_loss: 2.7671 - val_rmse: 1.5254\n",
      "Epoch 489/1000\n",
      "7329/7329 [==============================] - 5s 714us/sample - loss: 4.8890 - rmse: 2.1446 - val_loss: 2.8747 - val_rmse: 1.5401\n",
      "Epoch 490/1000\n",
      "7329/7329 [==============================] - 5s 711us/sample - loss: 4.8487 - rmse: 2.1386 - val_loss: 2.7575 - val_rmse: 1.5077\n",
      "Epoch 491/1000\n",
      "7329/7329 [==============================] - 5s 715us/sample - loss: 4.8129 - rmse: 2.1307 - val_loss: 2.8174 - val_rmse: 1.5225\n",
      "Epoch 492/1000\n",
      "7329/7329 [==============================] - 5s 744us/sample - loss: 4.8005 - rmse: 2.1277 - val_loss: 2.7168 - val_rmse: 1.4966\n",
      "Epoch 493/1000\n",
      "7329/7329 [==============================] - 5s 714us/sample - loss: 4.8526 - rmse: 2.1372 - val_loss: 2.7543 - val_rmse: 1.5061\n",
      "Epoch 494/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 4.8087 - rmse: 2.1294 - val_loss: 2.8165 - val_rmse: 1.5256\n",
      "Epoch 495/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 4.8026 - rmse: 2.1276 - val_loss: 2.7122 - val_rmse: 1.5005\n",
      "Epoch 496/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 4.7871 - rmse: 2.1245 - val_loss: 2.7023 - val_rmse: 1.4983\n",
      "Epoch 497/1000\n",
      "7329/7329 [==============================] - 5s 714us/sample - loss: 4.7229 - rmse: 2.1120 - val_loss: 2.7376 - val_rmse: 1.5014\n",
      "Epoch 498/1000\n",
      "7329/7329 [==============================] - 5s 716us/sample - loss: 4.7896 - rmse: 2.1248 - val_loss: 2.7717 - val_rmse: 1.5097\n",
      "Epoch 499/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 4.7225 - rmse: 2.1106 - val_loss: 2.7235 - val_rmse: 1.4983\n",
      "Epoch 500/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 4.7676 - rmse: 2.1191 - val_loss: 2.7878 - val_rmse: 1.5158\n",
      "Epoch 501/1000\n",
      "7329/7329 [==============================] - 5s 718us/sample - loss: 4.7605 - rmse: 2.1177 - val_loss: 2.7150 - val_rmse: 1.4993\n",
      "Epoch 502/1000\n",
      "7329/7329 [==============================] - 5s 716us/sample - loss: 4.7199 - rmse: 2.1089 - val_loss: 2.7567 - val_rmse: 1.5217\n",
      "Epoch 503/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 4.7244 - rmse: 2.1096 - val_loss: 2.9701 - val_rmse: 1.5591\n",
      "Epoch 504/1000\n",
      "7329/7329 [==============================] - 5s 718us/sample - loss: 4.7375 - rmse: 2.1113 - val_loss: 2.7245 - val_rmse: 1.4987\n",
      "Epoch 505/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 4.7698 - rmse: 2.1182 - val_loss: 2.9943 - val_rmse: 1.5772\n",
      "Epoch 506/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 4.7334 - rmse: 2.1099 - val_loss: 2.8880 - val_rmse: 1.5381\n",
      "Epoch 507/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 4.6770 - rmse: 2.0988 - val_loss: 2.7407 - val_rmse: 1.5048\n",
      "Epoch 508/1000\n",
      "7329/7329 [==============================] - 5s 715us/sample - loss: 4.6647 - rmse: 2.0955 - val_loss: 2.7303 - val_rmse: 1.4992\n",
      "Epoch 509/1000\n",
      "7329/7329 [==============================] - 5s 717us/sample - loss: 4.6633 - rmse: 2.0960 - val_loss: 2.8990 - val_rmse: 1.5435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 510/1000\n",
      "7329/7329 [==============================] - 5s 725us/sample - loss: 4.6582 - rmse: 2.0945 - val_loss: 2.6744 - val_rmse: 1.4861\n",
      "Epoch 511/1000\n",
      "7329/7329 [==============================] - 5s 710us/sample - loss: 4.6493 - rmse: 2.0923 - val_loss: 2.7913 - val_rmse: 1.5131\n",
      "Epoch 512/1000\n",
      "7329/7329 [==============================] - 5s 728us/sample - loss: 4.6721 - rmse: 2.0968 - val_loss: 2.6477 - val_rmse: 1.4796\n",
      "Epoch 513/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 4.6364 - rmse: 2.0899 - val_loss: 2.7129 - val_rmse: 1.4957\n",
      "Epoch 514/1000\n",
      "7329/7329 [==============================] - 5s 711us/sample - loss: 4.6504 - rmse: 2.0923 - val_loss: 2.8880 - val_rmse: 1.5480\n",
      "Epoch 515/1000\n",
      "7329/7329 [==============================] - 5s 711us/sample - loss: 4.6356 - rmse: 2.0893 - val_loss: 2.7730 - val_rmse: 1.5082\n",
      "Epoch 516/1000\n",
      "7329/7329 [==============================] - 5s 711us/sample - loss: 4.6172 - rmse: 2.0856 - val_loss: 2.7127 - val_rmse: 1.4929\n",
      "Epoch 517/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 4.5786 - rmse: 2.0762 - val_loss: 2.7006 - val_rmse: 1.4923\n",
      "Epoch 518/1000\n",
      "7329/7329 [==============================] - 5s 711us/sample - loss: 4.6298 - rmse: 2.0863 - val_loss: 2.7108 - val_rmse: 1.4937\n",
      "Epoch 519/1000\n",
      "7329/7329 [==============================] - 5s 710us/sample - loss: 4.5611 - rmse: 2.0722 - val_loss: 3.0185 - val_rmse: 1.5796\n",
      "Epoch 520/1000\n",
      "7329/7329 [==============================] - 5s 711us/sample - loss: 4.5922 - rmse: 2.0787 - val_loss: 2.8141 - val_rmse: 1.5163\n",
      "Epoch 521/1000\n",
      "7329/7329 [==============================] - 5s 714us/sample - loss: 4.5888 - rmse: 2.0793 - val_loss: 2.7573 - val_rmse: 1.5184\n",
      "Epoch 522/1000\n",
      "7329/7329 [==============================] - 5s 710us/sample - loss: 4.6168 - rmse: 2.0845 - val_loss: 2.6808 - val_rmse: 1.5066\n",
      "Epoch 523/1000\n",
      "7329/7329 [==============================] - 5s 710us/sample - loss: 4.5275 - rmse: 2.0645 - val_loss: 2.6763 - val_rmse: 1.4872\n",
      "Epoch 524/1000\n",
      "7329/7329 [==============================] - 5s 723us/sample - loss: 4.5308 - rmse: 2.0649 - val_loss: 2.6615 - val_rmse: 1.4814\n",
      "Epoch 525/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 4.5480 - rmse: 2.0685 - val_loss: 2.9187 - val_rmse: 1.5530\n",
      "Epoch 526/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 4.5513 - rmse: 2.0674 - val_loss: 2.7525 - val_rmse: 1.5042\n",
      "Epoch 527/1000\n",
      "7329/7329 [==============================] - 5s 717us/sample - loss: 4.5333 - rmse: 2.0633 - val_loss: 2.6616 - val_rmse: 1.4900\n",
      "Epoch 528/1000\n",
      "7329/7329 [==============================] - 5s 711us/sample - loss: 4.5041 - rmse: 2.0562 - val_loss: 2.7074 - val_rmse: 1.4923\n",
      "Epoch 529/1000\n",
      "7329/7329 [==============================] - 5s 711us/sample - loss: 4.4990 - rmse: 2.0569 - val_loss: 2.6753 - val_rmse: 1.4979\n",
      "Epoch 530/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 4.4593 - rmse: 2.0475 - val_loss: 2.7064 - val_rmse: 1.4905\n",
      "Epoch 531/1000\n",
      "7329/7329 [==============================] - 5s 729us/sample - loss: 4.4779 - rmse: 2.0516 - val_loss: 2.6372 - val_rmse: 1.4783\n",
      "Epoch 532/1000\n",
      "7329/7329 [==============================] - 5s 717us/sample - loss: 4.4495 - rmse: 2.0435 - val_loss: 2.6650 - val_rmse: 1.4834\n",
      "Epoch 533/1000\n",
      "7329/7329 [==============================] - 5s 708us/sample - loss: 4.4372 - rmse: 2.0430 - val_loss: 2.8656 - val_rmse: 1.5333\n",
      "Epoch 534/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 4.4867 - rmse: 2.0522 - val_loss: 2.7217 - val_rmse: 1.4977\n",
      "Epoch 535/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 4.4160 - rmse: 2.0396 - val_loss: 2.6447 - val_rmse: 1.4828\n",
      "Epoch 536/1000\n",
      "7329/7329 [==============================] - 5s 710us/sample - loss: 4.4205 - rmse: 2.0394 - val_loss: 2.6676 - val_rmse: 1.4793\n",
      "Epoch 537/1000\n",
      "7329/7329 [==============================] - 5s 710us/sample - loss: 4.4179 - rmse: 2.0377 - val_loss: 2.7902 - val_rmse: 1.5083\n",
      "Epoch 538/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 4.4420 - rmse: 2.0420 - val_loss: 2.7650 - val_rmse: 1.5030\n",
      "Epoch 539/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 4.4587 - rmse: 2.0456 - val_loss: 2.6798 - val_rmse: 1.4828\n",
      "Epoch 540/1000\n",
      "7329/7329 [==============================] - 5s 714us/sample - loss: 4.4208 - rmse: 2.0380 - val_loss: 2.7978 - val_rmse: 1.5158\n",
      "Epoch 541/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 4.3604 - rmse: 2.0261 - val_loss: 2.6951 - val_rmse: 1.4875\n",
      "Epoch 542/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 4.4061 - rmse: 2.0354 - val_loss: 2.8272 - val_rmse: 1.5221\n",
      "Epoch 543/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 4.3485 - rmse: 2.0220 - val_loss: 2.7887 - val_rmse: 1.5089\n",
      "Epoch 544/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 4.3986 - rmse: 2.0315 - val_loss: 2.7197 - val_rmse: 1.4961\n",
      "Epoch 545/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 4.4129 - rmse: 2.0354 - val_loss: 3.0236 - val_rmse: 1.5753\n",
      "Epoch 546/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 4.3466 - rmse: 2.0211 - val_loss: 2.7056 - val_rmse: 1.5066\n",
      "Epoch 547/1000\n",
      "7329/7329 [==============================] - 5s 718us/sample - loss: 4.3534 - rmse: 2.0206 - val_loss: 2.6436 - val_rmse: 1.4789\n",
      "Epoch 548/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 4.3812 - rmse: 2.0305 - val_loss: 2.7482 - val_rmse: 1.4961\n",
      "Epoch 549/1000\n",
      "7329/7329 [==============================] - 5s 731us/sample - loss: 4.3379 - rmse: 2.0185 - val_loss: 2.6296 - val_rmse: 1.4738\n",
      "Epoch 550/1000\n",
      "7329/7329 [==============================] - 5s 736us/sample - loss: 4.3469 - rmse: 2.0194 - val_loss: 2.6426 - val_rmse: 1.4718\n",
      "Epoch 551/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 4.3029 - rmse: 2.0109 - val_loss: 2.7038 - val_rmse: 1.4881\n",
      "Epoch 552/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 4.3243 - rmse: 2.0157 - val_loss: 2.7237 - val_rmse: 1.4924\n",
      "Epoch 553/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 4.3176 - rmse: 2.0130 - val_loss: 2.7261 - val_rmse: 1.4930\n",
      "Epoch 554/1000\n",
      "7329/7329 [==============================] - 5s 714us/sample - loss: 4.2928 - rmse: 2.0076 - val_loss: 2.7528 - val_rmse: 1.5044\n",
      "Epoch 555/1000\n",
      "7329/7329 [==============================] - 5s 715us/sample - loss: 4.3033 - rmse: 2.0083 - val_loss: 2.7463 - val_rmse: 1.5019\n",
      "Epoch 556/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 4.2732 - rmse: 2.0022 - val_loss: 2.7354 - val_rmse: 1.4995\n",
      "Epoch 557/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 4.2783 - rmse: 2.0048 - val_loss: 2.6909 - val_rmse: 1.4874\n",
      "Epoch 558/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 4.2483 - rmse: 1.9972 - val_loss: 2.6376 - val_rmse: 1.4733\n",
      "Epoch 559/1000\n",
      "7329/7329 [==============================] - 5s 714us/sample - loss: 4.2566 - rmse: 1.9974 - val_loss: 2.6737 - val_rmse: 1.4814\n",
      "Epoch 560/1000\n",
      "7329/7329 [==============================] - 5s 727us/sample - loss: 4.2589 - rmse: 2.0000 - val_loss: 2.6143 - val_rmse: 1.4675\n",
      "Epoch 561/1000\n",
      "7329/7329 [==============================] - 5s 720us/sample - loss: 4.2287 - rmse: 1.9922 - val_loss: 2.6004 - val_rmse: 1.4686\n",
      "Epoch 562/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 4.2438 - rmse: 1.9951 - val_loss: 2.7638 - val_rmse: 1.5068\n",
      "Epoch 563/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 4.2825 - rmse: 2.0053 - val_loss: 2.7128 - val_rmse: 1.4905\n",
      "Epoch 564/1000\n",
      "7329/7329 [==============================] - 5s 731us/sample - loss: 4.2376 - rmse: 1.9936 - val_loss: 2.5817 - val_rmse: 1.4590\n",
      "Epoch 565/1000\n",
      "7329/7329 [==============================] - 5s 714us/sample - loss: 4.2345 - rmse: 1.9922 - val_loss: 2.6794 - val_rmse: 1.4822\n",
      "Epoch 566/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 4.2696 - rmse: 1.9983 - val_loss: 2.8105 - val_rmse: 1.5187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 567/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 4.1613 - rmse: 1.9764 - val_loss: 2.7170 - val_rmse: 1.4900\n",
      "Epoch 568/1000\n",
      "7329/7329 [==============================] - 5s 711us/sample - loss: 4.2179 - rmse: 1.9902 - val_loss: 2.6728 - val_rmse: 1.4804\n",
      "Epoch 569/1000\n",
      "7329/7329 [==============================] - 5s 711us/sample - loss: 4.1756 - rmse: 1.9775 - val_loss: 2.7231 - val_rmse: 1.4929\n",
      "Epoch 570/1000\n",
      "7329/7329 [==============================] - 5s 716us/sample - loss: 4.1815 - rmse: 1.9800 - val_loss: 2.6500 - val_rmse: 1.4716\n",
      "Epoch 571/1000\n",
      "7329/7329 [==============================] - 5s 728us/sample - loss: 4.1313 - rmse: 1.9688 - val_loss: 2.5684 - val_rmse: 1.4532\n",
      "Epoch 572/1000\n",
      "7329/7329 [==============================] - 5s 715us/sample - loss: 4.1526 - rmse: 1.9740 - val_loss: 2.6458 - val_rmse: 1.4836\n",
      "Epoch 573/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 4.1199 - rmse: 1.9654 - val_loss: 2.6919 - val_rmse: 1.4896\n",
      "Epoch 574/1000\n",
      "7329/7329 [==============================] - 5s 711us/sample - loss: 4.1790 - rmse: 1.9798 - val_loss: 2.6031 - val_rmse: 1.4653\n",
      "Epoch 575/1000\n",
      "7329/7329 [==============================] - 5s 711us/sample - loss: 4.1663 - rmse: 1.9748 - val_loss: 2.6068 - val_rmse: 1.4670\n",
      "Epoch 576/1000\n",
      "7329/7329 [==============================] - 5s 711us/sample - loss: 4.1506 - rmse: 1.9720 - val_loss: 2.6161 - val_rmse: 1.4702\n",
      "Epoch 577/1000\n",
      "7329/7329 [==============================] - 5s 714us/sample - loss: 4.1318 - rmse: 1.9685 - val_loss: 2.6775 - val_rmse: 1.4839\n",
      "Epoch 578/1000\n",
      "7329/7329 [==============================] - 5s 714us/sample - loss: 4.1222 - rmse: 1.9647 - val_loss: 2.6444 - val_rmse: 1.4737\n",
      "Epoch 579/1000\n",
      "7329/7329 [==============================] - 5s 710us/sample - loss: 4.1060 - rmse: 1.9635 - val_loss: 2.6536 - val_rmse: 1.4731\n",
      "Epoch 580/1000\n",
      "7329/7329 [==============================] - 5s 711us/sample - loss: 4.1187 - rmse: 1.9634 - val_loss: 2.6344 - val_rmse: 1.4690\n",
      "Epoch 581/1000\n",
      "7329/7329 [==============================] - 5s 711us/sample - loss: 4.0644 - rmse: 1.9513 - val_loss: 2.7388 - val_rmse: 1.4992\n",
      "Epoch 582/1000\n",
      "7329/7329 [==============================] - 5s 711us/sample - loss: 4.0834 - rmse: 1.9567 - val_loss: 2.6769 - val_rmse: 1.4789\n",
      "Epoch 583/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 4.0492 - rmse: 1.9487 - val_loss: 2.7701 - val_rmse: 1.5044\n",
      "Epoch 584/1000\n",
      "7329/7329 [==============================] - 5s 713us/sample - loss: 4.0843 - rmse: 1.9562 - val_loss: 2.7144 - val_rmse: 1.4911\n",
      "Epoch 585/1000\n",
      "7329/7329 [==============================] - 5s 710us/sample - loss: 4.0983 - rmse: 1.9572 - val_loss: 2.7165 - val_rmse: 1.4894\n",
      "Epoch 586/1000\n",
      "7329/7329 [==============================] - 5s 728us/sample - loss: 4.0764 - rmse: 1.9535 - val_loss: 2.5553 - val_rmse: 1.4486\n",
      "Epoch 587/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 4.0857 - rmse: 1.9548 - val_loss: 2.6463 - val_rmse: 1.4729\n",
      "Epoch 588/1000\n",
      "7329/7329 [==============================] - 5s 712us/sample - loss: 4.0794 - rmse: 1.9538 - val_loss: 2.6314 - val_rmse: 1.4666\n",
      "Epoch 589/1000\n",
      "7329/7329 [==============================] - 5s 714us/sample - loss: 4.0601 - rmse: 1.9494 - val_loss: 2.6561 - val_rmse: 1.4738\n",
      "Epoch 590/1000\n",
      "7329/7329 [==============================] - 5s 717us/sample - loss: 4.0213 - rmse: 1.9400 - val_loss: 2.6361 - val_rmse: 1.4680\n",
      "Epoch 591/1000\n",
      "7329/7329 [==============================] - 5s 720us/sample - loss: 4.0514 - rmse: 1.9502 - val_loss: 2.8009 - val_rmse: 1.5115\n",
      "Epoch 592/1000\n",
      "7329/7329 [==============================] - 6s 756us/sample - loss: 4.0141 - rmse: 1.9381 - val_loss: 2.8656 - val_rmse: 1.5262\n",
      "Epoch 593/1000\n",
      "7329/7329 [==============================] - 6s 875us/sample - loss: 4.0555 - rmse: 1.9455 - val_loss: 2.6213 - val_rmse: 1.4631\n",
      "Epoch 594/1000\n",
      "7329/7329 [==============================] - 7s 906us/sample - loss: 4.0485 - rmse: 1.9462 - val_loss: 2.9540 - val_rmse: 1.5571\n",
      "Epoch 595/1000\n",
      "7329/7329 [==============================] - 6s 826us/sample - loss: 4.0410 - rmse: 1.9463 - val_loss: 2.7170 - val_rmse: 1.4864\n",
      "Epoch 596/1000\n",
      "7329/7329 [==============================] - 6s 808us/sample - loss: 4.0186 - rmse: 1.9394 - val_loss: 2.7879 - val_rmse: 1.5079\n",
      "Epoch 597/1000\n",
      "7329/7329 [==============================] - 6s 849us/sample - loss: 4.0004 - rmse: 1.9339 - val_loss: 2.7002 - val_rmse: 1.4812\n",
      "Epoch 598/1000\n",
      "7329/7329 [==============================] - 5s 681us/sample - loss: 4.0144 - rmse: 1.9369 - val_loss: 2.6789 - val_rmse: 1.4795\n",
      "Epoch 599/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 4.0391 - rmse: 1.9424 - val_loss: 2.7211 - val_rmse: 1.4917\n",
      "Epoch 600/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 3.9765 - rmse: 1.9284 - val_loss: 2.6154 - val_rmse: 1.4627\n",
      "Epoch 601/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 3.9885 - rmse: 1.9302 - val_loss: 2.7079 - val_rmse: 1.4887\n",
      "Epoch 602/1000\n",
      "7329/7329 [==============================] - 5s 697us/sample - loss: 4.0116 - rmse: 1.9366 - val_loss: 2.5427 - val_rmse: 1.4463\n",
      "Epoch 603/1000\n",
      "7329/7329 [==============================] - 5s 676us/sample - loss: 3.9736 - rmse: 1.9271 - val_loss: 2.7320 - val_rmse: 1.4901\n",
      "Epoch 604/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 3.9935 - rmse: 1.9326 - val_loss: 2.5563 - val_rmse: 1.4556\n",
      "Epoch 605/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 3.9839 - rmse: 1.9322 - val_loss: 2.6934 - val_rmse: 1.4825\n",
      "Epoch 606/1000\n",
      "7329/7329 [==============================] - 5s 680us/sample - loss: 3.9679 - rmse: 1.9252 - val_loss: 2.8557 - val_rmse: 1.5235\n",
      "Epoch 607/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 3.9283 - rmse: 1.9159 - val_loss: 2.5931 - val_rmse: 1.4541\n",
      "Epoch 608/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 3.9633 - rmse: 1.9246 - val_loss: 2.6810 - val_rmse: 1.4807\n",
      "Epoch 609/1000\n",
      "7329/7329 [==============================] - 5s 679us/sample - loss: 3.8854 - rmse: 1.9044 - val_loss: 2.7455 - val_rmse: 1.4994\n",
      "Epoch 610/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 3.9099 - rmse: 1.9117 - val_loss: 2.5761 - val_rmse: 1.4525\n",
      "Epoch 611/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 3.9425 - rmse: 1.9169 - val_loss: 2.6667 - val_rmse: 1.4755\n",
      "Epoch 612/1000\n",
      "7329/7329 [==============================] - 5s 683us/sample - loss: 3.8917 - rmse: 1.9075 - val_loss: 2.6054 - val_rmse: 1.4586\n",
      "Epoch 613/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 3.8848 - rmse: 1.9060 - val_loss: 2.5899 - val_rmse: 1.4570\n",
      "Epoch 614/1000\n",
      "7329/7329 [==============================] - 5s 694us/sample - loss: 3.8869 - rmse: 1.9070 - val_loss: 2.5510 - val_rmse: 1.4456\n",
      "Epoch 615/1000\n",
      "7329/7329 [==============================] - 5s 679us/sample - loss: 3.9048 - rmse: 1.9100 - val_loss: 2.7196 - val_rmse: 1.4884\n",
      "Epoch 616/1000\n",
      "7329/7329 [==============================] - 5s 681us/sample - loss: 3.9111 - rmse: 1.9116 - val_loss: 2.5729 - val_rmse: 1.4505\n",
      "Epoch 617/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 3.9086 - rmse: 1.9102 - val_loss: 2.5816 - val_rmse: 1.4536\n",
      "Epoch 618/1000\n",
      "7329/7329 [==============================] - 5s 684us/sample - loss: 3.8981 - rmse: 1.9077 - val_loss: 2.6231 - val_rmse: 1.4619\n",
      "Epoch 619/1000\n",
      "7329/7329 [==============================] - 5s 676us/sample - loss: 3.8956 - rmse: 1.9074 - val_loss: 2.7660 - val_rmse: 1.5028\n",
      "Epoch 620/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 3.8304 - rmse: 1.8931 - val_loss: 2.6484 - val_rmse: 1.4690\n",
      "Epoch 621/1000\n",
      "7329/7329 [==============================] - 5s 696us/sample - loss: 3.8995 - rmse: 1.9086 - val_loss: 2.5378 - val_rmse: 1.4410\n",
      "Epoch 622/1000\n",
      "7329/7329 [==============================] - 5s 676us/sample - loss: 3.8646 - rmse: 1.9006 - val_loss: 2.6034 - val_rmse: 1.4574\n",
      "Epoch 623/1000\n",
      "7329/7329 [==============================] - 5s 680us/sample - loss: 3.8478 - rmse: 1.8951 - val_loss: 2.6794 - val_rmse: 1.4761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 624/1000\n",
      "7329/7329 [==============================] - 5s 681us/sample - loss: 3.8788 - rmse: 1.9012 - val_loss: 2.6420 - val_rmse: 1.4693\n",
      "Epoch 625/1000\n",
      "7329/7329 [==============================] - 5s 675us/sample - loss: 3.8560 - rmse: 1.8961 - val_loss: 2.6254 - val_rmse: 1.4634\n",
      "Epoch 626/1000\n",
      "7329/7329 [==============================] - 5s 676us/sample - loss: 3.8311 - rmse: 1.8912 - val_loss: 2.7969 - val_rmse: 1.5083\n",
      "Epoch 627/1000\n",
      "7329/7329 [==============================] - 5s 676us/sample - loss: 3.8493 - rmse: 1.8968 - val_loss: 2.5942 - val_rmse: 1.4543\n",
      "Epoch 628/1000\n",
      "7329/7329 [==============================] - 5s 674us/sample - loss: 3.7932 - rmse: 1.8822 - val_loss: 2.8615 - val_rmse: 1.5376\n",
      "Epoch 629/1000\n",
      "7329/7329 [==============================] - 5s 676us/sample - loss: 3.8187 - rmse: 1.8873 - val_loss: 2.5829 - val_rmse: 1.4543\n",
      "Epoch 630/1000\n",
      "7329/7329 [==============================] - 5s 675us/sample - loss: 3.8243 - rmse: 1.8884 - val_loss: 2.6800 - val_rmse: 1.4786\n",
      "Epoch 631/1000\n",
      "7329/7329 [==============================] - 5s 675us/sample - loss: 3.8139 - rmse: 1.8859 - val_loss: 2.5722 - val_rmse: 1.4506\n",
      "Epoch 632/1000\n",
      "7329/7329 [==============================] - 5s 674us/sample - loss: 3.8070 - rmse: 1.8839 - val_loss: 2.6879 - val_rmse: 1.4800\n",
      "Epoch 633/1000\n",
      "7329/7329 [==============================] - 5s 682us/sample - loss: 3.8281 - rmse: 1.8882 - val_loss: 2.6578 - val_rmse: 1.4710\n",
      "Epoch 634/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 3.7892 - rmse: 1.8792 - val_loss: 2.6518 - val_rmse: 1.4720\n",
      "Epoch 635/1000\n",
      "7329/7329 [==============================] - 5s 676us/sample - loss: 3.8054 - rmse: 1.8836 - val_loss: 2.6132 - val_rmse: 1.4596\n",
      "Epoch 636/1000\n",
      "7329/7329 [==============================] - 5s 676us/sample - loss: 3.7684 - rmse: 1.8760 - val_loss: 2.5569 - val_rmse: 1.4515\n",
      "Epoch 637/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 3.7577 - rmse: 1.8722 - val_loss: 2.6003 - val_rmse: 1.4536\n",
      "Epoch 638/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 3.7414 - rmse: 1.8693 - val_loss: 2.5454 - val_rmse: 1.4470\n",
      "Epoch 639/1000\n",
      "7329/7329 [==============================] - 5s 675us/sample - loss: 3.8025 - rmse: 1.8811 - val_loss: 2.7544 - val_rmse: 1.4967\n",
      "Epoch 640/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 3.8182 - rmse: 1.8860 - val_loss: 2.6237 - val_rmse: 1.4611\n",
      "Epoch 641/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 3.7394 - rmse: 1.8686 - val_loss: 2.6119 - val_rmse: 1.4562\n",
      "Epoch 642/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 3.7902 - rmse: 1.8781 - val_loss: 2.5635 - val_rmse: 1.4554\n",
      "Epoch 643/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 3.7800 - rmse: 1.8766 - val_loss: 2.7537 - val_rmse: 1.5090\n",
      "Epoch 644/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 3.7222 - rmse: 1.8626 - val_loss: 2.6118 - val_rmse: 1.4584\n",
      "Epoch 645/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 3.7023 - rmse: 1.8580 - val_loss: 2.6111 - val_rmse: 1.4626\n",
      "Epoch 646/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 3.7314 - rmse: 1.8645 - val_loss: 2.7645 - val_rmse: 1.5056\n",
      "Epoch 647/1000\n",
      "7329/7329 [==============================] - 5s 676us/sample - loss: 3.7146 - rmse: 1.8601 - val_loss: 2.5467 - val_rmse: 1.4432\n",
      "Epoch 648/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 3.7380 - rmse: 1.8666 - val_loss: 2.5572 - val_rmse: 1.4520\n",
      "Epoch 649/1000\n",
      "7329/7329 [==============================] - 5s 697us/sample - loss: 3.6832 - rmse: 1.8544 - val_loss: 2.5079 - val_rmse: 1.4311\n",
      "Epoch 650/1000\n",
      "7329/7329 [==============================] - 5s 675us/sample - loss: 3.7059 - rmse: 1.8573 - val_loss: 2.5545 - val_rmse: 1.4454\n",
      "Epoch 651/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 3.7486 - rmse: 1.8661 - val_loss: 2.5638 - val_rmse: 1.4498\n",
      "Epoch 652/1000\n",
      "7329/7329 [==============================] - 5s 692us/sample - loss: 3.7015 - rmse: 1.8573 - val_loss: 2.4926 - val_rmse: 1.4279\n",
      "Epoch 653/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 3.6552 - rmse: 1.8463 - val_loss: 2.6034 - val_rmse: 1.4565\n",
      "Epoch 654/1000\n",
      "7329/7329 [==============================] - 5s 683us/sample - loss: 3.6595 - rmse: 1.8470 - val_loss: 2.5461 - val_rmse: 1.4468\n",
      "Epoch 655/1000\n",
      "7329/7329 [==============================] - 5s 676us/sample - loss: 3.7085 - rmse: 1.8569 - val_loss: 2.5387 - val_rmse: 1.4375\n",
      "Epoch 656/1000\n",
      "7329/7329 [==============================] - 5s 675us/sample - loss: 3.6888 - rmse: 1.8531 - val_loss: 2.5672 - val_rmse: 1.4467\n",
      "Epoch 657/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 3.6781 - rmse: 1.8512 - val_loss: 2.7058 - val_rmse: 1.4864\n",
      "Epoch 658/1000\n",
      "7329/7329 [==============================] - 5s 726us/sample - loss: 3.6876 - rmse: 1.8522 - val_loss: 2.5816 - val_rmse: 1.4520\n",
      "Epoch 659/1000\n",
      "7329/7329 [==============================] - 6s 762us/sample - loss: 3.7002 - rmse: 1.8559 - val_loss: 2.6735 - val_rmse: 1.4757\n",
      "Epoch 660/1000\n",
      "7329/7329 [==============================] - 5s 708us/sample - loss: 3.7096 - rmse: 1.8574 - val_loss: 2.5019 - val_rmse: 1.4342\n",
      "Epoch 661/1000\n",
      "7329/7329 [==============================] - 5s 684us/sample - loss: 3.6648 - rmse: 1.8461 - val_loss: 2.7507 - val_rmse: 1.5010\n",
      "Epoch 662/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 3.6187 - rmse: 1.8366 - val_loss: 2.5949 - val_rmse: 1.4509\n",
      "Epoch 663/1000\n",
      "7329/7329 [==============================] - 5s 687us/sample - loss: 3.6671 - rmse: 1.8435 - val_loss: 2.7799 - val_rmse: 1.5049\n",
      "Epoch 664/1000\n",
      "7329/7329 [==============================] - 5s 704us/sample - loss: 3.6102 - rmse: 1.8337 - val_loss: 2.4832 - val_rmse: 1.4255\n",
      "Epoch 665/1000\n",
      "7329/7329 [==============================] - 5s 681us/sample - loss: 3.6316 - rmse: 1.8356 - val_loss: 2.5823 - val_rmse: 1.4492\n",
      "Epoch 666/1000\n",
      "7329/7329 [==============================] - 5s 680us/sample - loss: 3.6141 - rmse: 1.8343 - val_loss: 2.7484 - val_rmse: 1.4927\n",
      "Epoch 667/1000\n",
      "7329/7329 [==============================] - 5s 680us/sample - loss: 3.6418 - rmse: 1.8411 - val_loss: 2.6020 - val_rmse: 1.4550\n",
      "Epoch 668/1000\n",
      "7329/7329 [==============================] - 5s 679us/sample - loss: 3.6195 - rmse: 1.8372 - val_loss: 2.6277 - val_rmse: 1.4585\n",
      "Epoch 669/1000\n",
      "7329/7329 [==============================] - 5s 680us/sample - loss: 3.6250 - rmse: 1.8378 - val_loss: 2.5505 - val_rmse: 1.4433\n",
      "Epoch 670/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 3.6287 - rmse: 1.8363 - val_loss: 2.6355 - val_rmse: 1.4617\n",
      "Epoch 671/1000\n",
      "7329/7329 [==============================] - 5s 687us/sample - loss: 3.5802 - rmse: 1.8275 - val_loss: 2.5264 - val_rmse: 1.4368\n",
      "Epoch 672/1000\n",
      "7329/7329 [==============================] - 5s 682us/sample - loss: 3.5751 - rmse: 1.8233 - val_loss: 2.5715 - val_rmse: 1.4453\n",
      "Epoch 673/1000\n",
      "7329/7329 [==============================] - 5s 683us/sample - loss: 3.6201 - rmse: 1.8332 - val_loss: 2.6558 - val_rmse: 1.4640\n",
      "Epoch 674/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 3.6164 - rmse: 1.8326 - val_loss: 2.5808 - val_rmse: 1.4647\n",
      "Epoch 675/1000\n",
      "7329/7329 [==============================] - 5s 679us/sample - loss: 3.6473 - rmse: 1.8395 - val_loss: 2.5869 - val_rmse: 1.4486\n",
      "Epoch 676/1000\n",
      "7329/7329 [==============================] - 5s 680us/sample - loss: 3.5979 - rmse: 1.8294 - val_loss: 2.5838 - val_rmse: 1.4503\n",
      "Epoch 677/1000\n",
      "7329/7329 [==============================] - 5s 680us/sample - loss: 3.5537 - rmse: 1.8184 - val_loss: 2.6254 - val_rmse: 1.4595\n",
      "Epoch 678/1000\n",
      "7329/7329 [==============================] - 5s 682us/sample - loss: 3.5650 - rmse: 1.8204 - val_loss: 2.5575 - val_rmse: 1.4407\n",
      "Epoch 679/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 3.6097 - rmse: 1.8309 - val_loss: 2.6351 - val_rmse: 1.4601\n",
      "Epoch 680/1000\n",
      "7329/7329 [==============================] - 5s 680us/sample - loss: 3.5878 - rmse: 1.8233 - val_loss: 2.7273 - val_rmse: 1.4914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 681/1000\n",
      "7329/7329 [==============================] - 5s 681us/sample - loss: 3.5327 - rmse: 1.8127 - val_loss: 2.5616 - val_rmse: 1.4489\n",
      "Epoch 682/1000\n",
      "7329/7329 [==============================] - 5s 679us/sample - loss: 3.6130 - rmse: 1.8340 - val_loss: 2.5792 - val_rmse: 1.4475\n",
      "Epoch 683/1000\n",
      "7329/7329 [==============================] - 5s 679us/sample - loss: 3.5666 - rmse: 1.8195 - val_loss: 2.6334 - val_rmse: 1.4628\n",
      "Epoch 684/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 3.4983 - rmse: 1.8058 - val_loss: 2.5447 - val_rmse: 1.4408\n",
      "Epoch 685/1000\n",
      "7329/7329 [==============================] - 5s 682us/sample - loss: 3.5845 - rmse: 1.8244 - val_loss: 2.5095 - val_rmse: 1.4360\n",
      "Epoch 686/1000\n",
      "7329/7329 [==============================] - 5s 687us/sample - loss: 3.5294 - rmse: 1.8108 - val_loss: 2.5496 - val_rmse: 1.4395\n",
      "Epoch 687/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 3.5403 - rmse: 1.8137 - val_loss: 2.5732 - val_rmse: 1.4478\n",
      "Epoch 688/1000\n",
      "7329/7329 [==============================] - 5s 683us/sample - loss: 3.5749 - rmse: 1.8199 - val_loss: 2.6006 - val_rmse: 1.4552\n",
      "Epoch 689/1000\n",
      "7329/7329 [==============================] - 5s 679us/sample - loss: 3.5428 - rmse: 1.8135 - val_loss: 2.6569 - val_rmse: 1.4674\n",
      "Epoch 690/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 3.5304 - rmse: 1.8095 - val_loss: 2.5746 - val_rmse: 1.4484\n",
      "Epoch 691/1000\n",
      "7329/7329 [==============================] - 5s 679us/sample - loss: 3.4743 - rmse: 1.7972 - val_loss: 2.5595 - val_rmse: 1.4422\n",
      "Epoch 692/1000\n",
      "7329/7329 [==============================] - 5s 680us/sample - loss: 3.5178 - rmse: 1.8075 - val_loss: 2.4921 - val_rmse: 1.4264\n",
      "Epoch 693/1000\n",
      "7329/7329 [==============================] - 5s 693us/sample - loss: 3.4564 - rmse: 1.7929 - val_loss: 2.4836 - val_rmse: 1.4240\n",
      "Epoch 694/1000\n",
      "7329/7329 [==============================] - 5s 680us/sample - loss: 3.5238 - rmse: 1.8079 - val_loss: 2.6545 - val_rmse: 1.4655\n",
      "Epoch 695/1000\n",
      "7329/7329 [==============================] - 5s 679us/sample - loss: 3.5045 - rmse: 1.8023 - val_loss: 2.5922 - val_rmse: 1.4525\n",
      "Epoch 696/1000\n",
      "7329/7329 [==============================] - 5s 679us/sample - loss: 3.4575 - rmse: 1.7941 - val_loss: 2.5141 - val_rmse: 1.4345\n",
      "Epoch 697/1000\n",
      "7329/7329 [==============================] - 5s 681us/sample - loss: 3.4813 - rmse: 1.7978 - val_loss: 2.5359 - val_rmse: 1.4371\n",
      "Epoch 698/1000\n",
      "7329/7329 [==============================] - 5s 681us/sample - loss: 3.5151 - rmse: 1.8054 - val_loss: 2.6451 - val_rmse: 1.4665\n",
      "Epoch 699/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 3.4721 - rmse: 1.7952 - val_loss: 2.5899 - val_rmse: 1.4518\n",
      "Epoch 700/1000\n",
      "7329/7329 [==============================] - 5s 676us/sample - loss: 3.4746 - rmse: 1.7979 - val_loss: 2.6004 - val_rmse: 1.4527\n",
      "Epoch 701/1000\n",
      "7329/7329 [==============================] - 5s 680us/sample - loss: 3.4485 - rmse: 1.7899 - val_loss: 2.5819 - val_rmse: 1.4491\n",
      "Epoch 702/1000\n",
      "7329/7329 [==============================] - 5s 683us/sample - loss: 3.4530 - rmse: 1.7899 - val_loss: 2.5172 - val_rmse: 1.4318\n",
      "Epoch 703/1000\n",
      "7329/7329 [==============================] - 5s 696us/sample - loss: 3.4134 - rmse: 1.7824 - val_loss: 2.4616 - val_rmse: 1.4187\n",
      "Epoch 704/1000\n",
      "7329/7329 [==============================] - 5s 681us/sample - loss: 3.5021 - rmse: 1.8021 - val_loss: 2.5814 - val_rmse: 1.4441\n",
      "Epoch 705/1000\n",
      "7329/7329 [==============================] - 5s 679us/sample - loss: 3.4604 - rmse: 1.7896 - val_loss: 2.6775 - val_rmse: 1.4717\n",
      "Epoch 706/1000\n",
      "7329/7329 [==============================] - 5s 679us/sample - loss: 3.4243 - rmse: 1.7839 - val_loss: 2.6677 - val_rmse: 1.4696\n",
      "Epoch 707/1000\n",
      "7329/7329 [==============================] - 5s 676us/sample - loss: 3.4474 - rmse: 1.7888 - val_loss: 2.5156 - val_rmse: 1.4303\n",
      "Epoch 708/1000\n",
      "7329/7329 [==============================] - 5s 681us/sample - loss: 3.4423 - rmse: 1.7864 - val_loss: 2.5065 - val_rmse: 1.4291\n",
      "Epoch 709/1000\n",
      "7329/7329 [==============================] - 5s 680us/sample - loss: 3.4351 - rmse: 1.7838 - val_loss: 2.5725 - val_rmse: 1.4438\n",
      "Epoch 710/1000\n",
      "7329/7329 [==============================] - 5s 680us/sample - loss: 3.4104 - rmse: 1.7809 - val_loss: 2.4865 - val_rmse: 1.4245\n",
      "Epoch 711/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 3.4224 - rmse: 1.7832 - val_loss: 2.5184 - val_rmse: 1.4331\n",
      "Epoch 712/1000\n",
      "7329/7329 [==============================] - 5s 682us/sample - loss: 3.4245 - rmse: 1.7830 - val_loss: 2.4720 - val_rmse: 1.4199\n",
      "Epoch 713/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 3.4126 - rmse: 1.7795 - val_loss: 2.6432 - val_rmse: 1.4681\n",
      "Epoch 714/1000\n",
      "7329/7329 [==============================] - 5s 679us/sample - loss: 3.4391 - rmse: 1.7845 - val_loss: 2.5080 - val_rmse: 1.4272\n",
      "Epoch 715/1000\n",
      "7329/7329 [==============================] - 5s 681us/sample - loss: 3.4189 - rmse: 1.7816 - val_loss: 2.7140 - val_rmse: 1.4853\n",
      "Epoch 716/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 3.3746 - rmse: 1.7706 - val_loss: 2.5260 - val_rmse: 1.4300\n",
      "Epoch 717/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 3.4188 - rmse: 1.7810 - val_loss: 2.5382 - val_rmse: 1.4343\n",
      "Epoch 718/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 3.3971 - rmse: 1.7748 - val_loss: 2.6006 - val_rmse: 1.4545\n",
      "Epoch 719/1000\n",
      "7329/7329 [==============================] - 5s 679us/sample - loss: 3.4240 - rmse: 1.7804 - val_loss: 2.5210 - val_rmse: 1.4340\n",
      "Epoch 720/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 3.3711 - rmse: 1.7680 - val_loss: 2.5054 - val_rmse: 1.4248\n",
      "Epoch 721/1000\n",
      "7329/7329 [==============================] - 5s 680us/sample - loss: 3.4008 - rmse: 1.7770 - val_loss: 2.5693 - val_rmse: 1.4436\n",
      "Epoch 722/1000\n",
      "7329/7329 [==============================] - 5s 699us/sample - loss: 3.4194 - rmse: 1.7797 - val_loss: 2.4655 - val_rmse: 1.4169\n",
      "Epoch 723/1000\n",
      "7329/7329 [==============================] - 5s 679us/sample - loss: 3.3928 - rmse: 1.7748 - val_loss: 2.6014 - val_rmse: 1.4502\n",
      "Epoch 724/1000\n",
      "7329/7329 [==============================] - 5s 676us/sample - loss: 3.3822 - rmse: 1.7715 - val_loss: 2.5184 - val_rmse: 1.4320\n",
      "Epoch 725/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 3.4102 - rmse: 1.7763 - val_loss: 2.5945 - val_rmse: 1.4503\n",
      "Epoch 726/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 3.3309 - rmse: 1.7565 - val_loss: 2.5950 - val_rmse: 1.4496\n",
      "Epoch 727/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 3.3545 - rmse: 1.7626 - val_loss: 2.5309 - val_rmse: 1.4416\n",
      "Epoch 728/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 3.3911 - rmse: 1.7720 - val_loss: 2.6119 - val_rmse: 1.4516\n",
      "Epoch 729/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 3.3571 - rmse: 1.7645 - val_loss: 2.5795 - val_rmse: 1.4434\n",
      "Epoch 730/1000\n",
      "7329/7329 [==============================] - 5s 680us/sample - loss: 3.3370 - rmse: 1.7590 - val_loss: 2.4827 - val_rmse: 1.4187\n",
      "Epoch 731/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 3.3286 - rmse: 1.7571 - val_loss: 2.5757 - val_rmse: 1.4394\n",
      "Epoch 732/1000\n",
      "7329/7329 [==============================] - 5s 692us/sample - loss: 3.3113 - rmse: 1.7512 - val_loss: 2.4403 - val_rmse: 1.4099\n",
      "Epoch 733/1000\n",
      "7329/7329 [==============================] - 5s 679us/sample - loss: 3.3640 - rmse: 1.7632 - val_loss: 2.5798 - val_rmse: 1.4437\n",
      "Epoch 734/1000\n",
      "7329/7329 [==============================] - 5s 679us/sample - loss: 3.2916 - rmse: 1.7472 - val_loss: 2.5608 - val_rmse: 1.4376\n",
      "Epoch 735/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 3.3257 - rmse: 1.7555 - val_loss: 2.5281 - val_rmse: 1.4320\n",
      "Epoch 736/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 3.3165 - rmse: 1.7539 - val_loss: 2.6070 - val_rmse: 1.4482\n",
      "Epoch 737/1000\n",
      "7329/7329 [==============================] - 5s 683us/sample - loss: 3.3051 - rmse: 1.7505 - val_loss: 2.4661 - val_rmse: 1.4277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 738/1000\n",
      "7329/7329 [==============================] - 5s 687us/sample - loss: 3.2867 - rmse: 1.7460 - val_loss: 2.5287 - val_rmse: 1.4320\n",
      "Epoch 739/1000\n",
      "7329/7329 [==============================] - 5s 675us/sample - loss: 3.3181 - rmse: 1.7527 - val_loss: 2.4486 - val_rmse: 1.4246\n",
      "Epoch 740/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 3.3114 - rmse: 1.7516 - val_loss: 2.5005 - val_rmse: 1.4273\n",
      "Epoch 741/1000\n",
      "7329/7329 [==============================] - 5s 679us/sample - loss: 3.3565 - rmse: 1.7623 - val_loss: 2.5856 - val_rmse: 1.4458\n",
      "Epoch 742/1000\n",
      "7329/7329 [==============================] - 5s 676us/sample - loss: 3.3155 - rmse: 1.7529 - val_loss: 2.4554 - val_rmse: 1.4121\n",
      "Epoch 743/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 3.2895 - rmse: 1.7436 - val_loss: 2.5172 - val_rmse: 1.4264\n",
      "Epoch 744/1000\n",
      "7329/7329 [==============================] - 5s 674us/sample - loss: 3.2922 - rmse: 1.7463 - val_loss: 2.5132 - val_rmse: 1.4245\n",
      "Epoch 745/1000\n",
      "7329/7329 [==============================] - 5s 681us/sample - loss: 3.3037 - rmse: 1.7480 - val_loss: 2.5982 - val_rmse: 1.4446\n",
      "Epoch 746/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 3.2583 - rmse: 1.7372 - val_loss: 2.7554 - val_rmse: 1.5014\n",
      "Epoch 747/1000\n",
      "7329/7329 [==============================] - 5s 676us/sample - loss: 3.2960 - rmse: 1.7454 - val_loss: 2.5238 - val_rmse: 1.4255\n",
      "Epoch 748/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 3.2607 - rmse: 1.7372 - val_loss: 2.4870 - val_rmse: 1.4181\n",
      "Epoch 749/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 3.2680 - rmse: 1.7390 - val_loss: 2.7267 - val_rmse: 1.4834\n",
      "Epoch 750/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 3.2842 - rmse: 1.7440 - val_loss: 2.6381 - val_rmse: 1.4682\n",
      "Epoch 751/1000\n",
      "7329/7329 [==============================] - 5s 676us/sample - loss: 3.2719 - rmse: 1.7420 - val_loss: 2.4737 - val_rmse: 1.4140\n",
      "Epoch 752/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 3.2919 - rmse: 1.7452 - val_loss: 2.6000 - val_rmse: 1.4464\n",
      "Epoch 753/1000\n",
      "7329/7329 [==============================] - 5s 679us/sample - loss: 3.2618 - rmse: 1.7369 - val_loss: 2.5843 - val_rmse: 1.4396\n",
      "Epoch 754/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 3.2774 - rmse: 1.7410 - val_loss: 2.5125 - val_rmse: 1.4247\n",
      "Epoch 755/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 3.2827 - rmse: 1.7426 - val_loss: 2.4771 - val_rmse: 1.4201\n",
      "Epoch 756/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 3.2627 - rmse: 1.7348 - val_loss: 2.5159 - val_rmse: 1.4311\n",
      "Epoch 757/1000\n",
      "7329/7329 [==============================] - 5s 696us/sample - loss: 3.2892 - rmse: 1.7418 - val_loss: 2.4330 - val_rmse: 1.4043\n",
      "Epoch 758/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 3.2431 - rmse: 1.7337 - val_loss: 2.5397 - val_rmse: 1.4315\n",
      "Epoch 759/1000\n",
      "7329/7329 [==============================] - 5s 676us/sample - loss: 3.2684 - rmse: 1.7377 - val_loss: 2.5261 - val_rmse: 1.4278\n",
      "Epoch 760/1000\n",
      "7329/7329 [==============================] - 5s 676us/sample - loss: 3.2335 - rmse: 1.7297 - val_loss: 2.5605 - val_rmse: 1.4396\n",
      "Epoch 761/1000\n",
      "7329/7329 [==============================] - 5s 682us/sample - loss: 3.2450 - rmse: 1.7322 - val_loss: 2.4602 - val_rmse: 1.4140\n",
      "Epoch 762/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 3.2816 - rmse: 1.7390 - val_loss: 2.4838 - val_rmse: 1.4183\n",
      "Epoch 763/1000\n",
      "7329/7329 [==============================] - 5s 675us/sample - loss: 3.2336 - rmse: 1.7277 - val_loss: 2.5023 - val_rmse: 1.4244\n",
      "Epoch 764/1000\n",
      "7329/7329 [==============================] - 5s 679us/sample - loss: 3.1962 - rmse: 1.7207 - val_loss: 2.9096 - val_rmse: 1.5499\n",
      "Epoch 765/1000\n",
      "7329/7329 [==============================] - 5s 679us/sample - loss: 3.2460 - rmse: 1.7317 - val_loss: 2.4991 - val_rmse: 1.4205\n",
      "Epoch 766/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 3.2627 - rmse: 1.7364 - val_loss: 2.5558 - val_rmse: 1.4359\n",
      "Epoch 767/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 3.1966 - rmse: 1.7196 - val_loss: 2.4742 - val_rmse: 1.4147\n",
      "Epoch 768/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 3.1989 - rmse: 1.7197 - val_loss: 2.6884 - val_rmse: 1.4743\n",
      "Epoch 769/1000\n",
      "7329/7329 [==============================] - 5s 679us/sample - loss: 3.1807 - rmse: 1.7148 - val_loss: 2.5341 - val_rmse: 1.4299\n",
      "Epoch 770/1000\n",
      "7329/7329 [==============================] - 5s 676us/sample - loss: 3.2366 - rmse: 1.7254 - val_loss: 2.5345 - val_rmse: 1.4345\n",
      "Epoch 771/1000\n",
      "7329/7329 [==============================] - 5s 680us/sample - loss: 3.1980 - rmse: 1.7155 - val_loss: 2.5702 - val_rmse: 1.4410\n",
      "Epoch 772/1000\n",
      "7329/7329 [==============================] - 5s 694us/sample - loss: 3.2180 - rmse: 1.7250 - val_loss: 2.4006 - val_rmse: 1.3994\n",
      "Epoch 773/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 3.1961 - rmse: 1.7196 - val_loss: 2.4624 - val_rmse: 1.4111\n",
      "Epoch 774/1000\n",
      "7329/7329 [==============================] - 5s 683us/sample - loss: 3.1694 - rmse: 1.7121 - val_loss: 2.4545 - val_rmse: 1.4138\n",
      "Epoch 775/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 3.1802 - rmse: 1.7134 - val_loss: 2.4731 - val_rmse: 1.4149\n",
      "Epoch 776/1000\n",
      "7329/7329 [==============================] - 5s 675us/sample - loss: 3.1802 - rmse: 1.7137 - val_loss: 2.4865 - val_rmse: 1.4207\n",
      "Epoch 777/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 3.1638 - rmse: 1.7080 - val_loss: 2.5414 - val_rmse: 1.4333\n",
      "Epoch 778/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 3.1766 - rmse: 1.7141 - val_loss: 2.5038 - val_rmse: 1.4205\n",
      "Epoch 779/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 3.1417 - rmse: 1.7041 - val_loss: 2.7804 - val_rmse: 1.5059\n",
      "Epoch 780/1000\n",
      "7329/7329 [==============================] - 5s 680us/sample - loss: 3.2016 - rmse: 1.7176 - val_loss: 2.5474 - val_rmse: 1.4333\n",
      "Epoch 781/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 3.1645 - rmse: 1.7100 - val_loss: 2.4577 - val_rmse: 1.4122\n",
      "Epoch 782/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 3.1228 - rmse: 1.6991 - val_loss: 2.5386 - val_rmse: 1.4321\n",
      "Epoch 783/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 3.1753 - rmse: 1.7112 - val_loss: 2.4613 - val_rmse: 1.4182\n",
      "Epoch 784/1000\n",
      "7329/7329 [==============================] - 5s 676us/sample - loss: 3.1822 - rmse: 1.7112 - val_loss: 2.6242 - val_rmse: 1.4530\n",
      "Epoch 785/1000\n",
      "7329/7329 [==============================] - 5s 682us/sample - loss: 3.1721 - rmse: 1.7097 - val_loss: 2.4840 - val_rmse: 1.4187\n",
      "Epoch 786/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 3.1708 - rmse: 1.7098 - val_loss: 2.4067 - val_rmse: 1.4068\n",
      "Epoch 787/1000\n",
      "7329/7329 [==============================] - 5s 682us/sample - loss: 3.1502 - rmse: 1.7044 - val_loss: 2.4523 - val_rmse: 1.4106\n",
      "Epoch 788/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 3.1202 - rmse: 1.6966 - val_loss: 2.4178 - val_rmse: 1.4005\n",
      "Epoch 789/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 3.1202 - rmse: 1.6945 - val_loss: 2.4999 - val_rmse: 1.4199\n",
      "Epoch 790/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 3.1505 - rmse: 1.7049 - val_loss: 2.5701 - val_rmse: 1.4402\n",
      "Epoch 791/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 3.1046 - rmse: 1.6929 - val_loss: 2.4954 - val_rmse: 1.4202\n",
      "Epoch 792/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 3.0975 - rmse: 1.6894 - val_loss: 2.5416 - val_rmse: 1.4377\n",
      "Epoch 793/1000\n",
      "7329/7329 [==============================] - 5s 679us/sample - loss: 3.1709 - rmse: 1.7095 - val_loss: 2.5474 - val_rmse: 1.4328\n",
      "Epoch 794/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 3.0994 - rmse: 1.6917 - val_loss: 2.4473 - val_rmse: 1.4169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 795/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 3.1115 - rmse: 1.6927 - val_loss: 2.5581 - val_rmse: 1.4365\n",
      "Epoch 796/1000\n",
      "7329/7329 [==============================] - 5s 676us/sample - loss: 3.1507 - rmse: 1.7020 - val_loss: 2.4681 - val_rmse: 1.4105\n",
      "Epoch 797/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 3.1234 - rmse: 1.6961 - val_loss: 2.4548 - val_rmse: 1.4108\n",
      "Epoch 798/1000\n",
      "7329/7329 [==============================] - 5s 682us/sample - loss: 3.1219 - rmse: 1.6957 - val_loss: 2.4836 - val_rmse: 1.4171\n",
      "Epoch 799/1000\n",
      "7329/7329 [==============================] - 5s 683us/sample - loss: 3.1100 - rmse: 1.6941 - val_loss: 2.5608 - val_rmse: 1.4333\n",
      "Epoch 800/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 3.1380 - rmse: 1.7006 - val_loss: 2.4571 - val_rmse: 1.4113\n",
      "Epoch 801/1000\n",
      "7329/7329 [==============================] - 5s 676us/sample - loss: 3.0980 - rmse: 1.6900 - val_loss: 2.5906 - val_rmse: 1.4426\n",
      "Epoch 802/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 3.1147 - rmse: 1.6960 - val_loss: 2.4790 - val_rmse: 1.4210\n",
      "Epoch 803/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 3.1048 - rmse: 1.6900 - val_loss: 2.4274 - val_rmse: 1.4031\n",
      "Epoch 804/1000\n",
      "7329/7329 [==============================] - 5s 679us/sample - loss: 3.0922 - rmse: 1.6911 - val_loss: 2.5456 - val_rmse: 1.4347\n",
      "Epoch 805/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 3.1191 - rmse: 1.6955 - val_loss: 2.5194 - val_rmse: 1.4301\n",
      "Epoch 806/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 3.1167 - rmse: 1.6930 - val_loss: 2.4351 - val_rmse: 1.4078\n",
      "Epoch 807/1000\n",
      "7329/7329 [==============================] - 5s 676us/sample - loss: 3.0953 - rmse: 1.6866 - val_loss: 2.5368 - val_rmse: 1.4353\n",
      "Epoch 808/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 3.0885 - rmse: 1.6882 - val_loss: 2.4179 - val_rmse: 1.4012\n",
      "Epoch 809/1000\n",
      "7329/7329 [==============================] - 5s 681us/sample - loss: 3.0793 - rmse: 1.6847 - val_loss: 2.4215 - val_rmse: 1.4049\n",
      "Epoch 810/1000\n",
      "7329/7329 [==============================] - 5s 676us/sample - loss: 3.1007 - rmse: 1.6889 - val_loss: 2.4368 - val_rmse: 1.4015\n",
      "Epoch 811/1000\n",
      "7329/7329 [==============================] - 5s 681us/sample - loss: 3.0961 - rmse: 1.6885 - val_loss: 2.4605 - val_rmse: 1.4081\n",
      "Epoch 812/1000\n",
      "7329/7329 [==============================] - 5s 676us/sample - loss: 3.0465 - rmse: 1.6758 - val_loss: 2.6199 - val_rmse: 1.4525\n",
      "Epoch 813/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 3.0220 - rmse: 1.6718 - val_loss: 2.5525 - val_rmse: 1.4380\n",
      "Epoch 814/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 3.0717 - rmse: 1.6830 - val_loss: 2.4608 - val_rmse: 1.4105\n",
      "Epoch 815/1000\n",
      "7329/7329 [==============================] - 5s 691us/sample - loss: 3.0638 - rmse: 1.6786 - val_loss: 2.4163 - val_rmse: 1.3993\n",
      "Epoch 816/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 3.0529 - rmse: 1.6768 - val_loss: 2.5684 - val_rmse: 1.4395\n",
      "Epoch 817/1000\n",
      "7329/7329 [==============================] - 5s 684us/sample - loss: 3.0933 - rmse: 1.6867 - val_loss: 2.5003 - val_rmse: 1.4205\n",
      "Epoch 818/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 3.0031 - rmse: 1.6639 - val_loss: 2.4433 - val_rmse: 1.4043\n",
      "Epoch 819/1000\n",
      "7329/7329 [==============================] - 5s 693us/sample - loss: 3.0396 - rmse: 1.6744 - val_loss: 2.3988 - val_rmse: 1.3963\n",
      "Epoch 820/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 3.0459 - rmse: 1.6741 - val_loss: 2.4421 - val_rmse: 1.4052\n",
      "Epoch 821/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 3.0831 - rmse: 1.6821 - val_loss: 2.4632 - val_rmse: 1.4091\n",
      "Epoch 822/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 3.0158 - rmse: 1.6676 - val_loss: 2.4545 - val_rmse: 1.4070\n",
      "Epoch 823/1000\n",
      "7329/7329 [==============================] - 5s 695us/sample - loss: 3.0492 - rmse: 1.6754 - val_loss: 2.3809 - val_rmse: 1.3913\n",
      "Epoch 824/1000\n",
      "7329/7329 [==============================] - 5s 681us/sample - loss: 3.0461 - rmse: 1.6723 - val_loss: 2.4468 - val_rmse: 1.4037\n",
      "Epoch 825/1000\n",
      "7329/7329 [==============================] - 5s 676us/sample - loss: 3.0431 - rmse: 1.6721 - val_loss: 2.5225 - val_rmse: 1.4281\n",
      "Epoch 826/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 3.0467 - rmse: 1.6740 - val_loss: 2.4422 - val_rmse: 1.4054\n",
      "Epoch 827/1000\n",
      "7329/7329 [==============================] - 5s 676us/sample - loss: 3.0239 - rmse: 1.6675 - val_loss: 2.4820 - val_rmse: 1.4146\n",
      "Epoch 828/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 2.9790 - rmse: 1.6568 - val_loss: 2.4018 - val_rmse: 1.3941\n",
      "Epoch 829/1000\n",
      "7329/7329 [==============================] - 5s 685us/sample - loss: 3.0334 - rmse: 1.6681 - val_loss: 2.5057 - val_rmse: 1.4190\n",
      "Epoch 830/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 2.9992 - rmse: 1.6608 - val_loss: 2.4900 - val_rmse: 1.4166\n",
      "Epoch 831/1000\n",
      "7329/7329 [==============================] - 5s 680us/sample - loss: 3.0205 - rmse: 1.6685 - val_loss: 2.4445 - val_rmse: 1.4045\n",
      "Epoch 832/1000\n",
      "7329/7329 [==============================] - 5s 676us/sample - loss: 2.9999 - rmse: 1.6606 - val_loss: 2.6185 - val_rmse: 1.4532\n",
      "Epoch 833/1000\n",
      "7329/7329 [==============================] - 5s 683us/sample - loss: 2.9924 - rmse: 1.6578 - val_loss: 2.4136 - val_rmse: 1.4002\n",
      "Epoch 834/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 2.9930 - rmse: 1.6597 - val_loss: 2.4904 - val_rmse: 1.4148\n",
      "Epoch 835/1000\n",
      "7329/7329 [==============================] - 5s 681us/sample - loss: 3.0262 - rmse: 1.6690 - val_loss: 2.5146 - val_rmse: 1.4209\n",
      "Epoch 836/1000\n",
      "7329/7329 [==============================] - 5s 679us/sample - loss: 3.0348 - rmse: 1.6678 - val_loss: 2.4551 - val_rmse: 1.4113\n",
      "Epoch 837/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 3.0181 - rmse: 1.6655 - val_loss: 2.4642 - val_rmse: 1.4079\n",
      "Epoch 838/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 2.9740 - rmse: 1.6548 - val_loss: 2.5176 - val_rmse: 1.4267\n",
      "Epoch 839/1000\n",
      "7329/7329 [==============================] - 5s 694us/sample - loss: 2.9780 - rmse: 1.6542 - val_loss: 2.3865 - val_rmse: 1.3904\n",
      "Epoch 840/1000\n",
      "7329/7329 [==============================] - 5s 681us/sample - loss: 2.9783 - rmse: 1.6544 - val_loss: 2.4472 - val_rmse: 1.4060\n",
      "Epoch 841/1000\n",
      "7329/7329 [==============================] - 5s 681us/sample - loss: 3.0269 - rmse: 1.6667 - val_loss: 2.4295 - val_rmse: 1.3986\n",
      "Epoch 842/1000\n",
      "7329/7329 [==============================] - 5s 681us/sample - loss: 2.9826 - rmse: 1.6530 - val_loss: 2.5179 - val_rmse: 1.4210\n",
      "Epoch 843/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 2.9895 - rmse: 1.6543 - val_loss: 2.5272 - val_rmse: 1.4250\n",
      "Epoch 844/1000\n",
      "7329/7329 [==============================] - 5s 679us/sample - loss: 2.9629 - rmse: 1.6516 - val_loss: 2.5488 - val_rmse: 1.4292\n",
      "Epoch 845/1000\n",
      "7329/7329 [==============================] - 5s 679us/sample - loss: 2.9944 - rmse: 1.6569 - val_loss: 2.3941 - val_rmse: 1.3906\n",
      "Epoch 846/1000\n",
      "7329/7329 [==============================] - 5s 695us/sample - loss: 2.9650 - rmse: 1.6487 - val_loss: 2.3472 - val_rmse: 1.3812\n",
      "Epoch 847/1000\n",
      "7329/7329 [==============================] - 5s 682us/sample - loss: 2.9498 - rmse: 1.6478 - val_loss: 2.5044 - val_rmse: 1.4200\n",
      "Epoch 848/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 2.9275 - rmse: 1.6412 - val_loss: 2.4451 - val_rmse: 1.4021\n",
      "Epoch 849/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 2.9637 - rmse: 1.6514 - val_loss: 2.4459 - val_rmse: 1.4117\n",
      "Epoch 850/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 2.9481 - rmse: 1.6480 - val_loss: 2.4426 - val_rmse: 1.4040\n",
      "Epoch 851/1000\n",
      "7329/7329 [==============================] - 5s 676us/sample - loss: 2.9528 - rmse: 1.6487 - val_loss: 2.4748 - val_rmse: 1.4119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 852/1000\n",
      "7329/7329 [==============================] - 5s 679us/sample - loss: 2.9085 - rmse: 1.6336 - val_loss: 2.4683 - val_rmse: 1.4090\n",
      "Epoch 853/1000\n",
      "7329/7329 [==============================] - 5s 675us/sample - loss: 2.9591 - rmse: 1.6497 - val_loss: 2.4449 - val_rmse: 1.4061\n",
      "Epoch 854/1000\n",
      "7329/7329 [==============================] - 5s 679us/sample - loss: 2.9727 - rmse: 1.6523 - val_loss: 2.5306 - val_rmse: 1.4236\n",
      "Epoch 855/1000\n",
      "7329/7329 [==============================] - 5s 676us/sample - loss: 2.8719 - rmse: 1.6265 - val_loss: 2.4939 - val_rmse: 1.4177\n",
      "Epoch 856/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 2.9177 - rmse: 1.6388 - val_loss: 2.3686 - val_rmse: 1.3863\n",
      "Epoch 857/1000\n",
      "7329/7329 [==============================] - 5s 676us/sample - loss: 2.9531 - rmse: 1.6478 - val_loss: 2.3929 - val_rmse: 1.3884\n",
      "Epoch 858/1000\n",
      "7329/7329 [==============================] - 5s 687us/sample - loss: 2.9764 - rmse: 1.6504 - val_loss: 2.4537 - val_rmse: 1.4038\n",
      "Epoch 859/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 2.9219 - rmse: 1.6392 - val_loss: 2.5446 - val_rmse: 1.4272\n",
      "Epoch 860/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 2.9286 - rmse: 1.6411 - val_loss: 2.4735 - val_rmse: 1.4099\n",
      "Epoch 861/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 2.9514 - rmse: 1.6451 - val_loss: 2.3981 - val_rmse: 1.3874\n",
      "Epoch 862/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 2.9376 - rmse: 1.6428 - val_loss: 2.4314 - val_rmse: 1.3968\n",
      "Epoch 863/1000\n",
      "7329/7329 [==============================] - 5s 676us/sample - loss: 2.9067 - rmse: 1.6319 - val_loss: 2.4769 - val_rmse: 1.4074\n",
      "Epoch 864/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 2.9201 - rmse: 1.6374 - val_loss: 2.5107 - val_rmse: 1.4175\n",
      "Epoch 865/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 2.9218 - rmse: 1.6364 - val_loss: 2.5146 - val_rmse: 1.4190\n",
      "Epoch 866/1000\n",
      "7329/7329 [==============================] - 5s 679us/sample - loss: 2.9385 - rmse: 1.6409 - val_loss: 2.4617 - val_rmse: 1.4054\n",
      "Epoch 867/1000\n",
      "7329/7329 [==============================] - 5s 676us/sample - loss: 2.9252 - rmse: 1.6386 - val_loss: 2.4423 - val_rmse: 1.4020\n",
      "Epoch 868/1000\n",
      "7329/7329 [==============================] - 5s 676us/sample - loss: 2.8788 - rmse: 1.6275 - val_loss: 2.4567 - val_rmse: 1.4073\n",
      "Epoch 869/1000\n",
      "7329/7329 [==============================] - 5s 676us/sample - loss: 2.9270 - rmse: 1.6363 - val_loss: 2.4481 - val_rmse: 1.4024\n",
      "Epoch 870/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 2.8769 - rmse: 1.6263 - val_loss: 2.4740 - val_rmse: 1.4161\n",
      "Epoch 871/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 2.8707 - rmse: 1.6250 - val_loss: 2.4814 - val_rmse: 1.4115\n",
      "Epoch 872/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 2.8930 - rmse: 1.6309 - val_loss: 2.5492 - val_rmse: 1.4306\n",
      "Epoch 873/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 2.9117 - rmse: 1.6344 - val_loss: 2.4535 - val_rmse: 1.4017\n",
      "Epoch 874/1000\n",
      "7329/7329 [==============================] - 5s 676us/sample - loss: 2.9100 - rmse: 1.6330 - val_loss: 2.4599 - val_rmse: 1.4037\n",
      "Epoch 875/1000\n",
      "7329/7329 [==============================] - 5s 680us/sample - loss: 2.8746 - rmse: 1.6245 - val_loss: 2.4885 - val_rmse: 1.4103\n",
      "Epoch 876/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 2.8870 - rmse: 1.6247 - val_loss: 2.5132 - val_rmse: 1.4170\n",
      "Epoch 877/1000\n",
      "7329/7329 [==============================] - 5s 676us/sample - loss: 2.8912 - rmse: 1.6275 - val_loss: 2.4822 - val_rmse: 1.4110\n",
      "Epoch 878/1000\n",
      "7329/7329 [==============================] - 5s 681us/sample - loss: 2.8762 - rmse: 1.6251 - val_loss: 2.5065 - val_rmse: 1.4167\n",
      "Epoch 879/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 2.8852 - rmse: 1.6260 - val_loss: 2.5201 - val_rmse: 1.4235\n",
      "Epoch 880/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 2.8765 - rmse: 1.6244 - val_loss: 2.4411 - val_rmse: 1.3986\n",
      "Epoch 881/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 2.8508 - rmse: 1.6159 - val_loss: 2.4087 - val_rmse: 1.3946\n",
      "Epoch 882/1000\n",
      "7329/7329 [==============================] - 5s 680us/sample - loss: 2.8941 - rmse: 1.6281 - val_loss: 2.4731 - val_rmse: 1.4099\n",
      "Epoch 883/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 2.8751 - rmse: 1.6217 - val_loss: 2.4719 - val_rmse: 1.4063\n",
      "Epoch 884/1000\n",
      "7329/7329 [==============================] - 5s 679us/sample - loss: 2.8201 - rmse: 1.6091 - val_loss: 2.4747 - val_rmse: 1.4163\n",
      "Epoch 885/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 2.8218 - rmse: 1.6078 - val_loss: 2.4953 - val_rmse: 1.4141\n",
      "Epoch 886/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 2.8575 - rmse: 1.6195 - val_loss: 2.4721 - val_rmse: 1.4216\n",
      "Epoch 887/1000\n",
      "7329/7329 [==============================] - 5s 682us/sample - loss: 2.8596 - rmse: 1.6175 - val_loss: 2.4674 - val_rmse: 1.4051\n",
      "Epoch 888/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 2.8502 - rmse: 1.6135 - val_loss: 2.4240 - val_rmse: 1.3969\n",
      "Epoch 889/1000\n",
      "7329/7329 [==============================] - 5s 676us/sample - loss: 2.8403 - rmse: 1.6142 - val_loss: 2.5733 - val_rmse: 1.4397\n",
      "Epoch 890/1000\n",
      "7329/7329 [==============================] - 5s 681us/sample - loss: 2.8110 - rmse: 1.6056 - val_loss: 2.4106 - val_rmse: 1.3916\n",
      "Epoch 891/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 2.8367 - rmse: 1.6132 - val_loss: 2.3994 - val_rmse: 1.3879\n",
      "Epoch 892/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 2.8674 - rmse: 1.6194 - val_loss: 2.4660 - val_rmse: 1.4231\n",
      "Epoch 893/1000\n",
      "7329/7329 [==============================] - 5s 679us/sample - loss: 2.8451 - rmse: 1.6129 - val_loss: 2.4107 - val_rmse: 1.3913\n",
      "Epoch 894/1000\n",
      "7329/7329 [==============================] - 5s 676us/sample - loss: 2.8685 - rmse: 1.6203 - val_loss: 2.3950 - val_rmse: 1.3929\n",
      "Epoch 895/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 2.8876 - rmse: 1.6241 - val_loss: 2.4362 - val_rmse: 1.3983\n",
      "Epoch 896/1000\n",
      "7329/7329 [==============================] - 5s 679us/sample - loss: 2.8678 - rmse: 1.6177 - val_loss: 2.4130 - val_rmse: 1.3942\n",
      "Epoch 897/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 2.8762 - rmse: 1.6216 - val_loss: 2.4770 - val_rmse: 1.4081\n",
      "Epoch 898/1000\n",
      "7329/7329 [==============================] - 5s 679us/sample - loss: 2.8275 - rmse: 1.6093 - val_loss: 2.4270 - val_rmse: 1.4002\n",
      "Epoch 899/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 2.8314 - rmse: 1.6120 - val_loss: 2.4184 - val_rmse: 1.3958\n",
      "Epoch 900/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 2.8225 - rmse: 1.6078 - val_loss: 2.4075 - val_rmse: 1.3921\n",
      "Epoch 901/1000\n",
      "7329/7329 [==============================] - 5s 679us/sample - loss: 2.8227 - rmse: 1.6072 - val_loss: 2.3960 - val_rmse: 1.3851\n",
      "Epoch 902/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 2.8282 - rmse: 1.6075 - val_loss: 2.4159 - val_rmse: 1.3959\n",
      "Epoch 903/1000\n",
      "7329/7329 [==============================] - 5s 679us/sample - loss: 2.8377 - rmse: 1.6108 - val_loss: 2.4849 - val_rmse: 1.4100\n",
      "Epoch 904/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 2.8475 - rmse: 1.6130 - val_loss: 2.4560 - val_rmse: 1.4018\n",
      "Epoch 905/1000\n",
      "7329/7329 [==============================] - 5s 676us/sample - loss: 2.8158 - rmse: 1.6039 - val_loss: 2.4094 - val_rmse: 1.3904\n",
      "Epoch 906/1000\n",
      "7329/7329 [==============================] - 5s 682us/sample - loss: 2.8418 - rmse: 1.6117 - val_loss: 2.6689 - val_rmse: 1.4635\n",
      "Epoch 907/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 2.8081 - rmse: 1.6046 - val_loss: 2.4193 - val_rmse: 1.3938\n",
      "Epoch 908/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 2.8010 - rmse: 1.6021 - val_loss: 2.3879 - val_rmse: 1.3847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 909/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 2.8296 - rmse: 1.6073 - val_loss: 2.3826 - val_rmse: 1.3852\n",
      "Epoch 910/1000\n",
      "7329/7329 [==============================] - 5s 675us/sample - loss: 2.8139 - rmse: 1.6044 - val_loss: 2.3770 - val_rmse: 1.3815\n",
      "Epoch 911/1000\n",
      "7329/7329 [==============================] - 5s 679us/sample - loss: 2.7878 - rmse: 1.5960 - val_loss: 2.4183 - val_rmse: 1.3948\n",
      "Epoch 912/1000\n",
      "7329/7329 [==============================] - 5s 675us/sample - loss: 2.7751 - rmse: 1.5947 - val_loss: 2.4094 - val_rmse: 1.3988\n",
      "Epoch 913/1000\n",
      "7329/7329 [==============================] - 5s 676us/sample - loss: 2.7768 - rmse: 1.5951 - val_loss: 2.4272 - val_rmse: 1.3934\n",
      "Epoch 914/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 2.8167 - rmse: 1.6053 - val_loss: 2.4195 - val_rmse: 1.3900\n",
      "Epoch 915/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 2.7585 - rmse: 1.5896 - val_loss: 2.4300 - val_rmse: 1.3953\n",
      "Epoch 916/1000\n",
      "7329/7329 [==============================] - 5s 676us/sample - loss: 2.7534 - rmse: 1.5874 - val_loss: 2.4774 - val_rmse: 1.4047\n",
      "Epoch 917/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 2.7778 - rmse: 1.5940 - val_loss: 2.4464 - val_rmse: 1.3956\n",
      "Epoch 918/1000\n",
      "7329/7329 [==============================] - 5s 676us/sample - loss: 2.8003 - rmse: 1.6003 - val_loss: 2.3955 - val_rmse: 1.3851\n",
      "Epoch 919/1000\n",
      "7329/7329 [==============================] - 5s 681us/sample - loss: 2.7722 - rmse: 1.5925 - val_loss: 2.4515 - val_rmse: 1.4011\n",
      "Epoch 920/1000\n",
      "7329/7329 [==============================] - 5s 676us/sample - loss: 2.8229 - rmse: 1.6047 - val_loss: 2.4248 - val_rmse: 1.3924\n",
      "Epoch 921/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 2.7700 - rmse: 1.5880 - val_loss: 2.3968 - val_rmse: 1.3895\n",
      "Epoch 922/1000\n",
      "7329/7329 [==============================] - 5s 676us/sample - loss: 2.7718 - rmse: 1.5904 - val_loss: 2.4075 - val_rmse: 1.3889\n",
      "Epoch 923/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 2.7843 - rmse: 1.5952 - val_loss: 2.4529 - val_rmse: 1.3992\n",
      "Epoch 924/1000\n",
      "7329/7329 [==============================] - 5s 675us/sample - loss: 2.7595 - rmse: 1.5897 - val_loss: 2.4721 - val_rmse: 1.4034\n",
      "Epoch 925/1000\n",
      "7329/7329 [==============================] - 5s 693us/sample - loss: 2.7802 - rmse: 1.5940 - val_loss: 2.3531 - val_rmse: 1.3765\n",
      "Epoch 926/1000\n",
      "7329/7329 [==============================] - 5s 680us/sample - loss: 2.7494 - rmse: 1.5862 - val_loss: 2.4139 - val_rmse: 1.3915\n",
      "Epoch 927/1000\n",
      "7329/7329 [==============================] - 5s 680us/sample - loss: 2.7754 - rmse: 1.5930 - val_loss: 2.4545 - val_rmse: 1.4012\n",
      "Epoch 928/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 2.7385 - rmse: 1.5802 - val_loss: 2.3762 - val_rmse: 1.3817\n",
      "Epoch 929/1000\n",
      "7329/7329 [==============================] - 5s 680us/sample - loss: 2.7627 - rmse: 1.5885 - val_loss: 2.3734 - val_rmse: 1.3785\n",
      "Epoch 930/1000\n",
      "7329/7329 [==============================] - 5s 697us/sample - loss: 2.7667 - rmse: 1.5896 - val_loss: 2.3567 - val_rmse: 1.3748\n",
      "Epoch 931/1000\n",
      "7329/7329 [==============================] - 5s 683us/sample - loss: 2.6948 - rmse: 1.5721 - val_loss: 2.4168 - val_rmse: 1.3892\n",
      "Epoch 932/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 2.7271 - rmse: 1.5796 - val_loss: 2.5155 - val_rmse: 1.4218\n",
      "Epoch 933/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 2.7496 - rmse: 1.5840 - val_loss: 2.3745 - val_rmse: 1.3792\n",
      "Epoch 934/1000\n",
      "7329/7329 [==============================] - 5s 679us/sample - loss: 2.7447 - rmse: 1.5833 - val_loss: 2.3984 - val_rmse: 1.3881\n",
      "Epoch 935/1000\n",
      "7329/7329 [==============================] - 5s 675us/sample - loss: 2.7695 - rmse: 1.5884 - val_loss: 2.3879 - val_rmse: 1.3854\n",
      "Epoch 936/1000\n",
      "7329/7329 [==============================] - 5s 675us/sample - loss: 2.6998 - rmse: 1.5721 - val_loss: 2.4223 - val_rmse: 1.3927\n",
      "Epoch 937/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 2.7478 - rmse: 1.5823 - val_loss: 2.3924 - val_rmse: 1.3862\n",
      "Epoch 938/1000\n",
      "7329/7329 [==============================] - 5s 682us/sample - loss: 2.7438 - rmse: 1.5822 - val_loss: 2.4519 - val_rmse: 1.3993\n",
      "Epoch 939/1000\n",
      "7329/7329 [==============================] - 5s 676us/sample - loss: 2.7444 - rmse: 1.5838 - val_loss: 2.5469 - val_rmse: 1.4278\n",
      "Epoch 940/1000\n",
      "7329/7329 [==============================] - 5s 679us/sample - loss: 2.7508 - rmse: 1.5851 - val_loss: 2.3959 - val_rmse: 1.3848\n",
      "Epoch 941/1000\n",
      "7329/7329 [==============================] - 5s 676us/sample - loss: 2.7544 - rmse: 1.5849 - val_loss: 2.4779 - val_rmse: 1.4052\n",
      "Epoch 942/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 2.7118 - rmse: 1.5732 - val_loss: 2.4897 - val_rmse: 1.4100\n",
      "Epoch 943/1000\n",
      "7329/7329 [==============================] - 5s 680us/sample - loss: 2.7467 - rmse: 1.5829 - val_loss: 2.4314 - val_rmse: 1.3937\n",
      "Epoch 944/1000\n",
      "7329/7329 [==============================] - 5s 676us/sample - loss: 2.6784 - rmse: 1.5646 - val_loss: 2.3918 - val_rmse: 1.3847\n",
      "Epoch 945/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 2.7214 - rmse: 1.5765 - val_loss: 2.3919 - val_rmse: 1.3863\n",
      "Epoch 946/1000\n",
      "7329/7329 [==============================] - 5s 680us/sample - loss: 2.7484 - rmse: 1.5816 - val_loss: 2.5371 - val_rmse: 1.4268\n",
      "Epoch 947/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 2.6740 - rmse: 1.5642 - val_loss: 2.3819 - val_rmse: 1.3844\n",
      "Epoch 948/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 2.7030 - rmse: 1.5684 - val_loss: 2.4215 - val_rmse: 1.3914\n",
      "Epoch 949/1000\n",
      "7329/7329 [==============================] - 5s 680us/sample - loss: 2.7018 - rmse: 1.5693 - val_loss: 2.3889 - val_rmse: 1.3934\n",
      "Epoch 950/1000\n",
      "7329/7329 [==============================] - 5s 680us/sample - loss: 2.7335 - rmse: 1.5776 - val_loss: 2.3922 - val_rmse: 1.3829\n",
      "Epoch 951/1000\n",
      "7329/7329 [==============================] - 5s 679us/sample - loss: 2.6995 - rmse: 1.5690 - val_loss: 2.4332 - val_rmse: 1.3937\n",
      "Epoch 952/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 2.7282 - rmse: 1.5776 - val_loss: 2.4645 - val_rmse: 1.4013\n",
      "Epoch 953/1000\n",
      "7329/7329 [==============================] - 5s 680us/sample - loss: 2.6926 - rmse: 1.5657 - val_loss: 2.5257 - val_rmse: 1.4178\n",
      "Epoch 954/1000\n",
      "7329/7329 [==============================] - 5s 695us/sample - loss: 2.6719 - rmse: 1.5621 - val_loss: 2.3282 - val_rmse: 1.3723\n",
      "Epoch 955/1000\n",
      "7329/7329 [==============================] - 5s 686us/sample - loss: 2.7006 - rmse: 1.5682 - val_loss: 2.4193 - val_rmse: 1.3910\n",
      "Epoch 956/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 2.6948 - rmse: 1.5675 - val_loss: 2.4384 - val_rmse: 1.3976\n",
      "Epoch 957/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 2.6981 - rmse: 1.5682 - val_loss: 2.4257 - val_rmse: 1.4009\n",
      "Epoch 958/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 2.6879 - rmse: 1.5650 - val_loss: 2.4211 - val_rmse: 1.3920\n",
      "Epoch 959/1000\n",
      "7329/7329 [==============================] - 5s 676us/sample - loss: 2.6588 - rmse: 1.5581 - val_loss: 2.3753 - val_rmse: 1.3960\n",
      "Epoch 960/1000\n",
      "7329/7329 [==============================] - 5s 679us/sample - loss: 2.6859 - rmse: 1.5640 - val_loss: 2.4300 - val_rmse: 1.3919\n",
      "Epoch 961/1000\n",
      "7329/7329 [==============================] - 5s 680us/sample - loss: 2.6917 - rmse: 1.5679 - val_loss: 2.4512 - val_rmse: 1.3989\n",
      "Epoch 962/1000\n",
      "7329/7329 [==============================] - 5s 681us/sample - loss: 2.7073 - rmse: 1.5708 - val_loss: 2.4252 - val_rmse: 1.3891\n",
      "Epoch 963/1000\n",
      "7329/7329 [==============================] - 5s 679us/sample - loss: 2.7010 - rmse: 1.5674 - val_loss: 2.4044 - val_rmse: 1.3890\n",
      "Epoch 964/1000\n",
      "7329/7329 [==============================] - 5s 679us/sample - loss: 2.6679 - rmse: 1.5598 - val_loss: 2.3650 - val_rmse: 1.3743\n",
      "Epoch 965/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 2.6428 - rmse: 1.5509 - val_loss: 2.4071 - val_rmse: 1.3884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 966/1000\n",
      "7329/7329 [==============================] - 5s 676us/sample - loss: 2.6734 - rmse: 1.5600 - val_loss: 2.3626 - val_rmse: 1.3810\n",
      "Epoch 967/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 2.6088 - rmse: 1.5448 - val_loss: 2.3669 - val_rmse: 1.3745\n",
      "Epoch 968/1000\n",
      "7329/7329 [==============================] - 5s 675us/sample - loss: 2.6639 - rmse: 1.5570 - val_loss: 2.3622 - val_rmse: 1.3780\n",
      "Epoch 969/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 2.6946 - rmse: 1.5660 - val_loss: 2.4501 - val_rmse: 1.3964\n",
      "Epoch 970/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 2.7007 - rmse: 1.5673 - val_loss: 2.3465 - val_rmse: 1.3823\n",
      "Epoch 971/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 2.6336 - rmse: 1.5497 - val_loss: 2.4308 - val_rmse: 1.3921\n",
      "Epoch 972/1000\n",
      "7329/7329 [==============================] - 5s 676us/sample - loss: 2.6731 - rmse: 1.5583 - val_loss: 2.3617 - val_rmse: 1.3766\n",
      "Epoch 973/1000\n",
      "7329/7329 [==============================] - 5s 680us/sample - loss: 2.6420 - rmse: 1.5529 - val_loss: 2.3858 - val_rmse: 1.3853\n",
      "Epoch 974/1000\n",
      "7329/7329 [==============================] - 5s 701us/sample - loss: 2.6609 - rmse: 1.5550 - val_loss: 2.3563 - val_rmse: 1.3716\n",
      "Epoch 975/1000\n",
      "7329/7329 [==============================] - 5s 676us/sample - loss: 2.6677 - rmse: 1.5592 - val_loss: 2.3880 - val_rmse: 1.3824\n",
      "Epoch 976/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 2.6363 - rmse: 1.5499 - val_loss: 2.3556 - val_rmse: 1.3746\n",
      "Epoch 977/1000\n",
      "7329/7329 [==============================] - 5s 693us/sample - loss: 2.6609 - rmse: 1.5546 - val_loss: 2.3351 - val_rmse: 1.3702\n",
      "Epoch 978/1000\n",
      "7329/7329 [==============================] - 5s 679us/sample - loss: 2.6538 - rmse: 1.5536 - val_loss: 2.4576 - val_rmse: 1.3981\n",
      "Epoch 979/1000\n",
      "7329/7329 [==============================] - 5s 688us/sample - loss: 2.6642 - rmse: 1.5566 - val_loss: 2.3975 - val_rmse: 1.3854\n",
      "Epoch 980/1000\n",
      "7329/7329 [==============================] - 5s 679us/sample - loss: 2.6319 - rmse: 1.5450 - val_loss: 2.6263 - val_rmse: 1.4579\n",
      "Epoch 981/1000\n",
      "7329/7329 [==============================] - 5s 676us/sample - loss: 2.6673 - rmse: 1.5576 - val_loss: 2.4805 - val_rmse: 1.4052\n",
      "Epoch 982/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 2.6649 - rmse: 1.5565 - val_loss: 2.3922 - val_rmse: 1.3828\n",
      "Epoch 983/1000\n",
      "7329/7329 [==============================] - 5s 676us/sample - loss: 2.6406 - rmse: 1.5499 - val_loss: 2.3713 - val_rmse: 1.3785\n",
      "Epoch 984/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 2.6100 - rmse: 1.5403 - val_loss: 2.3736 - val_rmse: 1.3787\n",
      "Epoch 985/1000\n",
      "7329/7329 [==============================] - 5s 682us/sample - loss: 2.6191 - rmse: 1.5446 - val_loss: 2.4187 - val_rmse: 1.3888\n",
      "Epoch 986/1000\n",
      "7329/7329 [==============================] - 5s 678us/sample - loss: 2.6432 - rmse: 1.5507 - val_loss: 2.3794 - val_rmse: 1.3787\n",
      "Epoch 987/1000\n",
      "7329/7329 [==============================] - 5s 681us/sample - loss: 2.6543 - rmse: 1.5522 - val_loss: 2.4103 - val_rmse: 1.3860\n",
      "Epoch 988/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 2.6588 - rmse: 1.5556 - val_loss: 2.3725 - val_rmse: 1.3789\n",
      "Epoch 989/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 2.6575 - rmse: 1.5516 - val_loss: 2.3809 - val_rmse: 1.3785\n",
      "Epoch 990/1000\n",
      "7329/7329 [==============================] - 5s 693us/sample - loss: 2.6246 - rmse: 1.5454 - val_loss: 2.3252 - val_rmse: 1.3688\n",
      "Epoch 991/1000\n",
      "7329/7329 [==============================] - 5s 684us/sample - loss: 2.6201 - rmse: 1.5410 - val_loss: 2.4063 - val_rmse: 1.3858\n",
      "Epoch 992/1000\n",
      "7329/7329 [==============================] - 5s 676us/sample - loss: 2.5871 - rmse: 1.5331 - val_loss: 2.4932 - val_rmse: 1.4107\n",
      "Epoch 993/1000\n",
      "7329/7329 [==============================] - 5s 679us/sample - loss: 2.6327 - rmse: 1.5490 - val_loss: 2.4222 - val_rmse: 1.3903\n",
      "Epoch 994/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 2.5962 - rmse: 1.5356 - val_loss: 2.3897 - val_rmse: 1.3822\n",
      "Epoch 995/1000\n",
      "7329/7329 [==============================] - 5s 680us/sample - loss: 2.6151 - rmse: 1.5433 - val_loss: 2.5426 - val_rmse: 1.4210\n",
      "Epoch 996/1000\n",
      "7329/7329 [==============================] - 5s 676us/sample - loss: 2.5886 - rmse: 1.5346 - val_loss: 2.3804 - val_rmse: 1.3797\n",
      "Epoch 997/1000\n",
      "7329/7329 [==============================] - 5s 676us/sample - loss: 2.6135 - rmse: 1.5434 - val_loss: 2.4374 - val_rmse: 1.3916\n",
      "Epoch 998/1000\n",
      "7329/7329 [==============================] - 5s 679us/sample - loss: 2.5808 - rmse: 1.5321 - val_loss: 2.4336 - val_rmse: 1.3914\n",
      "Epoch 999/1000\n",
      "7329/7329 [==============================] - 5s 697us/sample - loss: 2.6078 - rmse: 1.5409 - val_loss: 2.3149 - val_rmse: 1.3629\n",
      "Epoch 1000/1000\n",
      "7329/7329 [==============================] - 5s 677us/sample - loss: 2.6059 - rmse: 1.5390 - val_loss: 2.3509 - val_rmse: 1.3723\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "USE_SAVED_MODEL = False\n",
    "\n",
    "if USE_SAVED_MODEL == False:\n",
    "    history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs = 1000,\n",
    "                    batch_size = 256,\n",
    "                    validation_split = 0.2, #data = (x_test, y_test),\n",
    "                    callbacks = callbacks\n",
    "                    )\n",
    "    plt.style.use('seaborn-darkgrid')\n",
    "    plt.plot(history.history['rmse'])\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Root Mean Square Error')\n",
    "    plt.title('Model Training Error')\n",
    "    plt.show() \n",
    "    \n",
    "else:\n",
    "    model.load_weights(model_dir+\"base_model_weights_1.h5\")\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookid_dir = '../input/IdLookupTable.csv'\n",
    "lookid_data = pd.read_csv(lookid_dir)\n",
    "test_data = pd.read_csv(test_file)\n",
    "\n",
    "x_test = []\n",
    "for i in range(0,len(test_data)):\n",
    "    img = test_data['Image'][i].split(' ')\n",
    "    x_test.append(img)\n",
    "    \n",
    "x_test = np.array(x_test,dtype = 'float')\n",
    "x_test = x_test/255.0\n",
    "x_test = x_test.reshape(-1,96,96,1)    \n",
    "\n",
    "y_test = model.predict(x_test)\n",
    "y_test = np.clip(y_test,0,96)\n",
    "\n",
    "lookid_list = list(lookid_data['FeatureName'])\n",
    "imageID = list(lookid_data['ImageId']-1)\n",
    "pred_list = list(y_test)\n",
    "\n",
    "rowid = list(lookid_data['RowId'])\n",
    "\n",
    "feature = []\n",
    "for f in list(lookid_data['FeatureName']):\n",
    "    feature.append(lookid_list.index(f))\n",
    "    \n",
    "    \n",
    "submit_data = []\n",
    "for x,y in zip(imageID,feature):\n",
    "    submit_data.append(pred_list[x][y])\n",
    "rowid = pd.Series(rowid,name = 'RowId')\n",
    "loc = pd.Series(submit_data,name = 'Location')\n",
    "submission = pd.concat([rowid,loc],axis = 1)\n",
    "submission.to_csv('../output/w207_base_submission_1.csv',index = False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    sess = get_session()\n",
    "    clear_session()\n",
    "    sess.close()\n",
    "    sess = get_session()\n",
    "    try:\n",
    "        del model # this is from global space - change this as you need\n",
    "    except:\n",
    "        print(\"Model clear Failed\")\n",
    "    print(gc.collect())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
