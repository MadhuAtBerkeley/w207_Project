{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4dbced162440c393a0a5b7e5aee344711a30e994"
   },
   "source": [
    "# Facial Keypoint Detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "369fa247a546e39d82bdfdc5b7d4ed58baa40e4f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/user1/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/user1/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/user1/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/user1/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/user1/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/user1/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.backend.tensorflow_backend import clear_session\n",
    "from keras.backend.tensorflow_backend import get_session\n",
    "from tensorflow.keras.layers import LeakyReLU, ReLU\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Activation, Convolution2D, BatchNormalization, Flatten\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, MaxPool2D\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "from keras import backend\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import cv2\n",
    "import os, gc, json, math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n",
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print(keras.__version__)\n",
    "import tensorflow\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pixels(data):\n",
    "    \"\"\"\n",
    "    Convert pixels to the right intensity 0-1 and in a square matrix.\n",
    "    \"\"\"\n",
    "    data = np.array([row.split(' ') for row in data['Image']],dtype='float') / 255.0\n",
    "    data = data.reshape(-1,96,96,1)\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_img(sample_img,coord=None):\n",
    "    \"\"\"\n",
    "    Display an image. For debugging, display a coordinate on the image.\n",
    "    input:\n",
    "        - sample_img: numpy array. image to be displayed\n",
    "        - coord: lst. of coordinates in form [[x_coordinate,y_coordinate],[x_coordinate,y_coordinate]]\n",
    "    TODO handle multiple coordinates. Work out bugs with multiple coordinates\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.imshow(sample_img.reshape(96,96),cmap='gray')\n",
    "    if coord is not None:\n",
    "        plt.scatter(coord[0],coord[1],marker = '*',c='r')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_facial_keypoints(data,ind):\n",
    "    \"\"\"\n",
    "    Structure the coordinates for all facial keypoints for a single image.\n",
    "    inputs:\n",
    "        - data: numpy array containing rows as each image sample and columns as facial keypoint coordinates\n",
    "        - ind: index of the image\n",
    "    output:\n",
    "        - numpy array with format [[list of x-coordinates],[list of y-coordinates]]\n",
    "    \"\"\"\n",
    "    data[ind]\n",
    "    it = iter(data[ind])\n",
    "    x_coord = []\n",
    "    y_coord = []\n",
    "\n",
    "    for x in it:\n",
    "        x_coord.append(x)\n",
    "        y_coord.append(next(it))\n",
    "    \n",
    "    return(np.array([x_coord,y_coord]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "# Reset Keras Session\n",
    "def reset_keras():\n",
    "    sess = get_session()\n",
    "    clear_session()\n",
    "    sess.close()\n",
    "    sess = get_session()\n",
    "\n",
    "    try:\n",
    "        del model # this is from global space - change this as you need\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    print(gc.collect()) # if it's done something you should see a number being outputted\n",
    "\n",
    "    # use the same config as you used to create the session\n",
    "    config = tensorflow.ConfigProto()\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = 1\n",
    "    config.gpu_options.visible_device_list = \"0\"\n",
    "    set_session(tensorflow.Session(config=config))\n",
    "    \n",
    "reset_keras()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_uuid": "fa1b76273d02502e3fd668dddf74ecf522044524"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('../output/'):\n",
    "    os.makedirs('../output/model')\n",
    "    os.makedirs('../output/history')\n",
    "    \n",
    "    \n",
    "model_dir = \"../output/model/\"\n",
    "history_dir = \"../output/history/\"\n",
    "\n",
    "train_file = '../input/training/training.csv'\n",
    "test_file = '../input/test/test.csv'\n",
    "train_data = pd.read_csv(train_file)  \n",
    "#test_data = pd.read_csv(test_file)\n",
    "\n",
    "\n",
    "bad_samples = [1747, 1877, 1881, 1979, 2154, 2199, 2289, 2321, 2453, 3173, 3296, 3447, 4180, 6859,\n",
    "              2090, 2175, 1907, 2562, 2818, 3296, 3447, 4263, 4482, 4490, 4636, 5059, 6493, 6906]\n",
    "\n",
    "train_data = train_data.drop(bad_samples).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cfd045f7166f9cce2e2075b3ead83813d07012c8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e1b88f1528838c0a8fec61f9a02a70b5077312e9"
   },
   "source": [
    "Create training vector with images and normalize thee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = convert_pixels(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a833f4cc5e559774d3a310fd09d40d31e49e71da"
   },
   "source": [
    "Generate labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "_uuid": "e9d804a035809cdf8ffda19f41ce3feb278a38fb"
   },
   "outputs": [],
   "source": [
    "y_train = train_data[[col for col in train_data.columns if col != 'Image']].to_numpy()\n",
    "imputer = KNNImputer(n_neighbors=3, weights='distance')\n",
    "\n",
    "#imputer = IterativeImputer(max_iter=1000, tol=0.01)\n",
    "#y_train = imputer.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set feature engineering parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_na = False\n",
    "add_flip_horiz = True\n",
    "add_blur_img = False\n",
    "add_rotate_img = True\n",
    "orig_x_train = x_train.copy()\n",
    "orig_y_train = y_train.copy()\n",
    "y_train = imputer.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill NA in the training labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "if fill_na:\n",
    "    # https://stackoverflow.com/questions/18689235/numpy-array-replace-nan-values-with-average-of-columns\n",
    "    # get column means\n",
    "    col_mean = np.nanmean(y_train,axis=0)\n",
    "\n",
    "    # find the x,y indices that are missing from y_train\n",
    "    inds = np.where(np.isnan(y_train))\n",
    "\n",
    "    # fill in missing values in y_train with the column means. \"take\" is much more efficient than fancy indexing\n",
    "    y_train[inds] = np.take(col_mean, inds[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flip images horizontally and add to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_img_horiz():\n",
    "    \"\"\"\n",
    "    Flip images horizontally for all training images\n",
    "    \"\"\"\n",
    "    # Flip images\n",
    "    flip_img = np.array([np.fliplr(orig_x_train[[ind]][0]) for ind in range(orig_x_train.shape[0])])\n",
    "    \n",
    "    # Flip coordinates\n",
    "    train_data_flip = train_data.copy()\n",
    "    x_columns = [col for col in train_data.columns if '_x' in col]\n",
    "    train_data_flip[x_columns] = train_data[x_columns].applymap(lambda x: 96-x)\n",
    "    \n",
    "    #left and right are swapped so undo\n",
    "    left_columns = [col for col in train_data.columns if 'left' in col]\n",
    "    right_columns = [col for col in train_data.columns if 'right' in col]\n",
    "    train_data_flip[left_columns+right_columns] = train_data_flip[right_columns+left_columns]\n",
    "    \n",
    "    flip_coord = train_data_flip[[col for col in train_data if col != 'Image']].to_numpy()\n",
    "    return(flip_img,flip_coord)\n",
    "\n",
    "if add_flip_horiz:\n",
    "    # Apply the augmentation and add the new data to the training set\n",
    "    flipped_img,flipped_coord = flip_img_horiz()\n",
    "    flipped_coord = imputer.fit_transform(flipped_coord)\n",
    "    #x_train = np.append(x_train,flipped_img,axis=0)\n",
    "    #y_train = np.append(y_train,flipped_coord,axis=0)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Gaussian blurring with a 5x5 filter with $\\sigma$ = 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blur_img():\n",
    "    \"\"\"\n",
    "    Add Gaussian blurring to the images\n",
    "    \"\"\"\n",
    "    # https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_filtering/py_filtering.html\n",
    "    blur_img = np.array([cv2.GaussianBlur(orig_x_train[[ind]][0],(5,5),2).reshape(96,96,1) for ind in range(orig_x_train.shape[0])])\n",
    "    \n",
    "    return(blur_img)\n",
    "\n",
    "if add_blur_img:\n",
    "    x_train = np.append(x_train,blur_img(),axis=0)\n",
    "    y_train = np.append(y_train,orig_y_train,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_img(x_train, y_train):\n",
    "    \"\"\"\"\n",
    "    Rotate images by angles between [5, 10, 15 degrees]\n",
    "    \"\"\"\n",
    "    angles = [7, -7, 10, -10, 15, -15]\n",
    "    b = np.ones((1,3))\n",
    "    rows,cols = (96,96)\n",
    "    x_train_rot = []\n",
    "    y_train_rot = y_train.copy()\n",
    "    M_angles = [cv2.getRotationMatrix2D((cols/2,rows/2),angle,1) for angle in angles]\n",
    "    \n",
    "    for i in range(x_train.shape[0]):\n",
    "        #M = cv2.getRotationMatrix2D((cols/2,rows/2),np.random.choice(angles,1),1)\n",
    "        M = M_angles[np.random.choice(len(M_angles))]\n",
    "        x_train_rot.append((cv2.warpAffine(x_train[[i]].reshape(rows,cols,1),M,(cols,rows)).reshape(96,96,1)))\n",
    "       \n",
    "        #apply affine transformation to (x,y) labels\n",
    "        for j in range(int(y_train.shape[1]/2)):\n",
    "            b[:,0:2] = y_train[i,2*j:2*j+2]\n",
    "            y_train_rot[i,2*j:2*j+2] = np.dot(b,M.transpose()) \n",
    "    \n",
    "    x_train_rot = np.array(x_train_rot)\n",
    "    return x_train_rot, y_train_rot\n",
    "\n",
    "if add_rotate_img:\n",
    "    if add_flip_horiz:\n",
    "        x_rotate, y_rotate = rotate_img(flipped_img,flipped_coord) #x_train,y_train)\n",
    "    else:\n",
    "        x_rotate, y_rotate = rotate_img(x_train,y_train)\n",
    "    x_train = np.append(x_train,x_rotate,axis=0)\n",
    "    y_train = np.append(y_train,y_rotate,axis=0)   \n",
    "    \n",
    "else:\n",
    "    if add_flip_horiz:\n",
    "          x_train = np.append(x_train,flipped_image,axis=0)\n",
    "          y_train = np.append(y_train,flipped_coord,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callback function if detailed log required\n",
    "class History(tensorflow.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.train_loss = []\n",
    "        self.train_rmse = []\n",
    "        self.val_rmse = []\n",
    "        self.val_loss = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.train_loss.append(logs.get('loss'))\n",
    "        self.train_rmse.append(logs.get('rmse'))\n",
    "        \n",
    "    def on_epoch_end(self, batch, logs={}):    \n",
    "        self.val_rmse.append(logs.get('val_rmse'))\n",
    "        self.val_loss.append(logs.get('val_loss'))\n",
    "        \n",
    "# Implement ModelCheckPoint callback function to save CNN model\n",
    "class CNN_ModelCheckpoint(tensorflow.keras.callbacks.Callback):\n",
    "\n",
    "    def __init__(self, model, filename):\n",
    "        self.filename = filename\n",
    "        self.cnn_model = model\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.max_val_rmse = math.inf\n",
    "        \n",
    " \n",
    "    def on_epoch_end(self, batch, logs={}):    \n",
    "        val_rmse = logs.get('val_rmse')\n",
    "        if(val_rmse < self.max_val_rmse):\n",
    "           self.max_val_rmse = val_rmse\n",
    "           self.cnn_model.save_weights(self.filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 96, 96, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 96, 96, 32)        288       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 96, 96, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_7 (Ba (None, 96, 96, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 48, 48, 64)        18432     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 48, 48, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_8 (Ba (None, 48, 48, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 24, 24, 96)        55296     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 24, 24, 96)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_9 (Ba (None, 24, 24, 96)        384       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 12, 12, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 12, 12, 128)       110592    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_10 (B (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 6, 6, 256)         294912    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_11 (B (None, 6, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 3, 3, 512)         1179648   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 3, 3, 512)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_12 (B (None, 3, 3, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 3, 3, 512)         2359296   \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_13 (B (None, 3, 3, 512)         2048      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               2359808   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 30)                15390     \n",
      "=================================================================\n",
      "Total params: 6,400,062\n",
      "Trainable params: 6,396,862\n",
      "Non-trainable params: 3,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def base_model():\n",
    "    model_input = Input(shape=(96,96,1))\n",
    "\n",
    "    x = Convolution2D(32, (3,3), padding='same', use_bias=False)(model_input)\n",
    "    x = LeakyReLU(alpha = 0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "    x = Convolution2D(64, (3,3), padding='same', use_bias=False)(x)\n",
    "    x = LeakyReLU(alpha = 0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "    x = Convolution2D(96, (3,3), padding='same', use_bias=False)(x)\n",
    "    x = LeakyReLU(alpha = 0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "    x = Convolution2D(128, (3,3),padding='same', use_bias=False)(x)\n",
    "    x = LeakyReLU(alpha = 0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "    x = Convolution2D(256, (3,3),padding='same',use_bias=False)(x)\n",
    "    x = LeakyReLU(alpha = 0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "    x = Convolution2D(512, (3,3), padding='same', use_bias=False)(x)\n",
    "    x = LeakyReLU(alpha = 0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Convolution2D(512, (3,3), activation='relu', padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(512,activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    model_output = Dense(30)(x)\n",
    "    model = Model(model_input, model_output, name=\"base_model\")\n",
    "    return model\n",
    "\n",
    "model = base_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom RMSE metric\n",
    "def rmse(y_true, y_pred):\n",
    "    return backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))\n",
    "\n",
    "#from tensorflow.keras.optimizers.schedules import InverseTimeDecay\n",
    "\n",
    "# Use Nadam optimizer with variable learning rate\n",
    "optimizer = Nadam(lr=0.00001,\n",
    "                  beta_1=0.9,\n",
    "                  beta_2=0.999,\n",
    "                  epsilon=1e-08,\n",
    "                  schedule_decay=0.004)\n",
    "\n",
    "\n",
    "# Loss: MSE and Metric = RMSE\n",
    "model.compile(optimizer= optimizer, \n",
    "              loss='mean_squared_error',\n",
    "              metrics=[rmse])\n",
    "\n",
    "#Callback to save the best model\n",
    "saveBase_Model = CNN_ModelCheckpoint(model, model_dir+\"base_model_weights_1.h5\")\n",
    "\n",
    "#define callback functions\n",
    "callbacks = [#EarlyStopping(monitor='val_rmse', patience=3, verbose=2),\n",
    "             saveBase_Model]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e4cf4686b410841f2e34dbb081f3429d1b0f67e9"
   },
   "source": [
    "Run for 1000 epochs and keeping 20% train-valid split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_uuid": "894af9cbfcf2dca50e7407946cad318157b77d0a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "14046/14046 [==============================] - 9s 616us/sample - loss: 2269.9446 - rmse: 47.5334\n",
      "Epoch 2/500\n",
      "14046/14046 [==============================] - 7s 531us/sample - loss: 1324.8447 - rmse: 36.1411\n",
      "Epoch 3/500\n",
      "14046/14046 [==============================] - 7s 532us/sample - loss: 689.9581 - rmse: 25.9914\n",
      "Epoch 4/500\n",
      "14046/14046 [==============================] - 8s 562us/sample - loss: 345.2729 - rmse: 18.3002\n",
      "Epoch 5/500\n",
      "14046/14046 [==============================] - 8s 543us/sample - loss: 170.8310 - rmse: 12.8010\n",
      "Epoch 6/500\n",
      "14046/14046 [==============================] - 8s 544us/sample - loss: 91.9991 - rmse: 9.3500\n",
      "Epoch 7/500\n",
      "14046/14046 [==============================] - 8s 548us/sample - loss: 61.6054 - rmse: 7.6396\n",
      "Epoch 8/500\n",
      "14046/14046 [==============================] - 8s 558us/sample - loss: 50.6516 - rmse: 6.9335\n",
      "Epoch 9/500\n",
      "14046/14046 [==============================] - 8s 559us/sample - loss: 47.1752 - rmse: 6.6924\n",
      "Epoch 10/500\n",
      "14046/14046 [==============================] - 8s 561us/sample - loss: 45.4753 - rmse: 6.5728\n",
      "Epoch 11/500\n",
      "14046/14046 [==============================] - 8s 562us/sample - loss: 44.5634 - rmse: 6.5096\n",
      "Epoch 12/500\n",
      "14046/14046 [==============================] - 8s 561us/sample - loss: 43.7435 - rmse: 6.4495\n",
      "Epoch 13/500\n",
      "14046/14046 [==============================] - 8s 559us/sample - loss: 42.9445 - rmse: 6.3900\n",
      "Epoch 14/500\n",
      "14046/14046 [==============================] - 8s 559us/sample - loss: 42.0482 - rmse: 6.3245\n",
      "Epoch 15/500\n",
      "14046/14046 [==============================] - 8s 588us/sample - loss: 41.4307 - rmse: 6.2776\n",
      "Epoch 16/500\n",
      "14046/14046 [==============================] - 8s 554us/sample - loss: 40.5526 - rmse: 6.2087\n",
      "Epoch 17/500\n",
      "14046/14046 [==============================] - 8s 583us/sample - loss: 39.4366 - rmse: 6.1250\n",
      "Epoch 18/500\n",
      "14046/14046 [==============================] - 8s 597us/sample - loss: 38.4581 - rmse: 6.0480\n",
      "Epoch 19/500\n",
      "14046/14046 [==============================] - 8s 554us/sample - loss: 37.5159 - rmse: 5.9754\n",
      "Epoch 20/500\n",
      "14046/14046 [==============================] - 8s 557us/sample - loss: 36.3755 - rmse: 5.8836\n",
      "Epoch 21/500\n",
      "14046/14046 [==============================] - 8s 561us/sample - loss: 35.5266 - rmse: 5.8158\n",
      "Epoch 22/500\n",
      "14046/14046 [==============================] - 8s 561us/sample - loss: 34.7957 - rmse: 5.7554\n",
      "Epoch 23/500\n",
      "14046/14046 [==============================] - 8s 567us/sample - loss: 34.0621 - rmse: 5.6923\n",
      "Epoch 24/500\n",
      "14046/14046 [==============================] - 8s 558us/sample - loss: 33.7614 - rmse: 5.6675\n",
      "Epoch 25/500\n",
      "14046/14046 [==============================] - 8s 557us/sample - loss: 33.0569 - rmse: 5.6069\n",
      "Epoch 26/500\n",
      "14046/14046 [==============================] - 8s 562us/sample - loss: 32.5224 - rmse: 5.5608\n",
      "Epoch 27/500\n",
      "14046/14046 [==============================] - 8s 557us/sample - loss: 32.2342 - rmse: 5.5354\n",
      "Epoch 28/500\n",
      "14046/14046 [==============================] - 8s 556us/sample - loss: 31.8145 - rmse: 5.5008\n",
      "Epoch 29/500\n",
      "14046/14046 [==============================] - 8s 562us/sample - loss: 31.4452 - rmse: 5.4669\n",
      "Epoch 30/500\n",
      "14046/14046 [==============================] - 8s 560us/sample - loss: 31.0399 - rmse: 5.4339\n",
      "Epoch 31/500\n",
      "14046/14046 [==============================] - 8s 556us/sample - loss: 30.7161 - rmse: 5.4025\n",
      "Epoch 32/500\n",
      "14046/14046 [==============================] - 8s 561us/sample - loss: 30.4061 - rmse: 5.3732\n",
      "Epoch 33/500\n",
      "14046/14046 [==============================] - 8s 559us/sample - loss: 30.0321 - rmse: 5.3408\n",
      "Epoch 34/500\n",
      "14046/14046 [==============================] - 8s 559us/sample - loss: 29.6227 - rmse: 5.3058\n",
      "Epoch 35/500\n",
      "14046/14046 [==============================] - 8s 557us/sample - loss: 29.3056 - rmse: 5.2748\n",
      "Epoch 36/500\n",
      "14046/14046 [==============================] - 8s 555us/sample - loss: 29.1369 - rmse: 5.2595\n",
      "Epoch 37/500\n",
      "14046/14046 [==============================] - 8s 561us/sample - loss: 28.6258 - rmse: 5.2112\n",
      "Epoch 38/500\n",
      "14046/14046 [==============================] - 8s 559us/sample - loss: 28.3603 - rmse: 5.1888\n",
      "Epoch 39/500\n",
      "14046/14046 [==============================] - 8s 556us/sample - loss: 28.1103 - rmse: 5.1648\n",
      "Epoch 40/500\n",
      "14046/14046 [==============================] - 8s 558us/sample - loss: 27.6496 - rmse: 5.1183\n",
      "Epoch 41/500\n",
      "14046/14046 [==============================] - 8s 559us/sample - loss: 27.3053 - rmse: 5.0905\n",
      "Epoch 42/500\n",
      "14046/14046 [==============================] - 8s 558us/sample - loss: 27.0675 - rmse: 5.0682\n",
      "Epoch 43/500\n",
      "14046/14046 [==============================] - 8s 558us/sample - loss: 26.8598 - rmse: 5.0466\n",
      "Epoch 44/500\n",
      "14046/14046 [==============================] - 8s 558us/sample - loss: 26.5092 - rmse: 5.0163\n",
      "Epoch 45/500\n",
      "14046/14046 [==============================] - 8s 592us/sample - loss: 26.1715 - rmse: 4.9848\n",
      "Epoch 46/500\n",
      "14046/14046 [==============================] - 8s 571us/sample - loss: 25.9718 - rmse: 4.9626\n",
      "Epoch 47/500\n",
      "14046/14046 [==============================] - 8s 565us/sample - loss: 25.7652 - rmse: 4.9433\n",
      "Epoch 48/500\n",
      "14046/14046 [==============================] - 8s 561us/sample - loss: 25.4540 - rmse: 4.9129\n",
      "Epoch 49/500\n",
      "14046/14046 [==============================] - 8s 558us/sample - loss: 25.0046 - rmse: 4.8689\n",
      "Epoch 50/500\n",
      "14046/14046 [==============================] - 8s 558us/sample - loss: 24.8137 - rmse: 4.8517\n",
      "Epoch 51/500\n",
      "14046/14046 [==============================] - 8s 559us/sample - loss: 24.5225 - rmse: 4.8243\n",
      "Epoch 52/500\n",
      "14046/14046 [==============================] - 8s 555us/sample - loss: 24.2796 - rmse: 4.7978\n",
      "Epoch 53/500\n",
      "14046/14046 [==============================] - 8s 563us/sample - loss: 24.1311 - rmse: 4.7837\n",
      "Epoch 54/500\n",
      "14046/14046 [==============================] - 8s 560us/sample - loss: 23.7791 - rmse: 4.7515\n",
      "Epoch 55/500\n",
      "14046/14046 [==============================] - 8s 556us/sample - loss: 23.4141 - rmse: 4.7153\n",
      "Epoch 56/500\n",
      "14046/14046 [==============================] - 9s 647us/sample - loss: 23.2738 - rmse: 4.6975\n",
      "Epoch 57/500\n",
      "14046/14046 [==============================] - 8s 604us/sample - loss: 22.8509 - rmse: 4.6564\n",
      "Epoch 58/500\n",
      "14046/14046 [==============================] - 8s 564us/sample - loss: 22.7393 - rmse: 4.6474\n",
      "Epoch 59/500\n",
      "14046/14046 [==============================] - 8s 563us/sample - loss: 22.3784 - rmse: 4.6078\n",
      "Epoch 60/500\n",
      "14046/14046 [==============================] - 8s 556us/sample - loss: 22.1916 - rmse: 4.5881\n",
      "Epoch 61/500\n",
      "14046/14046 [==============================] - 8s 562us/sample - loss: 22.0324 - rmse: 4.5760\n",
      "Epoch 62/500\n",
      "14046/14046 [==============================] - 8s 555us/sample - loss: 21.7271 - rmse: 4.5432\n",
      "Epoch 63/500\n",
      "14046/14046 [==============================] - 8s 562us/sample - loss: 21.4337 - rmse: 4.5110\n",
      "Epoch 64/500\n",
      "14046/14046 [==============================] - 8s 555us/sample - loss: 21.2021 - rmse: 4.4899\n",
      "Epoch 65/500\n",
      "14046/14046 [==============================] - 8s 556us/sample - loss: 20.9380 - rmse: 4.4608\n",
      "Epoch 66/500\n",
      "14046/14046 [==============================] - 8s 554us/sample - loss: 20.7994 - rmse: 4.4450\n",
      "Epoch 67/500\n",
      "14046/14046 [==============================] - 8s 555us/sample - loss: 20.5432 - rmse: 4.4188\n",
      "Epoch 68/500\n",
      "14046/14046 [==============================] - 8s 555us/sample - loss: 20.2672 - rmse: 4.3911\n",
      "Epoch 69/500\n",
      "14046/14046 [==============================] - 8s 556us/sample - loss: 20.1338 - rmse: 4.3777\n",
      "Epoch 70/500\n",
      "14046/14046 [==============================] - 8s 557us/sample - loss: 19.9008 - rmse: 4.3534\n",
      "Epoch 71/500\n",
      "14046/14046 [==============================] - 8s 555us/sample - loss: 19.6601 - rmse: 4.3275\n",
      "Epoch 72/500\n",
      "14046/14046 [==============================] - 8s 556us/sample - loss: 19.4088 - rmse: 4.3015\n",
      "Epoch 73/500\n",
      "14046/14046 [==============================] - 8s 555us/sample - loss: 19.0870 - rmse: 4.2653\n",
      "Epoch 74/500\n",
      "14046/14046 [==============================] - 8s 555us/sample - loss: 18.9405 - rmse: 4.2510\n",
      "Epoch 75/500\n",
      "14046/14046 [==============================] - 8s 555us/sample - loss: 18.6447 - rmse: 4.2191\n",
      "Epoch 76/500\n",
      "14046/14046 [==============================] - 8s 556us/sample - loss: 18.4447 - rmse: 4.1960\n",
      "Epoch 77/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14046/14046 [==============================] - 7s 517us/sample - loss: 18.2617 - rmse: 4.1778\n",
      "Epoch 78/500\n",
      "14046/14046 [==============================] - 7s 524us/sample - loss: 18.0576 - rmse: 4.1557\n",
      "Epoch 79/500\n",
      "14046/14046 [==============================] - 7s 516us/sample - loss: 17.8543 - rmse: 4.1344\n",
      "Epoch 80/500\n",
      "14046/14046 [==============================] - 7s 515us/sample - loss: 17.6545 - rmse: 4.1117\n",
      "Epoch 81/500\n",
      "14046/14046 [==============================] - 7s 513us/sample - loss: 17.4717 - rmse: 4.0904\n",
      "Epoch 82/500\n",
      "14046/14046 [==============================] - 7s 518us/sample - loss: 17.1829 - rmse: 4.0583\n",
      "Epoch 83/500\n",
      "14046/14046 [==============================] - 7s 515us/sample - loss: 17.0764 - rmse: 4.0469\n",
      "Epoch 84/500\n",
      "14046/14046 [==============================] - 7s 518us/sample - loss: 16.9363 - rmse: 4.0287\n",
      "Epoch 85/500\n",
      "14046/14046 [==============================] - 7s 521us/sample - loss: 16.6816 - rmse: 4.0013\n",
      "Epoch 86/500\n",
      "14046/14046 [==============================] - 8s 602us/sample - loss: 16.4190 - rmse: 3.9682\n",
      "Epoch 87/500\n",
      "14046/14046 [==============================] - 8s 585us/sample - loss: 16.3071 - rmse: 3.9576\n",
      "Epoch 88/500\n",
      "14046/14046 [==============================] - 8s 575us/sample - loss: 16.1417 - rmse: 3.9372\n",
      "Epoch 89/500\n",
      "14046/14046 [==============================] - 8s 576us/sample - loss: 15.8888 - rmse: 3.9080\n",
      "Epoch 90/500\n",
      "14046/14046 [==============================] - 8s 572us/sample - loss: 15.8239 - rmse: 3.8995\n",
      "Epoch 91/500\n",
      "14046/14046 [==============================] - 8s 575us/sample - loss: 15.5599 - rmse: 3.8683\n",
      "Epoch 92/500\n",
      "14046/14046 [==============================] - 8s 562us/sample - loss: 15.3893 - rmse: 3.8455\n",
      "Epoch 93/500\n",
      "14046/14046 [==============================] - 8s 559us/sample - loss: 15.2702 - rmse: 3.8317\n",
      "Epoch 94/500\n",
      "14046/14046 [==============================] - 8s 566us/sample - loss: 15.1086 - rmse: 3.8121\n",
      "Epoch 95/500\n",
      "14046/14046 [==============================] - 8s 562us/sample - loss: 15.0085 - rmse: 3.8003\n",
      "Epoch 96/500\n",
      "14046/14046 [==============================] - 8s 562us/sample - loss: 14.8221 - rmse: 3.7761\n",
      "Epoch 97/500\n",
      "14046/14046 [==============================] - 8s 561us/sample - loss: 14.6390 - rmse: 3.7537\n",
      "Epoch 98/500\n",
      "14046/14046 [==============================] - 8s 560us/sample - loss: 14.5510 - rmse: 3.7424\n",
      "Epoch 99/500\n",
      "14046/14046 [==============================] - 8s 567us/sample - loss: 14.3043 - rmse: 3.7108\n",
      "Epoch 100/500\n",
      "14046/14046 [==============================] - 8s 561us/sample - loss: 14.2230 - rmse: 3.6997\n",
      "Epoch 101/500\n",
      "14046/14046 [==============================] - 8s 562us/sample - loss: 14.0723 - rmse: 3.6804\n",
      "Epoch 102/500\n",
      "14046/14046 [==============================] - 8s 562us/sample - loss: 13.8740 - rmse: 3.6556\n",
      "Epoch 103/500\n",
      "14046/14046 [==============================] - 8s 565us/sample - loss: 13.7749 - rmse: 3.6431\n",
      "Epoch 104/500\n",
      "14046/14046 [==============================] - 8s 559us/sample - loss: 13.5341 - rmse: 3.6106\n",
      "Epoch 105/500\n",
      "14046/14046 [==============================] - 8s 559us/sample - loss: 13.5001 - rmse: 3.6055\n",
      "Epoch 106/500\n",
      "14046/14046 [==============================] - 8s 560us/sample - loss: 13.2801 - rmse: 3.5781\n",
      "Epoch 107/500\n",
      "14046/14046 [==============================] - 8s 563us/sample - loss: 13.2263 - rmse: 3.5691\n",
      "Epoch 108/500\n",
      "14046/14046 [==============================] - 8s 559us/sample - loss: 13.1212 - rmse: 3.5543\n",
      "Epoch 109/500\n",
      "14046/14046 [==============================] - 8s 565us/sample - loss: 12.9723 - rmse: 3.5340\n",
      "Epoch 110/500\n",
      "14046/14046 [==============================] - 9s 606us/sample - loss: 12.8591 - rmse: 3.5206\n",
      "Epoch 111/500\n",
      "14046/14046 [==============================] - 8s 588us/sample - loss: 12.6951 - rmse: 3.4972\n",
      "Epoch 112/500\n",
      "14046/14046 [==============================] - 8s 583us/sample - loss: 12.6029 - rmse: 3.4850\n",
      "Epoch 113/500\n",
      "14046/14046 [==============================] - 9s 613us/sample - loss: 12.4608 - rmse: 3.4645\n",
      "Epoch 114/500\n",
      "14046/14046 [==============================] - 8s 593us/sample - loss: 12.3315 - rmse: 3.4465\n",
      "Epoch 115/500\n",
      "14046/14046 [==============================] - 8s 590us/sample - loss: 12.2156 - rmse: 3.4309\n",
      "Epoch 116/500\n",
      "14046/14046 [==============================] - 8s 562us/sample - loss: 12.0845 - rmse: 3.4133\n",
      "Epoch 117/500\n",
      "14046/14046 [==============================] - 8s 563us/sample - loss: 12.0193 - rmse: 3.4034\n",
      "Epoch 118/500\n",
      "14046/14046 [==============================] - 8s 566us/sample - loss: 11.8253 - rmse: 3.3770\n",
      "Epoch 119/500\n",
      "14046/14046 [==============================] - 8s 567us/sample - loss: 11.7999 - rmse: 3.3714\n",
      "Epoch 120/500\n",
      "14046/14046 [==============================] - 8s 562us/sample - loss: 11.6923 - rmse: 3.3575\n",
      "Epoch 121/500\n",
      "14046/14046 [==============================] - 8s 561us/sample - loss: 11.5985 - rmse: 3.3430\n",
      "Epoch 122/500\n",
      "14046/14046 [==============================] - 8s 566us/sample - loss: 11.4176 - rmse: 3.3165\n",
      "Epoch 123/500\n",
      "14046/14046 [==============================] - 8s 562us/sample - loss: 11.4294 - rmse: 3.3187\n",
      "Epoch 124/500\n",
      "14046/14046 [==============================] - 8s 574us/sample - loss: 11.1683 - rmse: 3.2813\n",
      "Epoch 125/500\n",
      "14046/14046 [==============================] - 8s 565us/sample - loss: 11.1688 - rmse: 3.2793\n",
      "Epoch 126/500\n",
      "14046/14046 [==============================] - 8s 564us/sample - loss: 11.0203 - rmse: 3.2581\n",
      "Epoch 127/500\n",
      "14046/14046 [==============================] - 8s 569us/sample - loss: 11.0276 - rmse: 3.2586\n",
      "Epoch 128/500\n",
      "14046/14046 [==============================] - 8s 563us/sample - loss: 10.8699 - rmse: 3.2355\n",
      "Epoch 129/500\n",
      "14046/14046 [==============================] - 8s 565us/sample - loss: 10.7968 - rmse: 3.2258\n",
      "Epoch 130/500\n",
      "14046/14046 [==============================] - 8s 561us/sample - loss: 10.6577 - rmse: 3.2043\n",
      "Epoch 131/500\n",
      "14046/14046 [==============================] - 8s 564us/sample - loss: 10.6179 - rmse: 3.1972\n",
      "Epoch 132/500\n",
      "14046/14046 [==============================] - 8s 564us/sample - loss: 10.4740 - rmse: 3.1767\n",
      "Epoch 133/500\n",
      "14046/14046 [==============================] - 8s 564us/sample - loss: 10.4241 - rmse: 3.1691\n",
      "Epoch 134/500\n",
      "14046/14046 [==============================] - 8s 574us/sample - loss: 10.3288 - rmse: 3.1535\n",
      "Epoch 135/500\n",
      "14046/14046 [==============================] - 8s 567us/sample - loss: 10.1706 - rmse: 3.1299\n",
      "Epoch 136/500\n",
      "14046/14046 [==============================] - 8s 566us/sample - loss: 10.0680 - rmse: 3.1149\n",
      "Epoch 137/500\n",
      "14046/14046 [==============================] - 8s 567us/sample - loss: 10.0701 - rmse: 3.1132\n",
      "Epoch 138/500\n",
      "14046/14046 [==============================] - 8s 563us/sample - loss: 9.9333 - rmse: 3.0930\n",
      "Epoch 139/500\n",
      "14046/14046 [==============================] - 8s 568us/sample - loss: 9.8917 - rmse: 3.0856\n",
      "Epoch 140/500\n",
      "14046/14046 [==============================] - 8s 564us/sample - loss: 9.8106 - rmse: 3.0751\n",
      "Epoch 141/500\n",
      "14046/14046 [==============================] - 8s 564us/sample - loss: 9.7005 - rmse: 3.0562\n",
      "Epoch 142/500\n",
      "14046/14046 [==============================] - 8s 564us/sample - loss: 9.6291 - rmse: 3.0457\n",
      "Epoch 143/500\n",
      "14046/14046 [==============================] - 8s 565us/sample - loss: 9.5487 - rmse: 3.0315\n",
      "Epoch 144/500\n",
      "14046/14046 [==============================] - 8s 564us/sample - loss: 9.4917 - rmse: 3.0230\n",
      "Epoch 145/500\n",
      "14046/14046 [==============================] - 8s 565us/sample - loss: 9.3702 - rmse: 3.0042\n",
      "Epoch 146/500\n",
      "14046/14046 [==============================] - 8s 560us/sample - loss: 9.3125 - rmse: 2.9935\n",
      "Epoch 147/500\n",
      "14046/14046 [==============================] - 8s 561us/sample - loss: 9.2591 - rmse: 2.9844\n",
      "Epoch 148/500\n",
      "14046/14046 [==============================] - 8s 562us/sample - loss: 9.1725 - rmse: 2.9709\n",
      "Epoch 149/500\n",
      "14046/14046 [==============================] - 8s 563us/sample - loss: 9.1093 - rmse: 2.9610\n",
      "Epoch 150/500\n",
      "14046/14046 [==============================] - 8s 562us/sample - loss: 9.0088 - rmse: 2.9455\n",
      "Epoch 151/500\n",
      "14046/14046 [==============================] - 8s 580us/sample - loss: 9.0201 - rmse: 2.9469\n",
      "Epoch 152/500\n",
      "14046/14046 [==============================] - 8s 588us/sample - loss: 8.9403 - rmse: 2.9330\n",
      "Epoch 153/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14046/14046 [==============================] - 7s 518us/sample - loss: 8.7896 - rmse: 2.9091\n",
      "Epoch 154/500\n",
      "14046/14046 [==============================] - 7s 509us/sample - loss: 8.7343 - rmse: 2.8994\n",
      "Epoch 155/500\n",
      "14046/14046 [==============================] - 7s 504us/sample - loss: 8.6445 - rmse: 2.8849\n",
      "Epoch 156/500\n",
      "14046/14046 [==============================] - 7s 504us/sample - loss: 8.6403 - rmse: 2.8820\n",
      "Epoch 157/500\n",
      "14046/14046 [==============================] - 7s 503us/sample - loss: 8.5670 - rmse: 2.8719\n",
      "Epoch 158/500\n",
      "14046/14046 [==============================] - 7s 503us/sample - loss: 8.4693 - rmse: 2.8537\n",
      "Epoch 159/500\n",
      "14046/14046 [==============================] - 7s 504us/sample - loss: 8.4322 - rmse: 2.8486\n",
      "Epoch 160/500\n",
      "14046/14046 [==============================] - 7s 505us/sample - loss: 8.3361 - rmse: 2.8313\n",
      "Epoch 161/500\n",
      "14046/14046 [==============================] - 7s 508us/sample - loss: 8.2617 - rmse: 2.8199\n",
      "Epoch 162/500\n",
      "14046/14046 [==============================] - 7s 506us/sample - loss: 8.2907 - rmse: 2.8222\n",
      "Epoch 163/500\n",
      "14046/14046 [==============================] - 7s 505us/sample - loss: 8.1502 - rmse: 2.7995\n",
      "Epoch 164/500\n",
      "14046/14046 [==============================] - 7s 504us/sample - loss: 8.1211 - rmse: 2.7952\n",
      "Epoch 165/500\n",
      "14046/14046 [==============================] - 7s 503us/sample - loss: 8.0383 - rmse: 2.7802\n",
      "Epoch 166/500\n",
      "14046/14046 [==============================] - 7s 503us/sample - loss: 8.0335 - rmse: 2.7777\n",
      "Epoch 167/500\n",
      "14046/14046 [==============================] - 7s 504us/sample - loss: 7.9963 - rmse: 2.7733\n",
      "Epoch 168/500\n",
      "14046/14046 [==============================] - 7s 508us/sample - loss: 7.8819 - rmse: 2.7530\n",
      "Epoch 169/500\n",
      "14046/14046 [==============================] - 7s 510us/sample - loss: 7.8285 - rmse: 2.7435\n",
      "Epoch 170/500\n",
      "14046/14046 [==============================] - 7s 509us/sample - loss: 7.7711 - rmse: 2.7323\n",
      "Epoch 171/500\n",
      "14046/14046 [==============================] - 7s 510us/sample - loss: 7.6948 - rmse: 2.7198\n",
      "Epoch 172/500\n",
      "14046/14046 [==============================] - 7s 507us/sample - loss: 7.6975 - rmse: 2.7194\n",
      "Epoch 173/500\n",
      "14046/14046 [==============================] - 7s 508us/sample - loss: 7.6127 - rmse: 2.7042\n",
      "Epoch 174/500\n",
      "14046/14046 [==============================] - 7s 508us/sample - loss: 7.5836 - rmse: 2.6997\n",
      "Epoch 175/500\n",
      "14046/14046 [==============================] - 7s 506us/sample - loss: 7.5185 - rmse: 2.6872\n",
      "Epoch 176/500\n",
      "14046/14046 [==============================] - 7s 506us/sample - loss: 7.4661 - rmse: 2.6776\n",
      "Epoch 177/500\n",
      "14046/14046 [==============================] - 7s 513us/sample - loss: 7.4025 - rmse: 2.6659\n",
      "Epoch 178/500\n",
      "14046/14046 [==============================] - 7s 509us/sample - loss: 7.3281 - rmse: 2.6534\n",
      "Epoch 179/500\n",
      "14046/14046 [==============================] - 7s 509us/sample - loss: 7.3404 - rmse: 2.6543\n",
      "Epoch 180/500\n",
      "14046/14046 [==============================] - 7s 507us/sample - loss: 7.2719 - rmse: 2.6420\n",
      "Epoch 181/500\n",
      "14046/14046 [==============================] - 7s 507us/sample - loss: 7.1949 - rmse: 2.6297\n",
      "Epoch 182/500\n",
      "14046/14046 [==============================] - 7s 507us/sample - loss: 7.1644 - rmse: 2.6220\n",
      "Epoch 183/500\n",
      "14046/14046 [==============================] - 7s 506us/sample - loss: 7.1782 - rmse: 2.6237\n",
      "Epoch 184/500\n",
      "14046/14046 [==============================] - 7s 507us/sample - loss: 7.0722 - rmse: 2.6049\n",
      "Epoch 185/500\n",
      "14046/14046 [==============================] - 7s 508us/sample - loss: 7.0102 - rmse: 2.5939\n",
      "Epoch 186/500\n",
      "14046/14046 [==============================] - 7s 508us/sample - loss: 6.9981 - rmse: 2.5922\n",
      "Epoch 187/500\n",
      "14046/14046 [==============================] - 7s 512us/sample - loss: 6.9552 - rmse: 2.5836\n",
      "Epoch 188/500\n",
      "14046/14046 [==============================] - 7s 510us/sample - loss: 6.9432 - rmse: 2.5816\n",
      "Epoch 189/500\n",
      "14046/14046 [==============================] - 7s 506us/sample - loss: 6.8727 - rmse: 2.5674\n",
      "Epoch 190/500\n",
      "14046/14046 [==============================] - 7s 510us/sample - loss: 6.7739 - rmse: 2.5502\n",
      "Epoch 191/500\n",
      "14046/14046 [==============================] - 7s 506us/sample - loss: 6.7872 - rmse: 2.5512\n",
      "Epoch 192/500\n",
      "14046/14046 [==============================] - 7s 507us/sample - loss: 6.7035 - rmse: 2.5363\n",
      "Epoch 193/500\n",
      "14046/14046 [==============================] - 7s 508us/sample - loss: 6.6551 - rmse: 2.5268\n",
      "Epoch 194/500\n",
      "14046/14046 [==============================] - 7s 508us/sample - loss: 6.6204 - rmse: 2.5203\n",
      "Epoch 195/500\n",
      "14046/14046 [==============================] - 7s 506us/sample - loss: 6.6184 - rmse: 2.5190\n",
      "Epoch 196/500\n",
      "14046/14046 [==============================] - 7s 509us/sample - loss: 6.5362 - rmse: 2.5041\n",
      "Epoch 197/500\n",
      "14046/14046 [==============================] - 7s 507us/sample - loss: 6.5103 - rmse: 2.4983\n",
      "Epoch 198/500\n",
      "14046/14046 [==============================] - 7s 508us/sample - loss: 6.4722 - rmse: 2.4921\n",
      "Epoch 199/500\n",
      "14046/14046 [==============================] - 7s 509us/sample - loss: 6.4000 - rmse: 2.4766\n",
      "Epoch 200/500\n",
      "14046/14046 [==============================] - 7s 509us/sample - loss: 6.4121 - rmse: 2.4787\n",
      "Epoch 201/500\n",
      "14046/14046 [==============================] - 7s 509us/sample - loss: 6.3681 - rmse: 2.4706\n",
      "Epoch 202/500\n",
      "14046/14046 [==============================] - 7s 507us/sample - loss: 6.3229 - rmse: 2.4615\n",
      "Epoch 203/500\n",
      "14046/14046 [==============================] - 7s 508us/sample - loss: 6.3280 - rmse: 2.4618\n",
      "Epoch 204/500\n",
      "14046/14046 [==============================] - 7s 513us/sample - loss: 6.2515 - rmse: 2.4467\n",
      "Epoch 205/500\n",
      "14046/14046 [==============================] - 7s 512us/sample - loss: 6.2006 - rmse: 2.4382\n",
      "Epoch 206/500\n",
      "14046/14046 [==============================] - 7s 510us/sample - loss: 6.1650 - rmse: 2.4310\n",
      "Epoch 207/500\n",
      "14046/14046 [==============================] - 7s 511us/sample - loss: 6.1036 - rmse: 2.4190\n",
      "Epoch 208/500\n",
      "14046/14046 [==============================] - 7s 509us/sample - loss: 6.1188 - rmse: 2.4198\n",
      "Epoch 209/500\n",
      "14046/14046 [==============================] - 7s 508us/sample - loss: 6.1086 - rmse: 2.4195\n",
      "Epoch 210/500\n",
      "14046/14046 [==============================] - 8s 536us/sample - loss: 6.0525 - rmse: 2.4069\n",
      "Epoch 211/500\n",
      "14046/14046 [==============================] - 8s 543us/sample - loss: 6.0291 - rmse: 2.4025\n",
      "Epoch 212/500\n",
      "14046/14046 [==============================] - 7s 506us/sample - loss: 5.9397 - rmse: 2.3850\n",
      "Epoch 213/500\n",
      "14046/14046 [==============================] - 7s 506us/sample - loss: 5.9534 - rmse: 2.3869\n",
      "Epoch 214/500\n",
      "14046/14046 [==============================] - 7s 509us/sample - loss: 5.9385 - rmse: 2.3846\n",
      "Epoch 215/500\n",
      "14046/14046 [==============================] - 7s 508us/sample - loss: 5.8688 - rmse: 2.3693\n",
      "Epoch 216/500\n",
      "14046/14046 [==============================] - 7s 507us/sample - loss: 5.8367 - rmse: 2.3645\n",
      "Epoch 217/500\n",
      "14046/14046 [==============================] - 7s 505us/sample - loss: 5.8079 - rmse: 2.3570\n",
      "Epoch 218/500\n",
      "14046/14046 [==============================] - 7s 507us/sample - loss: 5.7877 - rmse: 2.3535\n",
      "Epoch 219/500\n",
      "14046/14046 [==============================] - 7s 509us/sample - loss: 5.7407 - rmse: 2.3446\n",
      "Epoch 220/500\n",
      "14046/14046 [==============================] - 7s 505us/sample - loss: 5.7200 - rmse: 2.3390\n",
      "Epoch 221/500\n",
      "14046/14046 [==============================] - 7s 508us/sample - loss: 5.7180 - rmse: 2.3389\n",
      "Epoch 222/500\n",
      "14046/14046 [==============================] - 7s 507us/sample - loss: 5.6814 - rmse: 2.3317\n",
      "Epoch 223/500\n",
      "14046/14046 [==============================] - 7s 510us/sample - loss: 5.6265 - rmse: 2.3211\n",
      "Epoch 224/500\n",
      "14046/14046 [==============================] - 7s 509us/sample - loss: 5.6310 - rmse: 2.3200\n",
      "Epoch 225/500\n",
      "14046/14046 [==============================] - 7s 506us/sample - loss: 5.5565 - rmse: 2.3045\n",
      "Epoch 226/500\n",
      "14046/14046 [==============================] - 7s 507us/sample - loss: 5.5725 - rmse: 2.3079\n",
      "Epoch 227/500\n",
      "14046/14046 [==============================] - 7s 507us/sample - loss: 5.5422 - rmse: 2.3013\n",
      "Epoch 228/500\n",
      "14046/14046 [==============================] - 7s 506us/sample - loss: 5.4876 - rmse: 2.2906\n",
      "Epoch 229/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14046/14046 [==============================] - 7s 503us/sample - loss: 5.4875 - rmse: 2.2884\n",
      "Epoch 230/500\n",
      "14046/14046 [==============================] - 7s 502us/sample - loss: 5.4569 - rmse: 2.2832\n",
      "Epoch 231/500\n",
      "14046/14046 [==============================] - 7s 502us/sample - loss: 5.3862 - rmse: 2.2677\n",
      "Epoch 232/500\n",
      "14046/14046 [==============================] - 7s 505us/sample - loss: 5.4024 - rmse: 2.2723\n",
      "Epoch 233/500\n",
      "14046/14046 [==============================] - 7s 503us/sample - loss: 5.3484 - rmse: 2.2588\n",
      "Epoch 234/500\n",
      "14046/14046 [==============================] - 7s 502us/sample - loss: 5.3176 - rmse: 2.2535\n",
      "Epoch 235/500\n",
      "14046/14046 [==============================] - 7s 502us/sample - loss: 5.3498 - rmse: 2.2592\n",
      "Epoch 236/500\n",
      "14046/14046 [==============================] - 7s 506us/sample - loss: 5.2747 - rmse: 2.2439\n",
      "Epoch 237/500\n",
      "14046/14046 [==============================] - 7s 501us/sample - loss: 5.2625 - rmse: 2.2410\n",
      "Epoch 238/500\n",
      "14046/14046 [==============================] - 7s 505us/sample - loss: 5.2648 - rmse: 2.2410\n",
      "Epoch 239/500\n",
      "14046/14046 [==============================] - 7s 506us/sample - loss: 5.2139 - rmse: 2.2308\n",
      "Epoch 240/500\n",
      "14046/14046 [==============================] - 7s 503us/sample - loss: 5.1673 - rmse: 2.2211\n",
      "Epoch 241/500\n",
      "14046/14046 [==============================] - 7s 504us/sample - loss: 5.1603 - rmse: 2.2188\n",
      "Epoch 242/500\n",
      "14046/14046 [==============================] - 7s 504us/sample - loss: 5.1340 - rmse: 2.2133\n",
      "Epoch 243/500\n",
      "14046/14046 [==============================] - 7s 501us/sample - loss: 5.0787 - rmse: 2.2008\n",
      "Epoch 244/500\n",
      "14046/14046 [==============================] - 7s 504us/sample - loss: 5.0741 - rmse: 2.1994\n",
      "Epoch 245/500\n",
      "14046/14046 [==============================] - 7s 502us/sample - loss: 5.0539 - rmse: 2.1952\n",
      "Epoch 246/500\n",
      "14046/14046 [==============================] - 7s 502us/sample - loss: 5.0369 - rmse: 2.1916\n",
      "Epoch 247/500\n",
      "14046/14046 [==============================] - 7s 502us/sample - loss: 5.0144 - rmse: 2.1862\n",
      "Epoch 248/500\n",
      "14046/14046 [==============================] - 7s 504us/sample - loss: 4.9832 - rmse: 2.1799\n",
      "Epoch 249/500\n",
      "14046/14046 [==============================] - 7s 503us/sample - loss: 4.9719 - rmse: 2.1775\n",
      "Epoch 250/500\n",
      "14046/14046 [==============================] - 7s 507us/sample - loss: 4.9317 - rmse: 2.1676\n",
      "Epoch 251/500\n",
      "14046/14046 [==============================] - 7s 505us/sample - loss: 4.9292 - rmse: 2.1661\n",
      "Epoch 252/500\n",
      "14046/14046 [==============================] - 7s 503us/sample - loss: 4.9192 - rmse: 2.1644\n",
      "Epoch 253/500\n",
      "14046/14046 [==============================] - 7s 504us/sample - loss: 4.8994 - rmse: 2.1598\n",
      "Epoch 254/500\n",
      "14046/14046 [==============================] - 7s 502us/sample - loss: 4.8422 - rmse: 2.1479\n",
      "Epoch 255/500\n",
      "14046/14046 [==============================] - 7s 509us/sample - loss: 4.8149 - rmse: 2.1417\n",
      "Epoch 256/500\n",
      "14046/14046 [==============================] - 7s 504us/sample - loss: 4.8512 - rmse: 2.1474\n",
      "Epoch 257/500\n",
      "14046/14046 [==============================] - 7s 504us/sample - loss: 4.8374 - rmse: 2.1457\n",
      "Epoch 258/500\n",
      "14046/14046 [==============================] - 7s 504us/sample - loss: 4.7541 - rmse: 2.1295\n",
      "Epoch 259/500\n",
      "14046/14046 [==============================] - 7s 505us/sample - loss: 4.7546 - rmse: 2.1265\n",
      "Epoch 260/500\n",
      "14046/14046 [==============================] - 7s 504us/sample - loss: 4.7290 - rmse: 2.1219\n",
      "Epoch 261/500\n",
      "14046/14046 [==============================] - 7s 508us/sample - loss: 4.7476 - rmse: 2.1231\n",
      "Epoch 262/500\n",
      "14046/14046 [==============================] - 7s 503us/sample - loss: 4.6907 - rmse: 2.1123\n",
      "Epoch 263/500\n",
      "14046/14046 [==============================] - 7s 505us/sample - loss: 4.6792 - rmse: 2.1109\n",
      "Epoch 264/500\n",
      "14046/14046 [==============================] - 7s 505us/sample - loss: 4.6574 - rmse: 2.1042\n",
      "Epoch 265/500\n",
      "14046/14046 [==============================] - 7s 503us/sample - loss: 4.6177 - rmse: 2.0967\n",
      "Epoch 266/500\n",
      "14046/14046 [==============================] - 7s 507us/sample - loss: 4.6270 - rmse: 2.0971\n",
      "Epoch 267/500\n",
      "14046/14046 [==============================] - 7s 505us/sample - loss: 4.6043 - rmse: 2.0928\n",
      "Epoch 268/500\n",
      "14046/14046 [==============================] - 7s 506us/sample - loss: 4.5482 - rmse: 2.0799\n",
      "Epoch 269/500\n",
      "14046/14046 [==============================] - 7s 506us/sample - loss: 4.5165 - rmse: 2.0731\n",
      "Epoch 270/500\n",
      "14046/14046 [==============================] - 7s 506us/sample - loss: 4.5601 - rmse: 2.0819\n",
      "Epoch 271/500\n",
      "14046/14046 [==============================] - 7s 505us/sample - loss: 4.5122 - rmse: 2.0705\n",
      "Epoch 272/500\n",
      "14046/14046 [==============================] - 7s 508us/sample - loss: 4.5251 - rmse: 2.0730\n",
      "Epoch 273/500\n",
      "14046/14046 [==============================] - 7s 506us/sample - loss: 4.5052 - rmse: 2.0682\n",
      "Epoch 274/500\n",
      "14046/14046 [==============================] - 7s 508us/sample - loss: 4.4585 - rmse: 2.0587\n",
      "Epoch 275/500\n",
      "14046/14046 [==============================] - 7s 506us/sample - loss: 4.4547 - rmse: 2.0573\n",
      "Epoch 276/500\n",
      "14046/14046 [==============================] - 7s 507us/sample - loss: 4.4366 - rmse: 2.0527\n",
      "Epoch 277/500\n",
      "14046/14046 [==============================] - 7s 507us/sample - loss: 4.4118 - rmse: 2.0468\n",
      "Epoch 278/500\n",
      "14046/14046 [==============================] - 7s 510us/sample - loss: 4.3982 - rmse: 2.0431\n",
      "Epoch 279/500\n",
      "14046/14046 [==============================] - 7s 506us/sample - loss: 4.3810 - rmse: 2.0395\n",
      "Epoch 280/500\n",
      "14046/14046 [==============================] - 7s 506us/sample - loss: 4.3426 - rmse: 2.0305\n",
      "Epoch 281/500\n",
      "14046/14046 [==============================] - 7s 506us/sample - loss: 4.3341 - rmse: 2.0279\n",
      "Epoch 282/500\n",
      "14046/14046 [==============================] - 7s 504us/sample - loss: 4.3242 - rmse: 2.0254\n",
      "Epoch 283/500\n",
      "14046/14046 [==============================] - 8s 534us/sample - loss: 4.3313 - rmse: 2.0279\n",
      "Epoch 284/500\n",
      "14046/14046 [==============================] - 8s 562us/sample - loss: 4.2846 - rmse: 2.0159\n",
      "Epoch 285/500\n",
      "14046/14046 [==============================] - 8s 599us/sample - loss: 4.2920 - rmse: 2.0167\n",
      "Epoch 286/500\n",
      "14046/14046 [==============================] - 9s 611us/sample - loss: 4.2787 - rmse: 2.0139\n",
      "Epoch 287/500\n",
      "14046/14046 [==============================] - 8s 590us/sample - loss: 4.2285 - rmse: 2.0025\n",
      "Epoch 288/500\n",
      "14046/14046 [==============================] - 8s 582us/sample - loss: 4.2218 - rmse: 2.0020\n",
      "Epoch 289/500\n",
      "14046/14046 [==============================] - 8s 572us/sample - loss: 4.2453 - rmse: 2.0038\n",
      "Epoch 290/500\n",
      "14046/14046 [==============================] - 8s 564us/sample - loss: 4.1742 - rmse: 1.9884\n",
      "Epoch 291/500\n",
      "14046/14046 [==============================] - 8s 563us/sample - loss: 4.1619 - rmse: 1.9879\n",
      "Epoch 292/500\n",
      "14046/14046 [==============================] - 8s 562us/sample - loss: 4.1772 - rmse: 1.9893\n",
      "Epoch 293/500\n",
      "14046/14046 [==============================] - 8s 564us/sample - loss: 4.1683 - rmse: 1.9860\n",
      "Epoch 294/500\n",
      "14046/14046 [==============================] - 8s 565us/sample - loss: 4.1141 - rmse: 1.9753\n",
      "Epoch 295/500\n",
      "14046/14046 [==============================] - 8s 564us/sample - loss: 4.1337 - rmse: 1.9786\n",
      "Epoch 296/500\n",
      "14046/14046 [==============================] - 8s 564us/sample - loss: 4.1153 - rmse: 1.9734\n",
      "Epoch 297/500\n",
      "14046/14046 [==============================] - 8s 563us/sample - loss: 4.0992 - rmse: 1.9697\n",
      "Epoch 298/500\n",
      "14046/14046 [==============================] - 8s 564us/sample - loss: 4.0671 - rmse: 1.9628\n",
      "Epoch 299/500\n",
      "14046/14046 [==============================] - 8s 562us/sample - loss: 4.0729 - rmse: 1.9623\n",
      "Epoch 300/500\n",
      "14046/14046 [==============================] - 8s 560us/sample - loss: 4.0392 - rmse: 1.9556\n",
      "Epoch 301/500\n",
      "14046/14046 [==============================] - 8s 572us/sample - loss: 4.0569 - rmse: 1.9590\n",
      "Epoch 302/500\n",
      "14046/14046 [==============================] - 8s 567us/sample - loss: 3.9946 - rmse: 1.9444\n",
      "Epoch 303/500\n",
      "14046/14046 [==============================] - 8s 573us/sample - loss: 3.9910 - rmse: 1.9432\n",
      "Epoch 304/500\n",
      "14046/14046 [==============================] - 8s 564us/sample - loss: 4.0101 - rmse: 1.9481\n",
      "Epoch 305/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14046/14046 [==============================] - 7s 519us/sample - loss: 3.9533 - rmse: 1.9333\n",
      "Epoch 306/500\n",
      "14046/14046 [==============================] - 7s 514us/sample - loss: 3.9658 - rmse: 1.9364\n",
      "Epoch 307/500\n",
      "14046/14046 [==============================] - 7s 514us/sample - loss: 3.9861 - rmse: 1.9409\n",
      "Epoch 308/500\n",
      "14046/14046 [==============================] - 7s 513us/sample - loss: 3.9345 - rmse: 1.9283\n",
      "Epoch 309/500\n",
      "14046/14046 [==============================] - 7s 516us/sample - loss: 3.9631 - rmse: 1.9340\n",
      "Epoch 310/500\n",
      "14046/14046 [==============================] - 7s 514us/sample - loss: 3.9101 - rmse: 1.9218\n",
      "Epoch 311/500\n",
      "14046/14046 [==============================] - 7s 519us/sample - loss: 3.9009 - rmse: 1.9193\n",
      "Epoch 312/500\n",
      "14046/14046 [==============================] - 7s 513us/sample - loss: 3.8747 - rmse: 1.9126\n",
      "Epoch 313/500\n",
      "14046/14046 [==============================] - 7s 512us/sample - loss: 3.9069 - rmse: 1.9207\n",
      "Epoch 314/500\n",
      "14046/14046 [==============================] - 7s 515us/sample - loss: 3.8583 - rmse: 1.9088\n",
      "Epoch 315/500\n",
      "14046/14046 [==============================] - 7s 515us/sample - loss: 3.8458 - rmse: 1.9053\n",
      "Epoch 316/500\n",
      "14046/14046 [==============================] - 7s 513us/sample - loss: 3.8454 - rmse: 1.9050\n",
      "Epoch 317/500\n",
      "14046/14046 [==============================] - 7s 515us/sample - loss: 3.8176 - rmse: 1.8973\n",
      "Epoch 318/500\n",
      "14046/14046 [==============================] - 7s 514us/sample - loss: 3.8107 - rmse: 1.8948\n",
      "Epoch 319/500\n",
      "14046/14046 [==============================] - 7s 516us/sample - loss: 3.7928 - rmse: 1.8921\n",
      "Epoch 320/500\n",
      "14046/14046 [==============================] - 7s 526us/sample - loss: 3.7644 - rmse: 1.8834\n",
      "Epoch 321/500\n",
      "14046/14046 [==============================] - 7s 517us/sample - loss: 3.7727 - rmse: 1.8852\n",
      "Epoch 322/500\n",
      "14046/14046 [==============================] - 7s 517us/sample - loss: 3.7913 - rmse: 1.8889\n",
      "Epoch 323/500\n",
      "14046/14046 [==============================] - 7s 516us/sample - loss: 3.7479 - rmse: 1.8789\n",
      "Epoch 324/500\n",
      "14046/14046 [==============================] - 7s 514us/sample - loss: 3.7407 - rmse: 1.8770\n",
      "Epoch 325/500\n",
      "14046/14046 [==============================] - 7s 516us/sample - loss: 3.7120 - rmse: 1.8709\n",
      "Epoch 326/500\n",
      "14046/14046 [==============================] - 7s 518us/sample - loss: 3.7115 - rmse: 1.8705\n",
      "Epoch 327/500\n",
      "14046/14046 [==============================] - 7s 515us/sample - loss: 3.7344 - rmse: 1.8746\n",
      "Epoch 328/500\n",
      "14046/14046 [==============================] - 8s 544us/sample - loss: 3.6915 - rmse: 1.8654\n",
      "Epoch 329/500\n",
      "14046/14046 [==============================] - 8s 540us/sample - loss: 3.6863 - rmse: 1.8624\n",
      "Epoch 330/500\n",
      "14046/14046 [==============================] - 7s 518us/sample - loss: 3.6755 - rmse: 1.8611\n",
      "Epoch 331/500\n",
      "14046/14046 [==============================] - 7s 505us/sample - loss: 3.6440 - rmse: 1.8528\n",
      "Epoch 332/500\n",
      "14046/14046 [==============================] - 7s 509us/sample - loss: 3.6328 - rmse: 1.8491\n",
      "Epoch 333/500\n",
      "14046/14046 [==============================] - 7s 504us/sample - loss: 3.6431 - rmse: 1.8512\n",
      "Epoch 334/500\n",
      "14046/14046 [==============================] - 7s 508us/sample - loss: 3.6375 - rmse: 1.8513\n",
      "Epoch 335/500\n",
      "14046/14046 [==============================] - 7s 505us/sample - loss: 3.6078 - rmse: 1.8423\n",
      "Epoch 336/500\n",
      "14046/14046 [==============================] - 7s 517us/sample - loss: 3.5993 - rmse: 1.8404\n",
      "Epoch 337/500\n",
      "14046/14046 [==============================] - 7s 512us/sample - loss: 3.5879 - rmse: 1.8370\n",
      "Epoch 338/500\n",
      "14046/14046 [==============================] - 7s 510us/sample - loss: 3.5857 - rmse: 1.8348\n",
      "Epoch 339/500\n",
      "14046/14046 [==============================] - 7s 509us/sample - loss: 3.5835 - rmse: 1.8358\n",
      "Epoch 340/500\n",
      "14046/14046 [==============================] - 7s 506us/sample - loss: 3.5817 - rmse: 1.8334\n",
      "Epoch 341/500\n",
      "14046/14046 [==============================] - 7s 508us/sample - loss: 3.5240 - rmse: 1.8204\n",
      "Epoch 342/500\n",
      "14046/14046 [==============================] - 7s 510us/sample - loss: 3.5505 - rmse: 1.8261\n",
      "Epoch 343/500\n",
      "14046/14046 [==============================] - 7s 506us/sample - loss: 3.5349 - rmse: 1.8215\n",
      "Epoch 344/500\n",
      "14046/14046 [==============================] - 7s 507us/sample - loss: 3.5230 - rmse: 1.8190\n",
      "Epoch 345/500\n",
      "14046/14046 [==============================] - 7s 506us/sample - loss: 3.5405 - rmse: 1.8224\n",
      "Epoch 346/500\n",
      "14046/14046 [==============================] - 7s 506us/sample - loss: 3.4859 - rmse: 1.8097\n",
      "Epoch 347/500\n",
      "14046/14046 [==============================] - 7s 511us/sample - loss: 3.4738 - rmse: 1.8059\n",
      "Epoch 348/500\n",
      "14046/14046 [==============================] - 7s 506us/sample - loss: 3.4866 - rmse: 1.8098\n",
      "Epoch 349/500\n",
      "14046/14046 [==============================] - 7s 505us/sample - loss: 3.4708 - rmse: 1.8040\n",
      "Epoch 350/500\n",
      "14046/14046 [==============================] - 7s 505us/sample - loss: 3.4755 - rmse: 1.8050\n",
      "Epoch 351/500\n",
      "14046/14046 [==============================] - 7s 509us/sample - loss: 3.4421 - rmse: 1.7975\n",
      "Epoch 352/500\n",
      "14046/14046 [==============================] - 7s 506us/sample - loss: 3.4370 - rmse: 1.7957\n",
      "Epoch 353/500\n",
      "14046/14046 [==============================] - 7s 510us/sample - loss: 3.4342 - rmse: 1.7944\n",
      "Epoch 354/500\n",
      "14046/14046 [==============================] - 7s 507us/sample - loss: 3.4319 - rmse: 1.7930\n",
      "Epoch 355/500\n",
      "14046/14046 [==============================] - 7s 506us/sample - loss: 3.4124 - rmse: 1.7882\n",
      "Epoch 356/500\n",
      "14046/14046 [==============================] - 7s 510us/sample - loss: 3.3950 - rmse: 1.7853\n",
      "Epoch 357/500\n",
      "14046/14046 [==============================] - 7s 505us/sample - loss: 3.3757 - rmse: 1.7794\n",
      "Epoch 358/500\n",
      "14046/14046 [==============================] - 7s 507us/sample - loss: 3.3808 - rmse: 1.7801\n",
      "Epoch 359/500\n",
      "14046/14046 [==============================] - 7s 512us/sample - loss: 3.3973 - rmse: 1.7820\n",
      "Epoch 360/500\n",
      "14046/14046 [==============================] - 8s 535us/sample - loss: 3.3333 - rmse: 1.7683\n",
      "Epoch 361/500\n",
      "14046/14046 [==============================] - 7s 518us/sample - loss: 3.3699 - rmse: 1.7762\n",
      "Epoch 362/500\n",
      "14046/14046 [==============================] - 9s 637us/sample - loss: 3.3393 - rmse: 1.7692\n",
      "Epoch 363/500\n",
      "14046/14046 [==============================] - 7s 524us/sample - loss: 3.3190 - rmse: 1.7646\n",
      "Epoch 364/500\n",
      "14046/14046 [==============================] - 9s 630us/sample - loss: 3.3481 - rmse: 1.7710\n",
      "Epoch 365/500\n",
      "14046/14046 [==============================] - 7s 527us/sample - loss: 3.2971 - rmse: 1.7575\n",
      "Epoch 366/500\n",
      "14046/14046 [==============================] - 8s 575us/sample - loss: 3.3273 - rmse: 1.7634\n",
      "Epoch 367/500\n",
      "14046/14046 [==============================] - 8s 552us/sample - loss: 3.3224 - rmse: 1.7625\n",
      "Epoch 368/500\n",
      "14046/14046 [==============================] - 8s 547us/sample - loss: 3.2782 - rmse: 1.7526\n",
      "Epoch 369/500\n",
      "14046/14046 [==============================] - 8s 602us/sample - loss: 3.2635 - rmse: 1.7482\n",
      "Epoch 370/500\n",
      "14046/14046 [==============================] - 10s 722us/sample - loss: 3.2740 - rmse: 1.7512\n",
      "Epoch 371/500\n",
      "14046/14046 [==============================] - 8s 596us/sample - loss: 3.2507 - rmse: 1.7438\n",
      "Epoch 372/500\n",
      "14046/14046 [==============================] - 8s 543us/sample - loss: 3.2349 - rmse: 1.7398\n",
      "Epoch 373/500\n",
      "14046/14046 [==============================] - 8s 539us/sample - loss: 3.2402 - rmse: 1.7398\n",
      "Epoch 374/500\n",
      "14046/14046 [==============================] - 8s 552us/sample - loss: 3.2419 - rmse: 1.7412\n",
      "Epoch 375/500\n",
      "14046/14046 [==============================] - 8s 544us/sample - loss: 3.2304 - rmse: 1.7370\n",
      "Epoch 376/500\n",
      "14046/14046 [==============================] - 8s 544us/sample - loss: 3.2249 - rmse: 1.7348\n",
      "Epoch 377/500\n",
      "14046/14046 [==============================] - 8s 537us/sample - loss: 3.2265 - rmse: 1.7359\n",
      "Epoch 378/500\n",
      "14046/14046 [==============================] - 8s 543us/sample - loss: 3.1845 - rmse: 1.7252\n",
      "Epoch 379/500\n",
      "14046/14046 [==============================] - 8s 535us/sample - loss: 3.1723 - rmse: 1.7235\n",
      "Epoch 380/500\n",
      "14046/14046 [==============================] - 8s 543us/sample - loss: 3.1758 - rmse: 1.7218\n",
      "Epoch 381/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14046/14046 [==============================] - 8s 548us/sample - loss: 3.1895 - rmse: 1.7245\n",
      "Epoch 382/500\n",
      "14046/14046 [==============================] - 8s 555us/sample - loss: 3.1592 - rmse: 1.7178\n",
      "Epoch 383/500\n",
      "14046/14046 [==============================] - 8s 545us/sample - loss: 3.1715 - rmse: 1.7204\n",
      "Epoch 384/500\n",
      "14046/14046 [==============================] - 8s 545us/sample - loss: 3.1487 - rmse: 1.7146\n",
      "Epoch 385/500\n",
      "14046/14046 [==============================] - 8s 546us/sample - loss: 3.1438 - rmse: 1.7140\n",
      "Epoch 386/500\n",
      "14046/14046 [==============================] - 10s 726us/sample - loss: 3.1656 - rmse: 1.7166\n",
      "Epoch 387/500\n",
      "14046/14046 [==============================] - 8s 600us/sample - loss: 3.1056 - rmse: 1.7034\n",
      "Epoch 388/500\n",
      "14046/14046 [==============================] - 7s 533us/sample - loss: 3.1465 - rmse: 1.7108\n",
      "Epoch 389/500\n",
      "14046/14046 [==============================] - 7s 532us/sample - loss: 3.1026 - rmse: 1.7023\n",
      "Epoch 390/500\n",
      "14046/14046 [==============================] - 7s 529us/sample - loss: 3.0969 - rmse: 1.7003\n",
      "Epoch 391/500\n",
      "14046/14046 [==============================] - 7s 529us/sample - loss: 3.1054 - rmse: 1.7017\n",
      "Epoch 392/500\n",
      "14046/14046 [==============================] - 7s 530us/sample - loss: 3.1293 - rmse: 1.7069\n",
      "Epoch 393/500\n",
      "14046/14046 [==============================] - 7s 531us/sample - loss: 3.0946 - rmse: 1.6983\n",
      "Epoch 394/500\n",
      "14046/14046 [==============================] - 7s 532us/sample - loss: 3.0757 - rmse: 1.6930\n",
      "Epoch 395/500\n",
      "14046/14046 [==============================] - 8s 540us/sample - loss: 3.0641 - rmse: 1.6907\n",
      "Epoch 396/500\n",
      "14046/14046 [==============================] - 8s 601us/sample - loss: 3.0425 - rmse: 1.6839\n",
      "Epoch 397/500\n",
      "14046/14046 [==============================] - 8s 560us/sample - loss: 3.0680 - rmse: 1.6902\n",
      "Epoch 398/500\n",
      "14046/14046 [==============================] - 7s 528us/sample - loss: 3.0251 - rmse: 1.6796\n",
      "Epoch 399/500\n",
      "14046/14046 [==============================] - 7s 529us/sample - loss: 3.0390 - rmse: 1.6819\n",
      "Epoch 400/500\n",
      "14046/14046 [==============================] - 8s 559us/sample - loss: 3.0667 - rmse: 1.6889\n",
      "Epoch 401/500\n",
      "14046/14046 [==============================] - 7s 526us/sample - loss: 3.0516 - rmse: 1.6848\n",
      "Epoch 402/500\n",
      "14046/14046 [==============================] - 7s 533us/sample - loss: 3.0414 - rmse: 1.6807\n",
      "Epoch 403/500\n",
      "14046/14046 [==============================] - 7s 528us/sample - loss: 3.0141 - rmse: 1.6746\n",
      "Epoch 404/500\n",
      "14046/14046 [==============================] - 7s 531us/sample - loss: 3.0379 - rmse: 1.6807\n",
      "Epoch 405/500\n",
      "14046/14046 [==============================] - 9s 638us/sample - loss: 3.0205 - rmse: 1.6765\n",
      "Epoch 406/500\n",
      "14046/14046 [==============================] - 8s 534us/sample - loss: 3.0053 - rmse: 1.6720\n",
      "Epoch 407/500\n",
      "14046/14046 [==============================] - 8s 535us/sample - loss: 2.9815 - rmse: 1.6644\n",
      "Epoch 408/500\n",
      "14046/14046 [==============================] - 7s 530us/sample - loss: 2.9911 - rmse: 1.6660\n",
      "Epoch 409/500\n",
      "14046/14046 [==============================] - 7s 530us/sample - loss: 2.9857 - rmse: 1.6669\n",
      "Epoch 410/500\n",
      "14046/14046 [==============================] - 7s 530us/sample - loss: 2.9882 - rmse: 1.6650\n",
      "Epoch 411/500\n",
      "14046/14046 [==============================] - 7s 527us/sample - loss: 2.9983 - rmse: 1.6658\n",
      "Epoch 412/500\n",
      "14046/14046 [==============================] - 7s 533us/sample - loss: 2.9437 - rmse: 1.6537\n",
      "Epoch 413/500\n",
      "14046/14046 [==============================] - 8s 543us/sample - loss: 2.9452 - rmse: 1.6527\n",
      "Epoch 414/500\n",
      "14046/14046 [==============================] - 8s 575us/sample - loss: 2.9414 - rmse: 1.6528\n",
      "Epoch 415/500\n",
      "14046/14046 [==============================] - 8s 601us/sample - loss: 2.9315 - rmse: 1.6496\n",
      "Epoch 416/500\n",
      "14046/14046 [==============================] - 8s 541us/sample - loss: 2.9312 - rmse: 1.6494\n",
      "Epoch 417/500\n",
      "14046/14046 [==============================] - 8s 541us/sample - loss: 2.9303 - rmse: 1.6483\n",
      "Epoch 418/500\n",
      "14046/14046 [==============================] - 8s 546us/sample - loss: 2.9381 - rmse: 1.6496\n",
      "Epoch 419/500\n",
      "14046/14046 [==============================] - 8s 582us/sample - loss: 2.9001 - rmse: 1.6405\n",
      "Epoch 420/500\n",
      "14046/14046 [==============================] - 8s 583us/sample - loss: 2.8695 - rmse: 1.6324\n",
      "Epoch 421/500\n",
      "14046/14046 [==============================] - 8s 541us/sample - loss: 2.8917 - rmse: 1.6373\n",
      "Epoch 422/500\n",
      "14046/14046 [==============================] - 8s 555us/sample - loss: 2.8931 - rmse: 1.6369\n",
      "Epoch 423/500\n",
      "14046/14046 [==============================] - 8s 534us/sample - loss: 2.8829 - rmse: 1.6359\n",
      "Epoch 424/500\n",
      "14046/14046 [==============================] - 7s 533us/sample - loss: 2.8709 - rmse: 1.6320\n",
      "Epoch 425/500\n",
      "14046/14046 [==============================] - 8s 536us/sample - loss: 2.8806 - rmse: 1.6325\n",
      "Epoch 426/500\n",
      "14046/14046 [==============================] - 8s 534us/sample - loss: 2.8984 - rmse: 1.6389\n",
      "Epoch 427/500\n",
      "14046/14046 [==============================] - 8s 534us/sample - loss: 2.8725 - rmse: 1.6296\n",
      "Epoch 428/500\n",
      "14046/14046 [==============================] - 8s 536us/sample - loss: 2.8504 - rmse: 1.6237\n",
      "Epoch 429/500\n",
      "14046/14046 [==============================] - 8s 534us/sample - loss: 2.8620 - rmse: 1.6257\n",
      "Epoch 430/500\n",
      "14046/14046 [==============================] - 8s 535us/sample - loss: 2.8569 - rmse: 1.6263\n",
      "Epoch 431/500\n",
      "14046/14046 [==============================] - 8s 544us/sample - loss: 2.8593 - rmse: 1.6259\n",
      "Epoch 432/500\n",
      "14046/14046 [==============================] - 8s 536us/sample - loss: 2.8322 - rmse: 1.6187\n",
      "Epoch 433/500\n",
      "14046/14046 [==============================] - 7s 533us/sample - loss: 2.8313 - rmse: 1.6171\n",
      "Epoch 434/500\n",
      "14046/14046 [==============================] - 8s 536us/sample - loss: 2.8156 - rmse: 1.6144\n",
      "Epoch 435/500\n",
      "14046/14046 [==============================] - 8s 539us/sample - loss: 2.8396 - rmse: 1.6190\n",
      "Epoch 436/500\n",
      "14046/14046 [==============================] - 8s 547us/sample - loss: 2.7961 - rmse: 1.6087\n",
      "Epoch 437/500\n",
      "14046/14046 [==============================] - 8s 541us/sample - loss: 2.8216 - rmse: 1.6154\n",
      "Epoch 438/500\n",
      "14046/14046 [==============================] - 8s 549us/sample - loss: 2.8035 - rmse: 1.6084\n",
      "Epoch 439/500\n",
      "14046/14046 [==============================] - 8s 549us/sample - loss: 2.7745 - rmse: 1.6019\n",
      "Epoch 440/500\n",
      "14046/14046 [==============================] - 8s 542us/sample - loss: 2.7543 - rmse: 1.5972\n",
      "Epoch 441/500\n",
      "14046/14046 [==============================] - 8s 536us/sample - loss: 2.7596 - rmse: 1.5982\n",
      "Epoch 442/500\n",
      "14046/14046 [==============================] - 8s 564us/sample - loss: 2.7691 - rmse: 1.5994\n",
      "Epoch 443/500\n",
      "14046/14046 [==============================] - 8s 542us/sample - loss: 2.7439 - rmse: 1.5928\n",
      "Epoch 444/500\n",
      "14046/14046 [==============================] - 8s 571us/sample - loss: 2.7959 - rmse: 1.6060\n",
      "Epoch 445/500\n",
      "14046/14046 [==============================] - 8s 540us/sample - loss: 2.7536 - rmse: 1.5939\n",
      "Epoch 446/500\n",
      "14046/14046 [==============================] - 8s 543us/sample - loss: 2.7641 - rmse: 1.5965\n",
      "Epoch 447/500\n",
      "14046/14046 [==============================] - 8s 540us/sample - loss: 2.7570 - rmse: 1.5936\n",
      "Epoch 448/500\n",
      "14046/14046 [==============================] - 9s 621us/sample - loss: 2.7640 - rmse: 1.5947\n",
      "Epoch 449/500\n",
      "14046/14046 [==============================] - 8s 598us/sample - loss: 2.7642 - rmse: 1.5950\n",
      "Epoch 450/500\n",
      "14046/14046 [==============================] - 8s 553us/sample - loss: 2.7362 - rmse: 1.5884\n",
      "Epoch 451/500\n",
      "14046/14046 [==============================] - 8s 581us/sample - loss: 2.7183 - rmse: 1.5820\n",
      "Epoch 452/500\n",
      "14046/14046 [==============================] - 8s 552us/sample - loss: 2.6867 - rmse: 1.5741\n",
      "Epoch 453/500\n",
      "14046/14046 [==============================] - 9s 632us/sample - loss: 2.7108 - rmse: 1.5804\n",
      "Epoch 454/500\n",
      "14046/14046 [==============================] - 8s 548us/sample - loss: 2.7006 - rmse: 1.5784\n",
      "Epoch 455/500\n",
      "14046/14046 [==============================] - 8s 558us/sample - loss: 2.7157 - rmse: 1.5825\n",
      "Epoch 456/500\n",
      "14046/14046 [==============================] - 8s 544us/sample - loss: 2.6977 - rmse: 1.5764\n",
      "Epoch 457/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14046/14046 [==============================] - 9s 676us/sample - loss: 2.6806 - rmse: 1.5717\n",
      "Epoch 458/500\n",
      "14046/14046 [==============================] - 8s 574us/sample - loss: 2.7144 - rmse: 1.5811\n",
      "Epoch 459/500\n",
      "14046/14046 [==============================] - 7s 534us/sample - loss: 2.7062 - rmse: 1.5779\n",
      "Epoch 460/500\n",
      "14046/14046 [==============================] - 8s 575us/sample - loss: 2.6674 - rmse: 1.5678\n",
      "Epoch 461/500\n",
      "14046/14046 [==============================] - 8s 551us/sample - loss: 2.6749 - rmse: 1.5690\n",
      "Epoch 462/500\n",
      "14046/14046 [==============================] - 8s 546us/sample - loss: 2.6838 - rmse: 1.5706\n",
      "Epoch 463/500\n",
      "14046/14046 [==============================] - 8s 551us/sample - loss: 2.6439 - rmse: 1.5602\n",
      "Epoch 464/500\n",
      "14046/14046 [==============================] - 8s 543us/sample - loss: 2.6601 - rmse: 1.5626\n",
      "Epoch 465/500\n",
      "14046/14046 [==============================] - 8s 539us/sample - loss: 2.6292 - rmse: 1.5540\n",
      "Epoch 466/500\n",
      "14046/14046 [==============================] - 8s 540us/sample - loss: 2.6226 - rmse: 1.5518\n",
      "Epoch 467/500\n",
      "14046/14046 [==============================] - 8s 544us/sample - loss: 2.6310 - rmse: 1.5553\n",
      "Epoch 468/500\n",
      "14046/14046 [==============================] - 8s 540us/sample - loss: 2.6431 - rmse: 1.5581\n",
      "Epoch 469/500\n",
      "14046/14046 [==============================] - 8s 540us/sample - loss: 2.6011 - rmse: 1.5471\n",
      "Epoch 470/500\n",
      "14046/14046 [==============================] - 8s 540us/sample - loss: 2.6356 - rmse: 1.5565\n",
      "Epoch 471/500\n",
      "14046/14046 [==============================] - 8s 539us/sample - loss: 2.6101 - rmse: 1.5487\n",
      "Epoch 472/500\n",
      "14046/14046 [==============================] - 8s 543us/sample - loss: 2.5979 - rmse: 1.5443\n",
      "Epoch 473/500\n",
      "14046/14046 [==============================] - 8s 541us/sample - loss: 2.6380 - rmse: 1.5562\n",
      "Epoch 474/500\n",
      "14046/14046 [==============================] - 8s 539us/sample - loss: 2.6255 - rmse: 1.5513\n",
      "Epoch 475/500\n",
      "14046/14046 [==============================] - 8s 544us/sample - loss: 2.6064 - rmse: 1.5468\n",
      "Epoch 476/500\n",
      "14046/14046 [==============================] - 8s 579us/sample - loss: 2.5972 - rmse: 1.5456\n",
      "Epoch 477/500\n",
      "14046/14046 [==============================] - 8s 567us/sample - loss: 2.5963 - rmse: 1.5431\n",
      "Epoch 478/500\n",
      "14046/14046 [==============================] - 8s 566us/sample - loss: 2.5898 - rmse: 1.5417\n",
      "Epoch 479/500\n",
      "14046/14046 [==============================] - 8s 561us/sample - loss: 2.5809 - rmse: 1.5398\n",
      "Epoch 480/500\n",
      "14046/14046 [==============================] - 8s 603us/sample - loss: 2.5796 - rmse: 1.5378\n",
      "Epoch 481/500\n",
      "14046/14046 [==============================] - 8s 579us/sample - loss: 2.5604 - rmse: 1.5349\n",
      "Epoch 482/500\n",
      "14046/14046 [==============================] - 8s 591us/sample - loss: 2.5847 - rmse: 1.5372\n",
      "Epoch 483/500\n",
      "14046/14046 [==============================] - 8s 566us/sample - loss: 2.5770 - rmse: 1.5369\n",
      "Epoch 484/500\n",
      "14046/14046 [==============================] - 8s 570us/sample - loss: 2.5976 - rmse: 1.5416\n",
      "Epoch 485/500\n",
      "14046/14046 [==============================] - 8s 555us/sample - loss: 2.5634 - rmse: 1.5329\n",
      "Epoch 486/500\n",
      "14046/14046 [==============================] - 8s 554us/sample - loss: 2.5826 - rmse: 1.5369\n",
      "Epoch 487/500\n",
      "14046/14046 [==============================] - 8s 585us/sample - loss: 2.5486 - rmse: 1.5280\n",
      "Epoch 488/500\n",
      "14046/14046 [==============================] - 9s 619us/sample - loss: 2.5432 - rmse: 1.5259\n",
      "Epoch 489/500\n",
      "14046/14046 [==============================] - 8s 601us/sample - loss: 2.5687 - rmse: 1.5314\n",
      "Epoch 490/500\n",
      "14046/14046 [==============================] - 8s 605us/sample - loss: 2.5448 - rmse: 1.5262\n",
      "Epoch 491/500\n",
      "14046/14046 [==============================] - 8s 582us/sample - loss: 2.5504 - rmse: 1.5267\n",
      "Epoch 492/500\n",
      "14046/14046 [==============================] - 8s 588us/sample - loss: 2.5458 - rmse: 1.5265\n",
      "Epoch 493/500\n",
      "14046/14046 [==============================] - 8s 601us/sample - loss: 2.5168 - rmse: 1.5188\n",
      "Epoch 494/500\n",
      "14046/14046 [==============================] - 8s 571us/sample - loss: 2.5214 - rmse: 1.5191\n",
      "Epoch 495/500\n",
      "14046/14046 [==============================] - 8s 576us/sample - loss: 2.5145 - rmse: 1.5169\n",
      "Epoch 496/500\n",
      "14046/14046 [==============================] - 8s 553us/sample - loss: 2.5395 - rmse: 1.5224\n",
      "Epoch 497/500\n",
      "14046/14046 [==============================] - 9s 610us/sample - loss: 2.5428 - rmse: 1.5241\n",
      "Epoch 498/500\n",
      "14046/14046 [==============================] - 9s 615us/sample - loss: 2.4959 - rmse: 1.5116\n",
      "Epoch 499/500\n",
      "14046/14046 [==============================] - 8s 582us/sample - loss: 2.5071 - rmse: 1.5134\n",
      "Epoch 500/500\n",
      "14046/14046 [==============================] - 9s 617us/sample - loss: 2.4825 - rmse: 1.5085\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "USE_SAVED_MODEL = False\n",
    "\n",
    "if USE_SAVED_MODEL == False:\n",
    "    history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs = 500,\n",
    "                    batch_size = 256,\n",
    "                    validation_split = 0.2, #data = (x_test, y_test),\n",
    "                    callbacks = callbacks\n",
    "                    )\n",
    "    plt.style.use('seaborn-darkgrid')\n",
    "    plt.plot(history.history['rmse'])\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Root Mean Square Error')\n",
    "    plt.title('Model Training Error')\n",
    "    plt.show() \n",
    "    \n",
    "else:\n",
    "    model.load_weights(model_dir+\"base_model_weights_1.h5\")\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookid_dir = '../input/IdLookupTable.csv'\n",
    "lookid_data = pd.read_csv(lookid_dir)\n",
    "test_data = pd.read_csv(test_file)\n",
    "\n",
    "x_test = []\n",
    "for i in range(0,len(test_data)):\n",
    "    img = test_data['Image'][i].split(' ')\n",
    "    x_test.append(img)\n",
    "    \n",
    "x_test = np.array(x_test,dtype = 'float')\n",
    "x_test = x_test/255.0\n",
    "x_test = x_test.reshape(-1,96,96,1)    \n",
    "\n",
    "y_test = model.predict(x_test)\n",
    "y_test = np.clip(y_test,0,96)\n",
    "\n",
    "lookid_list = list(lookid_data['FeatureName'])\n",
    "imageID = list(lookid_data['ImageId']-1)\n",
    "pred_list = list(y_test)\n",
    "\n",
    "rowid = list(lookid_data['RowId'])\n",
    "\n",
    "feature = []\n",
    "for f in list(lookid_data['FeatureName']):\n",
    "    feature.append(lookid_list.index(f))\n",
    "    \n",
    "    \n",
    "submit_data = []\n",
    "for x,y in zip(imageID,feature):\n",
    "    submit_data.append(pred_list[x][y])\n",
    "rowid = pd.Series(rowid,name = 'RowId')\n",
    "loc = pd.Series(submit_data,name = 'Location')\n",
    "submission = pd.concat([rowid,loc],axis = 1)\n",
    "submission.to_csv('../output/w207_base_submission_1.csv',index = False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    sess = get_session()\n",
    "    clear_session()\n",
    "    sess.close()\n",
    "    sess = get_session()\n",
    "    try:\n",
    "        del model # this is from global space - change this as you need\n",
    "    except:\n",
    "        print(\"Model clear Failed\")\n",
    "    print(gc.collect())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
