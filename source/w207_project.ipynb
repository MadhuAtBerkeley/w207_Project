{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4dbced162440c393a0a5b7e5aee344711a30e994"
   },
   "source": [
    "# Facial Keypoint Detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "369fa247a546e39d82bdfdc5b7d4ed58baa40e4f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/madhuhegde/.pyenv/versions/miniconda3-latest/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/madhuhegde/.pyenv/versions/miniconda3-latest/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/madhuhegde/.pyenv/versions/miniconda3-latest/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/madhuhegde/.pyenv/versions/miniconda3-latest/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/madhuhegde/.pyenv/versions/miniconda3-latest/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/madhuhegde/.pyenv/versions/miniconda3-latest/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.backend.tensorflow_backend import clear_session\n",
    "from keras.backend.tensorflow_backend import get_session\n",
    "from tensorflow.keras.layers import LeakyReLU, ReLU\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Activation, Convolution2D, BatchNormalization, Flatten\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, MaxPool2D\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "from keras import backend\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import cv2\n",
    "import os, gc, json, math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n",
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print(keras.__version__)\n",
    "import tensorflow\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pixels(data):\n",
    "    \"\"\"\n",
    "    Convert pixels to the right intensity 0-1 and in a square matrix.\n",
    "    \"\"\"\n",
    "    data = np.array([row.split(' ') for row in data['Image']],dtype='float') / 255.0\n",
    "    data = data.reshape(-1,96,96,1)\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_img(sample_img,coord=None):\n",
    "    \"\"\"\n",
    "    Display an image. For debugging, display a coordinate on the image.\n",
    "    input:\n",
    "        - sample_img: numpy array. image to be displayed\n",
    "        - coord: lst. of coordinates in form [[x_coordinate,y_coordinate],[x_coordinate,y_coordinate]]\n",
    "    TODO handle multiple coordinates. Work out bugs with multiple coordinates\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.imshow(sample_img.reshape(96,96),cmap='gray')\n",
    "    if coord is not None:\n",
    "        plt.scatter(coord[0],coord[1],marker = '*',c='r')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_facial_keypoints(data,ind):\n",
    "    \"\"\"\n",
    "    Structure the coordinates for all facial keypoints for a single image.\n",
    "    inputs:\n",
    "        - data: numpy array containing rows as each image sample and columns as facial keypoint coordinates\n",
    "        - ind: index of the image\n",
    "    output:\n",
    "        - numpy array with format [[list of x-coordinates],[list of y-coordinates]]\n",
    "    \"\"\"\n",
    "    data[ind]\n",
    "    it = iter(data[ind])\n",
    "    x_coord = []\n",
    "    y_coord = []\n",
    "\n",
    "    for x in it:\n",
    "        x_coord.append(x)\n",
    "        y_coord.append(next(it))\n",
    "    \n",
    "    return(np.array([x_coord,y_coord]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "# Reset Keras Session\n",
    "def reset_keras():\n",
    "    sess = get_session()\n",
    "    clear_session()\n",
    "    sess.close()\n",
    "    sess = get_session()\n",
    "\n",
    "    try:\n",
    "        del model # this is from global space - change this as you need\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    print(gc.collect()) # if it's done something you should see a number being outputted\n",
    "\n",
    "    # use the same config as you used to create the session\n",
    "    config = tensorflow.ConfigProto()\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = 1\n",
    "    config.gpu_options.visible_device_list = \"0\"\n",
    "    set_session(tensorflow.Session(config=config))\n",
    "    \n",
    "reset_keras()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "fa1b76273d02502e3fd668dddf74ecf522044524"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('../output/'):\n",
    "    os.makedirs('../output/model')\n",
    "    os.makedirs('../output/history')\n",
    "    \n",
    "    \n",
    "model_dir = \"../output/model/\"\n",
    "history_dir = \"../output/history/\"\n",
    "\n",
    "train_file = '../input/training/training.csv'\n",
    "test_file = '../input/test/test.csv'\n",
    "train_data = pd.read_csv(train_file)  \n",
    "#test_data = pd.read_csv(test_file)\n",
    "\n",
    "\n",
    "bad_samples = [1747, 1731, 1877, 1881, 1979, 2199, 2289, 2321, 2453, 3173, 3296, 3447, 4180, 6859,\n",
    "              2090, 2175, 1907, 2562, 2818, 3296, 3447, 4263, 4482, 4490, 4636, 5059, 6493, 6585, 6906]\n",
    "\n",
    "train_data = train_data.drop(bad_samples).reset_index(drop=True)\n",
    "train_clean = train_data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "cfd045f7166f9cce2e2075b3ead83813d07012c8",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2140, 31)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_clean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e1b88f1528838c0a8fec61f9a02a70b5077312e9"
   },
   "source": [
    "Create training vector with images and normalize thee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = convert_pixels(train_data)\n",
    "x_clean = convert_pixels(train_clean)\n",
    "y_train = train_data[[col for col in train_data.columns if col != 'Image']].to_numpy()\n",
    "y_clean = train_clean[[col for col in train_clean.columns if col != 'Image']].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a833f4cc5e559774d3a310fd09d40d31e49e71da"
   },
   "source": [
    "Generate labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "e9d804a035809cdf8ffda19f41ce3feb278a38fb"
   },
   "outputs": [],
   "source": [
    "\n",
    "#imputer = KNNImputer(n_neighbors=3, weights='distance')\n",
    "\n",
    "imputer = IterativeImputer(max_iter=1000, tol=0.01, random_state=1)\n",
    "y_train = imputer.fit_transform(y_train)\n",
    "\n",
    "#imputer = IterativeImputer(max_iter=1000, tol=0.01, random_state=2)\n",
    "#y_clean = imputer.fit_transform(y_clean)\n",
    "\n",
    "bad_bottom_lip = [210, 350, 499, 512, 810, 839, 895, 1058, 1194,1230, 1245, 1546, 1548]\n",
    "for sample in bad_bottom_lip:\n",
    "    y_train[sample][29] = 94\n",
    "    y_train[sample][28] = y_train[sample][26]\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set feature engineering parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_na = False\n",
    "add_flip_horiz = True\n",
    "add_blur_img = False\n",
    "add_rotate_img = False\n",
    "orig_x_train = x_clean.copy()\n",
    "#orig_y_train = y_clean.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill NA in the training labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if fill_na:\n",
    "    # https://stackoverflow.com/questions/18689235/numpy-array-replace-nan-values-with-average-of-columns\n",
    "    # get column means\n",
    "    col_mean = np.nanmean(y_train,axis=0)\n",
    "\n",
    "    # find the x,y indices that are missing from y_train\n",
    "    inds = np.where(np.isnan(y_train))\n",
    "\n",
    "    # fill in missing values in y_train with the column means. \"take\" is much more efficient than fancy indexing\n",
    "    y_train[inds] = np.take(col_mean, inds[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flip images horizontally and add to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_img_horiz(train_data):\n",
    "    \"\"\"\n",
    "    Flip images horizontally for all training images\n",
    "    \"\"\"\n",
    "    # Flip images\n",
    "    x_train = convert_pixels(train_data)\n",
    "    flip_img = np.array([np.fliplr(x_train[[ind]][0]) for ind in range(x_train.shape[0])])\n",
    "    \n",
    "    # Flip coordinates\n",
    "    train_data_flip = train_data.copy()\n",
    "    x_columns = [col for col in train_data.columns if '_x' in col]\n",
    "    train_data_flip[x_columns] = train_data[x_columns].applymap(lambda x: 96-x)\n",
    "    \n",
    "    #left and right are swapped so undo\n",
    "    left_columns = [col for col in train_data.columns if 'left' in col]\n",
    "    right_columns = [col for col in train_data.columns if 'right' in col]\n",
    "    train_data_flip[left_columns+right_columns] = train_data_flip[right_columns+left_columns]\n",
    "    \n",
    "    flip_coord = train_data_flip[[col for col in train_data if col != 'Image']].to_numpy()\n",
    "    return(flip_img,flip_coord)\n",
    "\n",
    "if add_flip_horiz:\n",
    "    # Apply the augmentation and add the new data to the training set\n",
    "    flipped_img,flipped_coord = flip_img_horiz(train_clean)\n",
    "    \n",
    "   \n",
    "    x_train = np.append(x_train,flipped_img,axis=0)\n",
    "    y_train = np.append(y_train,flipped_coord,axis=0)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Gaussian blurring with a 5x5 filter with $\\sigma$ = 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9162, 30)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blur_img():\n",
    "    \"\"\"\n",
    "    Add Gaussian blurring to the images\n",
    "    \"\"\"\n",
    "    # https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_filtering/py_filtering.html\n",
    "    blur_img = np.array([cv2.GaussianBlur(orig_x_train[[ind]][0],(5,5),2).reshape(96,96,1) for ind in range(orig_x_train.shape[0])])\n",
    "    \n",
    "    return(blur_img)\n",
    "\n",
    "if add_blur_img:\n",
    "    x_train = np.append(x_train,blur_img(),axis=0)\n",
    "    y_train = np.append(y_train,orig_y_train,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_img(x_train, y_train):\n",
    "    \"\"\"\"\n",
    "    Rotate images by angles between [5, 10, 14 degrees]\n",
    "    \"\"\"\n",
    "    angles = [5, -5, 10, -10, 14, -14]\n",
    "    b = np.ones((1,3))\n",
    "    rows,cols = (96,96)\n",
    "    x_train_rot = []\n",
    "    y_train_rot = y_train.copy()\n",
    "    M_angles = [cv2.getRotationMatrix2D((cols/2,rows/2),angle,1) for angle in angles]\n",
    "    \n",
    "    for i in range(x_train.shape[0]):\n",
    "        #M = cv2.getRotationMatrix2D((cols/2,rows/2),np.random.choice(angles,1),1)\n",
    "        M = M_angles[np.random.choice(len(M_angles))]\n",
    "        x_train_rot.append((cv2.warpAffine(x_train[[i]].reshape(rows,cols,1),M,(cols,rows)).reshape(96,96,1)))\n",
    "       \n",
    "        #apply affine transformation to (x,y) labels\n",
    "        for j in range(int(y_train.shape[1]/2)):\n",
    "            b[:,0:2] = y_train[i,2*j:2*j+2]\n",
    "            y_train_rot[i,2*j:2*j+2] = np.dot(b,M.transpose()) \n",
    "    \n",
    "    x_train_rot = np.array(x_train_rot)\n",
    "    return x_train_rot, y_train_rot\n",
    "\n",
    "if add_rotate_img:\n",
    "    \n",
    "    x_rotate, y_rotate = rotate_img(x_train,y_train)\n",
    "    x_train = np.append(x_train,x_rotate,axis=0)\n",
    "    y_train = np.append(y_train,y_rotate,axis=0)   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callback function if detailed log required\n",
    "class History(tensorflow.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.train_loss = []\n",
    "        self.train_rmse = []\n",
    "        self.val_rmse = []\n",
    "        self.val_loss = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.train_loss.append(logs.get('loss'))\n",
    "        self.train_rmse.append(logs.get('rmse'))\n",
    "        \n",
    "    def on_epoch_end(self, batch, logs={}):    \n",
    "        self.val_rmse.append(logs.get('val_rmse'))\n",
    "        self.val_loss.append(logs.get('val_loss'))\n",
    "        \n",
    "# Implement ModelCheckPoint callback function to save CNN model\n",
    "class CNN_ModelCheckpoint(tensorflow.keras.callbacks.Callback):\n",
    "\n",
    "    def __init__(self, model, filename):\n",
    "        self.filename = filename\n",
    "        self.cnn_model = model\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.max_val_rmse = math.inf\n",
    "        \n",
    " \n",
    "    def on_epoch_end(self, batch, logs={}):    \n",
    "        val_rmse = logs.get('val_rmse')\n",
    "        if(val_rmse < self.max_val_rmse):\n",
    "           self.max_val_rmse = val_rmse\n",
    "           self.cnn_model.save_weights(self.filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 96, 96, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 96, 96, 32)        288       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 96, 96, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_12 (B (None, 96, 96, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 48, 48, 64)        18432     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 48, 48, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_13 (B (None, 48, 48, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 48, 48, 64)        36864     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 48, 48, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_14 (B (None, 48, 48, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 24, 24, 96)        55296     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 24, 24, 96)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_15 (B (None, 24, 24, 96)        384       \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 24, 24, 96)        82944     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 24, 24, 96)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_16 (B (None, 24, 24, 96)        384       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 12, 12, 96)        0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 12, 12, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 12, 12, 128)       110592    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)   (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_17 (B (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 12, 12, 128)       147456    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)   (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_18 (B (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 6, 6, 256)         294912    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)   (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_19 (B (None, 6, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 6, 6, 256)         589824    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)   (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_20 (B (None, 6, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 3, 3, 512)         1179648   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)   (None, 3, 3, 512)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_21 (B (None, 3, 3, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 3, 3, 512)         2359296   \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_22 (B (None, 3, 3, 512)         2048      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               2359808   \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 30)                15390     \n",
      "=================================================================\n",
      "Total params: 7,259,326\n",
      "Trainable params: 7,255,038\n",
      "Non-trainable params: 4,288\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def base_model():\n",
    "    model_input = Input(shape=(96,96,1))\n",
    "\n",
    "    x = Convolution2D(16, (3,3), padding='same', use_bias=False)(model_input)\n",
    "    x = LeakyReLU(alpha = 0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Convolution2D(16, (3,3), padding='same', use_bias=False)(model_input)\n",
    "    x = LeakyReLU(alpha = 0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Convolution2D(64, (3,3), padding='same', use_bias=False)(x)\n",
    "    x = LeakyReLU(alpha = 0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Convolution2D(64, (3,3), padding='same', use_bias=False)(x)\n",
    "    x = LeakyReLU(alpha = 0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Convolution2D(96, (3,3), padding='same', use_bias=False)(x)\n",
    "    x = LeakyReLU(alpha = 0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Convolution2D(96, (3,3), padding='same', use_bias=False)(x)\n",
    "    x = LeakyReLU(alpha = 0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Convolution2D(128, (3,3),padding='same', use_bias=False)(x)\n",
    "    x = LeakyReLU(alpha = 0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Convolution2D(128, (3,3),padding='same', use_bias=False)(x)\n",
    "    x = LeakyReLU(alpha = 0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    #x = Dropout(0.1)(x)\n",
    "    x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Convolution2D(256, (3,3),padding='same',use_bias=False)(x)\n",
    "    x = LeakyReLU(alpha = 0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Convolution2D(256, (3,3),padding='same',use_bias=False)(x)\n",
    "    x = LeakyReLU(alpha = 0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Convolution2D(512, (3,3), padding='same', use_bias=False)(x)\n",
    "    x = LeakyReLU(alpha = 0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Convolution2D(512, (3,3), activation='relu', padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(512,activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    model_output = Dense(30)(x)\n",
    "    model = Model(model_input, model_output, name=\"base_model\")\n",
    "    return model\n",
    "\n",
    "model = base_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/madhuhegde/.pyenv/versions/miniconda3-latest/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 96, 96, 32)        288       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 96, 96, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_2 (Ba (None, 96, 96, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 96, 96, 32)        9216      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 96, 96, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_3 (Ba (None, 96, 96, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 48, 48, 64)        18432     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 48, 48, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_4 (Ba (None, 48, 48, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 48, 48, 64)        36864     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 48, 48, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_5 (Ba (None, 48, 48, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 24, 24, 96)        55296     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 24, 24, 96)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_6 (Ba (None, 24, 24, 96)        384       \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 24, 24, 96)        82944     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 24, 24, 96)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_7 (Ba (None, 24, 24, 96)        384       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 12, 12, 96)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 12, 12, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 12, 12, 128)       110592    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_8 (Ba (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 12, 12, 128)       147456    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_9 (Ba (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 6, 6, 256)         294912    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_10 (B (None, 6, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 6, 6, 256)         589824    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_11 (B (None, 6, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 3, 3, 512)         1179648   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 3, 3, 512)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_12 (B (None, 3, 3, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 3, 3, 512)         2359296   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 3, 3, 512)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_13 (B (None, 3, 3, 512)         2048      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 3, 3, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 3, 3, 1024)        4718592   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 3, 3, 1024)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_14 (B (None, 3, 3, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 3, 3, 1024)        9437184   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 3, 3, 1024)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_15 (B (None, 3, 3, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              9438208   \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                30750     \n",
      "=================================================================\n",
      "Total params: 28,526,398\n",
      "Trainable params: 28,517,950\n",
      "Non-trainable params: 8,448\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "def final_model():               \n",
    "    \n",
    "                model = Sequential()\n",
    "\n",
    "                # Input dimensions: (None, 96, 96, 1)\n",
    "                model.add(Convolution2D(32, (3,3), padding='same', use_bias=False, input_shape=(96,96,1)))\n",
    "                model.add(LeakyReLU(alpha = 0.1))\n",
    "                model.add(BatchNormalization())\n",
    "                # Input dimensions: (None, 96, 96, 32)\n",
    "                model.add(Convolution2D(32, (3,3), padding='same', use_bias=False))\n",
    "                model.add(LeakyReLU(alpha = 0.1))\n",
    "                model.add(BatchNormalization())\n",
    "                model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "                # CDB: 3/5 DROPOUT ADDED\n",
    "                model.add(Dropout(0.2))\n",
    "\n",
    "                # Input dimensions: (None, 48, 48, 32)\n",
    "                model.add(Convolution2D(64, (3,3), padding='same', use_bias=False))\n",
    "                model.add(LeakyReLU(alpha = 0.1))\n",
    "                model.add(BatchNormalization())\n",
    "                # Input dimensions: (None, 48, 48, 64)\n",
    "                model.add(Convolution2D(64, (3,3), padding='same', use_bias=False))\n",
    "                model.add(LeakyReLU(alpha = 0.1))\n",
    "                model.add(BatchNormalization())\n",
    "                model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "                # CDB: 3/5 DROPOUT ADDED\n",
    "                model.add(Dropout(0.25))\n",
    "\n",
    "                # Input dimensions: (None, 24, 24, 64)\n",
    "                model.add(Convolution2D(96, (3,3), padding='same', use_bias=False))\n",
    "                model.add(LeakyReLU(alpha = 0.1))\n",
    "                model.add(BatchNormalization())\n",
    "                # Input dimensions: (None, 24, 24, 96)\n",
    "                model.add(Convolution2D(96, (3,3), padding='same', use_bias=False))\n",
    "                model.add(LeakyReLU(alpha = 0.1))\n",
    "                model.add(BatchNormalization())\n",
    "                model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "                # CDB: 3/5 DROPOUT ADDED\n",
    "                model.add(Dropout(0.15))\n",
    "\n",
    "                # Input dimensions: (None, 12, 12, 96)\n",
    "                model.add(Convolution2D(128, (3,3),padding='same', use_bias=False))\n",
    "                model.add(LeakyReLU(alpha = 0.1))\n",
    "                model.add(BatchNormalization())\n",
    "                # Input dimensions: (None, 12, 12, 128)\n",
    "                model.add(Convolution2D(128, (3,3),padding='same', use_bias=False))\n",
    "                model.add(LeakyReLU(alpha = 0.1))\n",
    "                model.add(BatchNormalization())\n",
    "                model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "                # CDB: 3/5 DROPOUT ADDED\n",
    "                model.add(Dropout(0.3))\n",
    "\n",
    "                # Input dimensions: (None, 6, 6, 128)\n",
    "                model.add(Convolution2D(256, (3,3),padding='same',use_bias=False))\n",
    "                model.add(LeakyReLU(alpha = 0.1))\n",
    "                model.add(BatchNormalization())\n",
    "                # Input dimensions: (None, 6, 6, 256)\n",
    "                model.add(Convolution2D(256, (3,3),padding='same',use_bias=False))\n",
    "                model.add(LeakyReLU(alpha = 0.1))\n",
    "                model.add(BatchNormalization())\n",
    "                model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "                # CDB: 3/5 DROPOUT ADDED\n",
    "                model.add(Dropout(0.2))\n",
    "\n",
    "                # Input dimensions: (None, 3, 3, 256)\n",
    "                model.add(Convolution2D(512, (3,3), padding='same', use_bias=False))\n",
    "                model.add(LeakyReLU(alpha = 0.1))\n",
    "                model.add(BatchNormalization())\n",
    "                # Input dimensions: (None, 3, 3, 512)\n",
    "                model.add(Convolution2D(512, (3,3), padding='same', use_bias=False))\n",
    "                model.add(LeakyReLU(alpha = 0.1))\n",
    "                model.add(BatchNormalization())\n",
    "\n",
    "                # TEST added 4/8\n",
    "                model.add(Dropout(0.3))\n",
    "                model.add(Convolution2D(1024, (3,3), padding='same', use_bias=False))\n",
    "                model.add(LeakyReLU(alpha = 0.1))\n",
    "                model.add(BatchNormalization())\n",
    "                # Input dimensions: (None, 3, 3, 512)\n",
    "                model.add(Convolution2D(1024, (3,3), padding='same', use_bias=False))\n",
    "                model.add(LeakyReLU(alpha = 0.1))\n",
    "                model.add(BatchNormalization())\n",
    "\n",
    "                # Input dimensions: (None, 3, 3, 512)\n",
    "                model.add(Flatten())\n",
    "                model.add(Dense(1024,activation='relu'))\n",
    "                \n",
    "                # CDB DROPOUT INCREASED FROM 0.1 to 0.2\n",
    "                model.add(Dropout(0.15))\n",
    "        \n",
    "                model.add(Dense(30))\n",
    "            \n",
    "                return(model)\n",
    "            \n",
    "model = final_model()\n",
    "model.summary()                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/madhuhegde/.pyenv/versions/miniconda3-latest/lib/python3.7/site-packages/tensorflow/python/keras/utils/losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "# Custom RMSE metric\n",
    "def rmse(y_true, y_pred):\n",
    "    return backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))\n",
    "\n",
    "#from tensorflow.keras.optimizers.schedules import InverseTimeDecay\n",
    "\n",
    "# Use Nadam optimizer with variable learning rate\n",
    "optimizer = Nadam(lr=0.00001,\n",
    "                  beta_1=0.9,\n",
    "                  beta_2=0.999,\n",
    "                  epsilon=1e-08,\n",
    "                  schedule_decay=0.004)\n",
    "\n",
    "\n",
    "# Loss: MSE and Metric = RMSE\n",
    "model.compile(optimizer= optimizer, \n",
    "              loss='mean_squared_error',\n",
    "              metrics=[rmse])\n",
    "\n",
    "#Callback to save the best model\n",
    "saveBase_Model = CNN_ModelCheckpoint(model, model_dir+\"base_model_weights_1.h5\")\n",
    "\n",
    "#define callback functions\n",
    "callbacks = [#EarlyStopping(monitor='val_rmse', patience=3, verbose=2),\n",
    "             saveBase_Model]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e4cf4686b410841f2e34dbb081f3429d1b0f67e9"
   },
   "source": [
    "Run for 1000 epochs and keeping 20% train-valid split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "894af9cbfcf2dca50e7407946cad318157b77d0a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7329 samples, validate on 1833 samples\n",
      "WARNING:tensorflow:From /Users/madhuhegde/.pyenv/versions/miniconda3-latest/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/1000\n",
      " 256/7329 [>.............................] - ETA: 11:44 - loss: 2648.8848 - rmse: 51.4419"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-86c6d90dae1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m                     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                     \u001b[0mvalidation_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m#data = (x_test, y_test),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m                     )\n\u001b[1;32m     12\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'seaborn-darkgrid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/miniconda3-latest/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    878\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/.pyenv/versions/miniconda3-latest/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/miniconda3-latest/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3076\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[0;32m~/.pyenv/versions/miniconda3-latest/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "USE_SAVED_MODEL = False\n",
    "\n",
    "if USE_SAVED_MODEL == False:\n",
    "    history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs = 1000,\n",
    "                    batch_size = 256,\n",
    "                    validation_split = 0.2, #data = (x_test, y_test),\n",
    "                    callbacks = callbacks\n",
    "                    )\n",
    "    plt.style.use('seaborn-darkgrid')\n",
    "    plt.plot(history.history['rmse'])\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Root Mean Square Error')\n",
    "    plt.title('Model Training Error')\n",
    "    plt.show() \n",
    "    \n",
    "else:\n",
    "    model.load_weights(model_dir+\"base_model_weights_1.h5\")\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookid_dir = '../input/IdLookupTable.csv'\n",
    "lookid_data = pd.read_csv(lookid_dir)\n",
    "test_data = pd.read_csv(test_file)\n",
    "\n",
    "x_test = []\n",
    "for i in range(0,len(test_data)):\n",
    "    img = test_data['Image'][i].split(' ')\n",
    "    x_test.append(img)\n",
    "    \n",
    "x_test = np.array(x_test,dtype = 'float')\n",
    "x_test = x_test/255.0\n",
    "x_test = x_test.reshape(-1,96,96,1)    \n",
    "\n",
    "y_test = model.predict(x_test)\n",
    "y_test = np.clip(y_test,0,96)\n",
    "\n",
    "lookid_list = list(lookid_data['FeatureName'])\n",
    "imageID = list(lookid_data['ImageId']-1)\n",
    "pred_list = list(y_test)\n",
    "\n",
    "rowid = list(lookid_data['RowId'])\n",
    "\n",
    "feature = []\n",
    "for f in list(lookid_data['FeatureName']):\n",
    "    feature.append(lookid_list.index(f))\n",
    "    \n",
    "    \n",
    "submit_data = []\n",
    "for x,y in zip(imageID,feature):\n",
    "    submit_data.append(pred_list[x][y])\n",
    "rowid = pd.Series(rowid,name = 'RowId')\n",
    "loc = pd.Series(submit_data,name = 'Location')\n",
    "submission = pd.concat([rowid,loc],axis = 1)\n",
    "submission.to_csv('../output/w207_base_submission_1.csv',index = False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    sess = get_session()\n",
    "    clear_session()\n",
    "    sess.close()\n",
    "    sess = get_session()\n",
    "    try:\n",
    "        del model # this is from global space - change this as you need\n",
    "    except:\n",
    "        print(\"Model clear Failed\")\n",
    "    print(gc.collect())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
