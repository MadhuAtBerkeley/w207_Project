{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capsule Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand intuition behind capsule networks follow these blogs: \n",
    "1. [Capsule Networks](https://theailearner.com/2019/01/21/capsule-networks/)\n",
    "2. [Implementing Capsule Network in Keras](https://theailearner.com/2019/01/21/implementing-capsule-network-in-keras/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A capsule is a group of neurons which uses vectors to represent an object or object part. Length of a vector represents presence of an object and orientation of vector represents its pose(size, position, orientation, etc). Group of these capsules forms a capsule layer and then these layers lead to form a capsule network.\n",
    "\n",
    "We can break the implementation of capsule network into following steps:\n",
    "\n",
    "1. Initial convolutional layer\n",
    "2. Primary capsule layer\n",
    "3. Digit capsule layer\n",
    "4. Decoder network\n",
    "5. Loss Functions\n",
    "6. Training and testing of model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv2D, Dense, Input, Reshape, Lambda, Layer, Flatten\n",
    "from keras.layers import LeakyReLU, BatchNormalization, MaxPool2D\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import initializers\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers.core import Activation\n",
    "import pandas as pd\n",
    "from keras.optimizers import Nadam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Kaggle Face Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJztfVusZVd15VhVZfMwGKr8KMqusssg22AMsZFpCLRaVpy00nQU8gMCiZY7Tcs/6Q4JkYLp/oj6j48oCh+tSBbpiO6gThBBwUJRQuQ0iAbZ2NjGEJeN3YXtcj1cD4xtwOBy1eqPe8bd84wz1rr7uOxzrjlzSFfn3LPX3mvttR9zrjnHnLPUWpFIJFYPW5Y9gEQisRzkw59IrCjy4U8kVhT58CcSK4p8+BOJFUU+/InEiiIf/kRiRXFGD38p5ddLKQ+WUh4updz8Yg0qkUi89CgvlORTStkK4PsAfg3A4wDuBPDhWuv9L97wEonES4VtZ7DvvwDwcK11PwCUUv4KwPsBNB/+c845p+7YsWPqt61btwIAzjrrLADAli2zyghfUPw8deoUAODnP//5epuf/OQnAICTJ09OtZ2MbeqTOH36dPPkuH8cz7Zt25pj1L56x9Q2bp/ecXr761z1jjOmrx7G9NFr05t/tp9HOI2ZM3e81v0FAM8///zUJ4/D+xbo37sEt51zzjkAhnsp4uyzz55q+0IE86FDh/Dkk0+OuqBn8vBfDOBA+P9xAO/SRqWUmwDcBADbt2/Hxz/+8amLzpfBBRdcAAB49atfPdMRJ/65554DAPz4xz8GADz00EPrbb797W8DAA4ePAhgeAkAw4Xi5HJS48tDb1Lu/6pXvWq9zfnnnw8AeOUrXznVNl701osGGG523S/eSARvKNcH2/MzbtOblW3cC0Jv5NhGb+R4zbg/f2PbeV9CnH89DjA8HDwPPV48ppsjQufIHYfX+mc/+xkA4Kc//en6tmPHjgEAjh8/PjWuc889d73NxRdfDGC4v7RvYLhn3vWutUeE93scy969e6faxntYXwStl9mHP/xhjMWZPPzu7TLzqqq13gLgFgDYs2dP1cHyovBm1wck/sbJOHr0KABg//79621+9KMfTR2Xx4vgxXUPNl86Tz/9NADgFa94BQDgda973XobvrX1Zo8PRu/hV81BH+K4jXA3q0qoKKn0OO7B5ng5R/zkgxb7cGPU68HPePPrPOgLwx3bnaO+BOJ1bb3EenPlXph6XnGMfMgpeJ599tmp8cTvr33tawEM9xUf4tgv790o5NiHPgvueozR+sbiTAx+jwPYE/7fDeDQGRwvkUgsEGfy8N8J4PJSymWllLMBfAjArS/OsBKJxEuNF6z211qfL6X8JwD/AGArgP9Ra/3nMfu69aNTbwlVs6mCxTU726iaCAxqqapnVOPjsfjb61//egB9G4QzAKq6G1VhnjdVUO4fz1mXPc7wyDbcxiVKRG/5oUsBZ3jjNo7VLaN6yx9trwYzh55dgnMV++DYdM0fjWlqAHbLGEJtObEPtufykgbm+BvvlV27dgEY7iFg2kYAAM8888z6dy4TOGe8Z3ifx/GPMa6OxZms+VFr/TsAf3cmx0gkEsvBGT38LwSllKk3s76BncRS4wy3RakcDXMA8NRTT61/V6n+mte8BsBgAAQGiULvA9tEyc3jqMTrWbnj27hleXZQI1acJ353VnJ+V+3AjVGlcdQgeppYS3rGcagR0kksNXDF40VLd0T8XTUwPa/YL4/NbVGD0PsrGuq4P8fI/mn4i8dUb8HOnTvX25x33nlTx47zqoZkno/TYHRcztg8FknvTSRWFAuV/KUUbN261a4x+ZbjGzpKDl33Uspfeuml620olemPjX3wzcq1VZQMBO0A27dvBzBIwfjG7fmTCdUKnNup9X/sT9/+rk+nQbQkbm/M2tbtpz7s2FdvPc7fnBuO18zZPtx9AEzbEnSMYwhiPb6AG2PUAoDhXopcAPJOuI6nuziu2fW+iMclf0RdhVEzbdlwXihBC0jJn0isLBYu+bdt22bXKY7co6CE4BvyscceW99Giyvf3tGST+mhkjKucWmNpR3BrZV1bI7corRNdz5KYHHte5KfUkPX9/FcFbGNSlonOVXiOxahtu15Ftz1VanqWJmU9I6VybEpc9Npa0oycnOmrMr4nefItrEPSmh+0h5AjcCN1T0DlPiqBcc2Y56TsUjJn0isKPLhTyRWFAtV+0+fPo3nnnuuawRzao2qt4cPHwYAHDlyZL2NGvMiiUJdSvyf7jxgMCIqrzqOQ9X0ntruor90PM5gp5x8Lk16/H/nOlX13+2vZB93ro7c03KtuSWHqtvxeLokiMswVXPdfUHVOxrWNkIvmKoXGKTzGo1xNNjpPMR4kx/+8IcAgLe+9a0ApklCLUOwm0+9Vr3IyI2Qkj+RWFEsnOQDTL+9nQsJ6JMXKOUvueSS9d8oxR944AEA029mGgi1r2gUVOOTM6j0wmQJ3eYIJ4RqJA4ajgz0pWHLtReli2oszgCpEYNxm2pQTnPguNV12aPwuv31vBw5h31wrHGuVIo6AybhpKhSftkm0nXVrcn7M46DbXi8uE21EeeOVINjj4Q1Fin5E4kVxcIl/6lTp6be6vomc4E5BN1we/asRRJHyc01vmZMAWbftvyM8fxqV1BXEzBIGJXg7u3bI+c4MgmhVFdHB+U2Z3NQaezWjaqd8DixrQa0xL5aGkxsw+un26J0VQpyxBiyUotm7CS/0mOd3cm5+rQvdQUDg5ZJN6S7v3j+HAftBMBwjdlGs1IBs1qJm49Tp07NpQmk5E8kVhQLlfy11pk3mP7v1p8aCuskp77Z41uXWoBa0F2YrK5N43HjMQGvnfQs8XqOvXUnx6gBJXF/Z5FXqeWCf7g/reSO5KNEE2cd70nTViBN75x7UkvHGsfYo02rVCeiZqG2E5efr2WbAgaJz7aU5FH7pE2KxJ/oaWJKL56bu69a5B61gcxD/knJn0isKPLhTyRWFAvn9m/ZsmVKrVLDmIuGU5XTJXPkd6ce0tWnxjxHKmmRS+KYegSeVjRbHGPLuAjMGic1SSYwaxTtGeMcYaSngmsbNw9E5Nkr2F8rDsD9FvvgdeS58Vq5xJmExgHENrqM6WWMimNU451mEwamVfg4jp7r88SJE+vbGPfPZQL7dK5XvQ7uHhyLlPyJxIpi4a6+Wqt96/boimogc29vuvoojWL+NJXULgpNDX2O3KJuIydBe0QNQqWHe3v3MvEocSbSW1sGH0fS0eNF7UK1lBi/rtqEk5g8R3WP9rQlhx6FWCnDjtqsBkvnImu5Lt0YXRYpvdauiAfnT2sEAEMUoLoRo2bTovG2slyNQUr+RGJFsZQcflHyqMR1krKVhSUGR/BNqplQgdl1n6N4jnn76xq9ZxdwLjKV4i6wh1Jc6cFOKvdy1qlrKGoHev49yesCYZxWBExLM3VDOtuDag6Ouqs0YzcPehyX/0+vZxyH5umL9w5tDfzsuW4J2m1iXkm9Z2L/qh25AifaV8stmiSfRCKxIfLhTyRWFEsx+Dl+txpUnPtKVZ7oqiNLinHTcUmg8fvKxIptNCWTK+ap7klXU20Ml92p1PxOJphGg8X91R0GtJdPLi029+exYx/cxs9eVKAbYy/ykOilMG+pvj2mnRpigTZfP56DXgfnQlbGpYuk5DZG/MU2GvHHRLHAbPyGY4DqktEt1eaN7U/Jn0isKBbO7VdXXyvtsuNnqyvlwgsvXG/DN6mrcMo3osbj96LiKGHi25tSsFfxVd1NLgljTztQt5szhqkx0VWu1T5756jzo8fUbWqwdC5PlUy9smM995QeO86DI3speM3UHeiSlfZSkLfaRui8OA3CaTm8DuqOdMbRMRGlY5GSP5FYUSwlk4+Thrqtl/JaJTgwkHpIkewVPKA06EXl8dg9Ao1bdzn6ZwsuP0Aru08vjj321Sri6dafOo9j8g26bb2iG6q1xfls5ceLY+tB17huHBpV6CR2b62srlPagOL4NJU64eLxXeFX1YB47ceMNSV/IpGYGwuX/KdPn556a2omH6L3RnMUTV2ju/US0ZOYjoSh/aqEdKXFXOGFViCNs4HoGtUFiThKtBYScYQc9ag4baVV8DPur22ctqb59SKUZu2yEPc0gB6BSPsYU0yFUj0G6qjm466hxvM78hXp5/yM1n69n5SMpseKfYwpw9ZCSv5EYkWRD38isaJYCrd/TBJGx/NWUkhU3VRNHhOH31OldV/97sYT++WnS9FMuOWLFulwxig9jssH0CP5qAG0l8DTEZFUrXWuPj2mS6elsRKxD11KODVdl2huqdjLR6Bwqdt0HC4xqxKSlCAVv5N8duDAgfVtV199NYDBGEhjtStGMo97dSOk5E8kVhRLyeTjJL8SYHquPvc/jTTM2tMzcDmDTCu2Pr59dWxjUm9Hyd8i+bgqv63Ekw49em/PcKnn4zIb8fyjNFT3YS+vAcfvshb1ovC0L2fY4n4a6x/PWY1wvUInDrqfuz+1jcu6o67XOEZKev7G2P+e5Cd6xW02Qkr+RGJFsaHkL6XsAfA/AbwBwGkAt9RaP11K2QHgrwHsBfAIgA/WWp/sHYtBPc41FvoD0A9qcBJX8wK4NV7v7d1KA+0y+fSg7itHmVUSR2897rLC9DIatYKHItzattXWZT1qrX97bdy6XM/DkVq0YKqj3mpfcT7VRdbL9tNzE7eIOLEN59PNvR6HGiow2GnUPuGo0ISjO8+LMZL/eQB/UGt9C4B3A/idUspVAG4GcFut9XIAt03+TyQSLxNsKPlrrYcBHJ58f6aUsg/AxQDeD+D6SbPPAvgqgE/0jlVK2ZBs4dbjLWt/hIbpxvBSzYjbK8ekgTWuLw3acdmIXWFKzRrsCkK0JGWUArqOdQScniakxJ8erdcFD+l5aCkqYJZC7chGDG91ng3NvuTCp1tSubdWdqShVsh43NYLPGtldooUc35n/+7+VLKVu3fG5Dsci7nW/KWUvQCuBXAHgJ2TFwNfEBe290wkEpsNox/+UsprAPwNgN+rtT49x343lVLuKqXcFRNsJBKJ5WKUq6+UchbWHvzP1Vq/OPn5iVLKrlrr4VLKLgBH3b611lsA3AIAu3fvrqdOnbIZdHpuilat9rgPVcg3vvGNAICjR4fhaBy+M6i0YrGj6taq5xfHQRVeXX7xN3Vf9fj/PZKNVvJ17Z2hTdVkbnP1AInYP+dal3BxHEoAcnERqor3ogKd+t0idsU25N3r/eWWQc7dyznhcXiOrg3Hw8jSKOzYh0se28sepfv38KK7+sraEf8cwL5a65+ETbcCuHHy/UYAX5qr50QisVSMkfzvBfDvAHy3lHLv5Lf/AuBTAD5fSvkogMcAfGCjA5VSsHXrVmtgUpdOj7bYy07DXH47duxY3/bEE08AmH2za9VdNw7n6lPtwOWu45gjcUYJL07r0br2ztWnx4vzSYpoz7DKsakRKo5Vz9mRWtTg6cpL9Qp7tKItgdloQJeBR+fTGfM0r1+vXJfT5Ngf7xXOryNvKbErSnA17jLPXxx3K3IvjqOX3nveKr1jrP3/F0DriDeM7imRSGwqLCWe35F83No27hPbEG4d1MtKo2/fKKn0rduT/NymOfDcefQy++qY4/66NnQUXicN1aWmxTNiGx7HrYt1f5dlp5dtVkkoroCqZktyeQrZnmN0BBzN2xhdfa3CJC7voXNHquuYGkDUknSOXFZl7h/LyBEayNOjuG8UxDSPuy/pvYnEimIpOfycpOzRclvZSlwOd1dWSQtbOmu1agou2ETX47pmjtvcepzj5RvekVvYhsd2gT1KfHG0WJXucYwqVV2eP6KXU59zxONFz4jm7HPnwwKVTz/99Mw2DYBx+Q7ZB63qbBvX2q3su/HeUdp13MbvHI+T/GzDWgs6r3Hc7j5/sVzgLynJJ5FI/OIgH/5EYkWxlKIdznjVcmVE9Ax+6sKJbjxdHoxxJxJRFeV+VO+oXjqykBaLAAbVV41QcYmhhjJWeo2GInUpxXTQmiOA25w7UV1+8bpoBdx4PfQ3x4nnOXGuGKNOVR8YklnyWsVrQFW4FbMf4QyfOg6NFXBLHK3IG4/NeTzvvPNmzlXdkU899dRMG15Pl5Zbowp1eRm39VT7LNeVSCRGYSkGv15ePCdFCDViuUIUjvKqcdouDxuhrrpoPKL0UkOfo/D2XGQ8pssqo8Y09klpEs+NRJFoaFN6MTWQnmvKZcBRKrQrOKqEJhfVR1DiU9oDs9oatYR43s5FqFCpSnpt7EM1hzhWjs1JXM41rxE1sXjNOf+cY5eunJmmuH9MD97K8OTO0aVS1+OMRUr+RGJFsfAcfmeddZZ9i7u3rts/fvba9Ao6uvJKSpyhdGbJ73hMHk8LZMQ+XA49Shgex0kIXfM7yav9u6xHShl1cfQ6j/F/pcW6wqdKMXVZb9XVF8+V7X/0ox8B8AEtSvByLtxevkJdj3P/qJnw+vEcOZ7YnteO+51//vkzfVHy03XpcvhxHty2HrlH7VROA0hXXyKRGIV8+BOJFcVSuP29OHrHyde2jkuuvzm3lRbEcH2payqqovyuPHNXdMMtX1QFdmq/qtLKMIu/cfkS9+dvmiosjocGMWUPOjbhPHH4UZXl/GkbN+e9ghy6jItLC42xcGzGVgRj7JPzESPtCE1drrH7wOw1pzvw+PHj6224fNy5c+dU2ziWVkq7iDNJ2KlIyZ9IrCiWUq7LxUsryWdMIQVH8nEx8iopXVQfJT5dapTu0TBEaaYuwx4XvFdQwxWy4Db277QD5bCfOHFifRuNeXQpUarFcahri+glTXXQDEkuE5CeczxXdd1GYpbu73j3mlVHjwcMc6wko3iu/E5ikYujpxHPSX6C/TItd9Qk2K/LmTAPWinmgSzXlUgkRmIpkr+3lnHuDl33uXWo1pqPUkTflhojDgzuHa7N+BmJJ0r11Eiv2BelUiRzKKmFkjNSXil9ekUold7r4uiVZOPy/PVi3HVtGdsqddnZa5Q4pK4uYCD88Ld4jjqPzrWlEZSOtq2EGc6ZoyLz3onXQyX9oUOHAAB79uyZ6YNjVHtL7K9nE1KXn8tvoC5xl4twLFLyJxIrioUH9pw8edKu+XWtPyZnXISuXx3xRtdbUfJTCvGTb9QoMXkcF9NNKDkmxmqrXYJj5no07qf/O5qwy9KjpJxeQQyVnBFKU47Xo0VkciQhzifP0cWuK8kmHlut87FNJONEOC2J18zlgCBUe4znxntFtRVgWNvrPG7fvn39O7Ul2mJ6thSH3vUkep4yh5T8icSKIh/+RGJFsXBu/5YtW6wK24OqyU8++eTU78CgeqkxDZg12jijmLp5aKhzhkfdx8XjqxEKmHWJuaSUNCYqF90l4OzVvOenpjCLx+ol4GxFB8Z+dT6icZTqMV2nLmdALx2ZLiVcqjCCSzNHaOIcc4nAPlxKdRdVpwY6XvO4fKF6r0ukaAjmODj+uPzQ2oeuWrAm93RLlHmRkj+RWFEs3NVXa7WumJ4U4lvz3nvXaobcfffdM8d929veBgC45pprAExLKk2YSfRIKRoNFttoXH98i/Pt7eLX+Z3HdsUaqHFwXkgYiS5DSrpW4Ya4P7dF91WrNJkzCvbi+dk/jx0lPyVjK6JSzxuYlsacYxKYeLyY14BQtyrptcBQxIWaIccYJbfSnV32Jl4zuoAPHz683obHVsNfvHc0W0+8Z3h/79u3b6qPCy8cat9efvnlAGajEvX+nCemPyV/IrGiWArJJ0KliKPn8k0YaazAtFT92te+BmCwB9ClAgxrL6WDxjUZpU6UTLEtMEg4XdPFtRlLgxHxeNQUdu3aBQB4+9vfDmBaqh45cgTAUGj04osvBjAt+XneLkilVcIqur8oadTFFs+Vbdw6mu2oOTmyEvtj7kHVROK4ub8LtuE4KFU5Z7F/zrlqVsBwz3D+GIcfrz2pu46spK5nzlUM2tm9e/fUGIl47anBsV+ShYDh3n3kkUcADHPF+wQY7k9qAM71yRyZY5GSP5FYUSyc5NMqzqgaQXyjPf744wCA73//+wCGN3t8s1KKfe973wMwvW7jm1mlj8scwzdzL1ssx8p1KSVH7PfNb37zzBgfeughAIM0uv322wFMaylqnefYL7roovU2SkGOY9Q1Ide4cT3uCpIAvniI2ifcb1w/u/nUdbCj1VI7iJKO5/Hd734XwCDVXdZcSmzO9WWXXbbehhJz//79U2ON62n27/IFqkeC1z56HZSS7ejnHBvvT157YLgfNCAoXjO257GpEUZkDr9EIjEK+fAnEiuKpZB8olql9d5cymuqRceOHQMwqJt04wCD6kz1MhoDNfqOhqno7lG3k4t8axkrY0ENjom/RZWex3zggQcADGp7XDbQIHXVVVcBAC655JKpfYFBPSSc+4vnHOMGFBpp58hCSkgCZo2Kmoo8gnNM95tLqe5cdGxHA+53vvMdAMCDDz643kbjKDj3kVPP7+zrwIEDAKbnjP2zT0eIIvmK91BcfvD6qSE1Gqi//vWvAwDuuOOOmf15r/DYPF4cI4/F5QrnysVljEVK/kRiRbH0oh2tqL4oaVy9c2DatcQ3M11EUTpSIlAaunr0Gmvfg+bJi8fR6rJRGtIQxfEcPHgQwLTWQcMe48V5jtG1RK2G8xIJI5wHzTYU506NmZyP6P7S8UcJo64+557lPHBsmmY7HpuGrahlcR6oAXHOSYSJ80Dt6sorrwQwfe0pPTlmtokakWo+Lk+ERjnG66rUXd6D0bBKerFqELG9EqocBZjn7DIDZTx/IpEYhYW7+lrx6vp/dKXwu665XaHNXnBGb33UKooQ++TbWu0Scax823PdFt/GlDY8jnPXcPyPPvro1P+Oruwy16iLj3aFuD/ngRoAJYxz1Tnqr+YVdJJfz8dlnlGKaozPp2Tjmn3Hjh0AgOuuu269jeYl4PFoG4q/aUBPLOnl6MlEK0beuSy1IEfsg+t6aiIuG7POv5tP2lBcEZR5kZI/kVhRjH74SylbSyn3lFK+PPl/RynlH0spD00+t290jEQisXkwj9r/MQD7AJDAfDOA22qtnyql3Dz5/xMbHaTFPx5jaFPudTSIaPSVY2lRTdTY6HhMjeaLrqlWvHVcYvDYyn4DZmMDXMoyZYc5BmQvDZgan1z8u+YBcGq7JuB0+QR0rmMbjkPVfZdGy1UJVhXepRLnvGuKsHg9VG13tQv5XdmdEZpOLS5fVAXXeQUGdyKXAk5dV4O2q8Onxtp5WX0RoyR/KWU3gH8L4DPh5/cD+Ozk+2cB/NYLHkUikVg4xkr+PwXwhwAiu2RnrfUwANRaD5dSLrR7ClpGO42n72XQUcNb3J9SIL7xVdI7ScX9VANxkp9wWXLYxkWq8bu6j+JY+WZ3WX4IlaZRulO7UP5+Txo6rUv372ULcm20uq0aO4GBw+5Sqasxzxk3dawuzTnbU+NwCTC1jSv6odch9qGFScakpndSnf3zM45RswS5+2PepKAbti6l/AaAo7XWb8915GH/m0opd5VS7nKZWxOJxHIwRvK/F8BvllLeB+CVAM4tpfwlgCdKKbsmUn8XgKNu51rrLQBuAYDdu3dXYPoNpfXXXUFGjVt36aRVUjmJq9lpIjSPmssopGs6Sm5XNsyNUfPiubJj6pJyRUx0rd/LwKPjid+1rSvXpTH38ZjaJvah1FtXdkzLW7nil0qoctGFrRyN7jguP55qDD0J6shK7jrGvoBBUmvJt/idmpDmcYxt1B7R05A3woaSv9b6yVrr7lrrXgAfAvBPtdaPALgVwI2TZjcC+NJcPScSiaXiTEg+nwLw+VLKRwE8BuADY3d0kpe/qcUTmI2h5lswBtS44pt6bF0nRUnDt60GwrisLponz5XYdtl7NUecoxm3pE6vUEmUyrq/Sry4Xw86j/EcKf3UM+ECYjTrUbRPkFTDMUZSjNpDtNxV/K2nbTk6btw3tnfaiWo5PQqwemHccXiucR702vJedkQ39uHKfs1L+Jnr4a+1fhXAVyffTwC4Ya7eEonEpkEy/BKJFcVSEng6NVdJDM4Vo+pR/F8NQi7NkpJ0ohtP02A7Moe6aXrRgU4t47GUCOTmQ11TMYKRMe4udTjHpipsrwKv5lSI43CqtLqyHBFJjZtEXBow/oGRalHt57UZY4xT1TyOT5OTKpkrjlvV/3gsLlt4PnHO1TWn0ZLxHDmeGHmokZPcPy5BuR/nqFdzcCxS8icSK4qFS341NqmrTw0awKwxj2/oKFVcwQWCb12+vUm1jG9Pvm1VKveMKC4ppe7naLH8TUkuwKzEplEsZnXhNkcVpbTROH6nwfToqK4giG5TjSyeK/vleHhecc6ZtYgSLkb18fpTQnIcLjORM7wSY4yb6kZzx9FzdAU5VHJHbU0TqcZx8dy0krC7l7nNaTmnT5/O1N2JRGJjLCWTT0+aKNURGDK1UAo4MoPG38c+9M2ubpP4XUkpzqWjGkh0tTk7AKHrRZe1iGOj1KA0dAUttHhkRM9V1yod5eZD9wFmr5WSbIDZ4ieuMIhmXYp5F1mIg5Ke2hpdsvEc1fUY1+xqZ3FrZc4/zyfaHtSNR40kaqaaL4L9u+viJLbmUnSpvzWIzGmdmbo7kUiMwsIz+bQKdSqBJgaAUDJQA2jl9AOGt7grPaXrRbfmJxxJR6mhLpBES4I5CzrHT0nnwkMpKTWzbDyO02DUI+JooTwP9Uj0QpyjNG2V+HZEILWEu/kgImmL56Z02DhXql24zErsg9JcbTrArBfIzafSjB25Rq9HtNNohmM314TTjDnnPA+n4c6LlPyJxIoiH/5EYkWxlKIdERq37WLtmeqaSwFn2IhuFdevg3MDacy+U2U1xbJLa63GMGC2EIgmfozguSrZJo6b+2ul1jgOzks8DxrNetx8tncx7q2Kyq6OHufGxRgQLrknr4OmCXc1B7XasHNr6pIvXjMdv1uWqpHUpeXWtO0u5sJFgnLcui0uB3jN6B51Br8tW7bMZfRLyZ9IrCiWnro7bgN8vLQW5KALJb4Z1YXjaKCaKtrFn8e3NeDJGNzPuciInjTU3AOR6qnGN0dlCCf8AAAgAElEQVQV1Tl0LqFeJp8xkrtFqY59qFYQ564lMR2ca0s1GH7GPtS964y0rexNzpCrWoobPzWpeM15f/Je4WecT+3X3XvaNmoHvEdoAHYZiZLkk0gkRmHhJJ/Tp0/bQJaehKCEoauPGV1dSS+t1c4+gf5aufXWdrHhvbFqcEeUEPrWd6W0VPNwUoA2Am0bx61up15egl62H/bhXH1Ezy6hrtt4zdTO48ao+RGiC1htL73gH/3NufMcRVzvHXeuasuhXSIWYFX3arRBqC1JS4MBQ2FOSn6Hed1+KfkTiRVFPvyJxIpi03D7lQHmDH5U+zWtFzAYYqhWRpcda5qrutrjRTuVWtl7vYg/F+mmhRZURQdmjYqaHyC2dy4yzoMWyYhqrkam8bx6LkvnPlOjU1TptXiI7hPnwyXe1OWTi6PXJYmLguM4nBFO4QyXOlb2FWMMNHKR/7NOIjCw/WisjssojcTktWbUJzDUdVRW45gUcC2k5E8kVhQLl/zK7VejjzPa8O3GNyHfkFFi0riihj9gkIbK73aSppWlJkJ55u58nDRTyd/bX+MI4rho9HESRjPPaBRY/K6uMqcdaNai3jy4ZKUaxxClmWouPaOk9hnba86C2IZSVF3IvfTtjpilmabifFALYBsa/JhxCRgkPrc51y01W/4fXcDxe0ScnzG5C6b2nat1IpH4hcFScvhF6Ju9F63E9Y5L083f+KaP+1MyUvLH9RrBt7aW9nJx1xrr7/INOolNKHHFaSDqFnTZfqjtxDe+Rp9xfxdFxnlgm+hGUikY++ActWwHcbyUeFpMM/anUXWxD9UMlRATx+80EI38dNqB5mBwBUfVVRfnU/s4fvw4AODEiRPrv3HcPUo355H36Rve8Ib1bVFjiuehmXzmQUr+RGJFsRRrvyuEqG/4nuRXiQHMZvuJb3auN7Vsl1sv6brcraN6WYR1TRn3b8XaO69Dq6gnMCuNXZkr1bBc+W1qS/SGxHh6gvYS179a1908qH0iSv6e7UPX870y4mozcOQrDdiKa+5eZiadY+e1UCIW77cYbMbx79ixA8C014Ljp3TnvRyvh3qIWjaqeZCSP5FYUeTDn0isKJai9ve416r2RtBARXUoqvZUvbgtqmXkRVMdo7HFFe3o1a5XDrbLPaDn4QxUqu67OPZerXY1Hrk00po2Kqq9qt5SPY2GUParxJUITbHlXIU8TkyKSfB6OLJUq2iIcycSPdKVjt9FOY6JPNR7II6V/dPNumvXrpmxchnk8kRwPngNo9qvLken7jNN3lik5E8kVhQLl/xbtmyZiUEGZo03zgjGNyIlOT+BWYpqlIaaNrpV69zBvUk1mqyXFtvRe3UcLg20i7QjNArOua2YXYZaTiSJbN++HcAg6SlxYpYcSi9nuNToyF7iTE25HY1pSmSKmocjV7WgktsZlJWu7PIsuG2t3Akuqk9pwTTcxTYuM9Lhw4cBDMbQnTt3AvARjHo/zluZNyIlfyKxoli6q08DWBxRQ6UAJf7u3bvXf6PU0rTawGyxDkch1jerc8MpLZZw0qwnsfV/t9ZUbcdlsOG2KJUpjVXSxDUvyUGULEpdjd/dOXMb5/zgwYNTxwUGiU83opJUgFlyUbQLcPxqu3Bao2pbruyYzrHT6FSjicdWG0bU6JQgRhuKy63INlHb49qe+flI7nEajI71TJCSP5FYUSwlk0+ESloX5EH0KK9827pCFmqx7mWQ1f+dlV3JKU4KuAyseu4u+IfoFc3gNmc51qAQFwTFtSVLYvE4cY3JuVLJCwwS/tChQ1PnzLUqMEhIzZ4bJR7X+BoqDbQt7y7DsFKY3ZpdNYA4Z+qtcKHBeu1cwRcG8pBO7rRXt0anV+DSSy8FMNgK3BhVy8miHYlEYm7kw59IrCiW4uqLUC6942e3lgourbYaioDByNIzxrUMjj2XYy9iT/dx25wRS5cNjiykxrhIBqFKTjW7Z3xSI2lcGqihK84nXYMaCxBVejVm8njR5Uj11i3x+BvVa40kjNBiHb0MTc5Q5pYCOo5eKnNeG2brcXNOaGJUYDYtN8/RxYX0MhJl6u5EIjEKoyR/KeX1AD4D4GoAFcB/APAggL8GsBfAIwA+WGt9snGIdWjqbif9Jn26cQAYDFORRMF03o5co1Kcb10ncRVOS1B3ojP49cbfq27bKlPljFjOOKoZfDSXnmujrlCgn2UnGvZiH84tyr5UusVjso+o3Shhx1U9bhm9onapVXl5zj2Dn7t2mksxajlKtyacW5LjiPtr5V22cfn5WunKXwjGSv5PA/j7WuubAfwSgH0AbgZwW631cgC3Tf5PJBIvE2wo+Usp5wL4VwD+PQDUWp8D8Fwp5f0Arp80+yyArwL4xJhOXRYUjed32oHmp3vTm9603obr0CNHjszsr+tGrnXjmopx1q3xxPZqt3AEmDFFHwkXmNNyUcVjamBNhK4tHbGK86j59mJ7SigXvMRzdUU4+ZtKrCgddY6iNFQJ566HZjp2dGtee3W1RY2uFSsf91OST7x3uNYnpZp9xMAxHWN0q5K01juPeTPzjsGYI74RwDEAf1FKuaeU8plSyjkAdtZaDwPA5PNCt3Mp5aZSyl2llLtiIodEIrFcjHn4twF4B4A/q7VeC+AnmEPFr7XeUmu9rtZ6nQvrTCQSy8EYg9/jAB6vtd4x+f8LWHv4nyil7Kq1Hi6l7AJw9IUMoMXk6lVa1bRHAHDttdcCAPbv3w9gYK8Bsy4Y8s0vuOCCmTY0erUMkb0xx21u+aIqvKvVpyq8i6fnd2VFuv3ZpxZ7AGbzGjDaD5iNXov9cz9lHzo2oxpFnavMLTsIXSrF81N1XY16cRxqRHPLKMfK1GO7+4L3FcfP+YlLHM4xDZ9XXHHF+rYY96/HJty1VrzoRTtqrUcAHCilXDn56QYA9wO4FcCNk99uBPCluXpOJBJLxViSz38G8LlSytkA9gP4bay9OD5fSvkogMcAfOCFDKDFWY5oxXZHKcAlBTWAKKloa+CxSU6Jrq077lhTbGJkGuCLh+i4XFYWl5ZbXTm9AiXqBoxSxEmWFlz8uboa+RkNbpTGro+W28kZqHqZZygpncFwDNTwSMQx6/gdeYvjd0SiljEw3l/UJC+//HIAg+EvtuH9SW2TBmZg1v3YOq/Y/4uBUQ9/rfVeANeZTTe8aCNJJBILxVLKdTnKq667XAkrlR49slCMumL0mPYRpaESLVxegNb60+UC7FFFFS4dNaUG15PRU6Jr7ohWuTC3HuS4neTV3HlRCrXyLbrCID3tQI8Xoam2exGUOq8ul6AW0+xRb+N5aF6EnuS/7LLLpvZxBV/c/aH2hNb9rucW9+Exk96bSCQ2xEIlfykF27Zts5KuR/JprRt7GoTrQ63tUbrQc9ArztCyYEeo9OjZDFTKx99U4kf7hOYkjBqMWtV7tFhKfKX5AgMRymVWosWaY+T+UTuhhFWyj1uPOws6t2lm4egu5vXTa+5otZoNuJenwXkUevRcnqMex0nhXhBRK6ir1V772LJly1x2k5T8icSKIh/+RGJFsVC1v9a6ofrijGKqPjlVdAzxRlXA2BeTJ1KlZYXVqKbS6ENe9phUSo4n3lqGuP2oZjoDlTNcqktKi4Boe2CIi3CuKbe0aNUBjIZPdUc6V65b9ugxta/YRy/lG6HLMKdS837QZUxsp0vPmMvAFYqJ44v7uaIfY+rvtWI9WrkuxiAlfyKxolhKAk9X6miMkWQMnMRQyq2T1JT4/GQSRh07MGvYieiVkNK3NaPy4jlrmSzNaBPhyCBK5+W2uL+m43YUXiVfxTnjuDUvgiP5qDR0Up6I9F4tbkGpHPenEVBj/l3yVs2u4wqdOC1Hf+P9EanhWhm5J32dW1Mlv5PqLbeqkpVS8icSiQ2x9KIdupZ16+CW5HaS17nYiF62IL696fJTaRLHSmnCtb9L4+zOoxUs5FxTKuldUVJNjx3HyH41nh2YJTTxeHE9ra4557LkcbgtuiPVpeWKVVDiOvdZyzUX6dcct7r8HCVZMyP17p2e9qnpxuO2Fm069tvTTuYpGPpi0HxT8icSK4qFW/s3KiPsMuy2KLMuS6trq1qAW5tRejCslf0zSAOYJdC4XIC6NnOFOlUKuCw5Skd18+HCZJWAxM9YBFPnwwXxODotodl9nBTSvHaqvQGzBCBn19Aw36gpqjahIbXArM3Chf3GudHz4dg4Vs2x6Pbr0Z17QTs9qa7tx9gXNkJK/kRiRZEPfyKxolg4t7+UYo1xreSY3M/93zPaRCi5xx2X+9GVQ1UwqpBEz2VItdAZHtXARUQVVEkgzmVGFdSp3Rq5SJDIE0GDpaq0cRzO/aWGRke2UTXfFZvoRUDq+bi51kxEbsnIMWnNQbdkVCNjPFeN6HTZgnQ+XA1FoldYpLWPQ6+PjZCSP5FYUSzc1VdKsW8opZw6+qO+oV1a7V5GIGd0UtDgx8IUkeyjb3b3hnf5CAhKKNUm4rmqa85F5blcAwQlNg2VlPiuoAbnnFpOLKjBNozUi8ZAPUdNBR7PtZWuHJg1vjlNTOc6ak0cB8dIzaVHd3Zzp4ZHl0tCKcxjKivHc25pZK3fYp8R6kYcS3F3SMmfSKwoFu7qe/7557tlspxW0JLmLuOKK7yghTgIl6WH0oP0zYceemi9DWPcNf49uqOUXhv7dLny9Lxa2YLimp2aA9expNsCg6bCsTo3GiU8JTXHH7UcJcq4PIVurgmeP/uidhELdUZNIZ47MLt+d3kaeEwtpOHsCqoZOgKOIzSxvWpH0QXayuzb0z5dkdkeyajn5taxjkVK/kRiRbFwa//WrVvt2k4DUFx21ZbVP353ngCVTG4dqqQWhvjG2gCklrZyrrk+3HiV3BP35/pVM/k8+eRQA5XreW6LlFdK8UsuuQTAIKmitnHRRRdNnQclfjyO1kbo0Yuj5kFQOzlw4MDUOcfiqgyFZR/R5qBEIlfSS204HI+zgHM8nJ+oCanEdNl7lTTlrPU9kk8voEnH6u531Vj0ueH3DOxJJBIbIh/+RGJFsRRuv8uiEtsAXm1vxYhH9Iwsqo45Yxz7okpKFRkYSoBRdexFDroCEMrFdyQXNXBxn7j80JThLoOOurZcJCXbqHEujl+zFkXQCMblR1RBuUzhbxxXnA/NVeDUbZ2HCI2VcBGMLQJOVI95HDXExmPpdXGRmOoC7ZGXnFGy9X9ELzI1E3gmEolRWEomnzEFBZ1UVvQomhFa5NCRSvSNTmkYM7ZwPxrjepLG0YLZTqnMjlRCl5gavmK/HAfru8dz1eKkcTw08KmRNfahcxWhLjVnfOI4WNDCxcFT06CWFV1/SiDSbDlxvOyfRjXnxnPp2hUuh4Nqoj3NUnMP9KRwz909T8ReZvJJJBJzYymuPvd20ljuKEV0jdzL3uvWWe6Nrm11bUdSzcMPP7zeRokzbj09Bq1CEnGbSiwnld15EEp8ifuozcKVCidaueLi2Hp5EzUrc5T8arOI27Q8FsfsxtgrYaU2kF4prJ59pJeZV4+p2ZxiG3fvtoJ+HHmqVUSkdU49pORPJFYU+fAnEiuKpSfwJHq85lbCy6jutVRR159TvbSowt133w0AuP3229fbOJaZQtNGRd6/JsrU9NTAbPoux2nX8URVWJcALtUXx+aMcIRWAnauKTVgusSZOo5o1KPBzy079H5wc9UqxBHHocbJXlQf94uMRTU0uog9/qbJU91SycUftOpVRqg70R0nDX6JRGIUFk7yOXny5NSbWSUu4UgdSs7pRTaNyejjpOmjjz4KAPj6178OADhy5Mj6tj179ky11Ww3btxRwiiv3LnIWumbo1RUyeCq9GriT5emnFKY+7g21AB66axdCSol5zi3qKYXd0bJlhvNjcmlRlcDmdMaW65LbRfHH+fKnb+20TH3YgN6WjDB+ehpWxshJX8isaJY+Jp/y5YtlsDTc5f1cu+12vRSK7s++QZl/D41gFhzXtfYKp3isR11l1K0V95JcwXwM2oULJah6/J43qpdOOIK17aOIKUag0aPuW1RA+F3JRC5cfRyOPDcHGlKx91z8+o94yQ/0Sse4qSynkevjJrrs1VoJtqLWhGkqkXnmj+RSGyIUZK/lPL7AP4jgArguwB+G8CrAfw1gL0AHgHwwVrrk41D8Dg466yzbOknRS8vniN6OKs0oW9WZ009ePAgAODee+8FABw/fhzA9JuV0odWag1aiWNzZZlaZZ2iVG9JAVcAglZqUnjjmFRyuxj3XvZc9Wg4SaVjdUFMuo8jb7nsv1p6yxFwWkVD4v+6n5bjjm3ceThbxUZwtofWOOI2N35FL3/jvNhQ8pdSLgbwuwCuq7VeDWArgA8BuBnAbbXWywHcNvk/kUi8TDBW7d8G4FWllG1Yk/iHALwfwGcn2z8L4Lde/OElEomXChuq/bXWg6WUPwbwGIBnAXyl1vqVUsrOWuvhSZvDpZQLRxxrxjXRSscd1cOWCusSLfbSeDHSjemqDh06tN7mgQceADCknSLZJqZfarmEogqmxiNn3KShrafCKaEozptGr0UVXevP99xwbOPcgdqHSyml16NnbHIusl7MflTLgdlYBWAwiPXcu7p8UBJVbOPIX0pO0iIgrt/edXXb1NXnUn718hEQL3rRjlLKdqxJ+csAXATgnFLKR8Z2UEq5qZRyVynlLpfrLZFILAdjDH6/CuAHtdZjAFBK+SKA9wB4opSyayL1dwE46nautd4C4BYAuOiii6pSENUQ5KLYWkbBeBxG3B07dgwAcPToMBxG6DGOnRl5uA8wS+10BB6NcecY43E4JlJnHVmp58ZrpfWO42D/lBDRJaRUXUeLpftSS1j13KNRqmgCUjXcAbO0XOdy5LlS8roU6Bqr73IwtIg8Ea0yaHF/LYMGDPfO/fffDwB4y1veMnVe8dg9DUi1RkfOUU3SaY094+BLEdX3GIB3l1JeXdZm6QYA+wDcCuDGSZsbAXxprp4TicRSMWbNf0cp5QsA7gbwPIB7sCbJXwPg86WUj2LtBfGBFzIAjaN3ri6+3bgO5Ns4Um9JyqGLLi4xuD8lNKUIyTLA4K7TNlHSqLvGBaRo/Lxb42qsfnQV6jrYuQy5/mS/LkhFNYC4hlb7ipNCvfTiqolpWmtgnIusl4q9Ra5xko4ag5P4msWpd13UXhKPvW/fPgCDtnLllVfOjFWLmfQksbu/WzThOCaV/Eowm0f6j/Lz11r/CMAfyc8/x5oWkEgkXoZYeCafs88+264/KakpBWN5KhapOHHiBIBh7R6tomohjX3wWPyMWWp1HCrd4zqUY9M3fJR4zktAKOXVSV61eShdN25TiRGhwTLO9tDKkBS/O0u8BlZpgA4waCfaNo5VPQBxzlRiE1Gysb22daG0GgoboQU+HfWX2YgffPDBmbFyf2Z65lzFNhqiHc9L7V2uTYvU467ZWCS9N5FYUeTDn0isKBaq9p88eRJHjx5dJ9IAs2qyusGAWZXccft7bh7lu2vqa45N+wWmDVaascapxFozPqqgVA/5G41yziimxh9NcBr7da5QPQ/nOtX93Lm6pY26D3nOriAHoa4/t83FXOjSKLpVW4lZndrfq6uobkiXwJNLTWfcvO+++wAMacqvueYaANMFX+j67Rn11J3njJsbxTFkVF8ikdgQC5X8zz77LO677z7rZlGiRjRw8G02piiCK5pBrYKS1tEo1c3jjEdKSFLDHTBIdx7bxaErGcQRaNT4FCW5SkPnauyRWjSfnIvZ78XI89jqBuzl1+tlwFEqshu/ukfdeWjfwKCBaUYhVyjFRUCSCs5PXt9LL710vQ33+8Y3vgFgIATt3bt3vc073vEOAIOL0LmQe1K7VSJOozVT8icSiQ2xlHJdkcLacjfFN5xKiJ4rxMXYO2pp3McdewxBw62d1f0VpRltDL3sv0rgcYEkBLe5PpQ67HLn9c5DXY4uH4BeqzgOpTcrISce02XrUXcq93PZbXQfp620chAAngJN0MZADYLXLvbNtT6zQNE+EG1KJJ/RXf2ud71rfRuveS/2RbXelit0HqTkTyRWFPnwJxIriqUk8IyqinK3e+pMj9FGOH62cvC5vyvu0IuaahkDXaw7VUm3tFAjoIsc1L6ci6rH2lO4+WBb5fHH31zlW11uaFw9MLAp1dAW51zn2qUnp7rNOXOJRPVecXkeXIyEjkOLkACDoY/nyPHHxK5c4midxwge55vf/CaAaZcllwA0SLtzJXrpvV+KqL5EIvELiIUX7Th16pQtdaRS1HGfe+mTXYYWglx+jb+PkkeNX84wpHHrPWKRk6YqzTV1dtxP94/nSkMSP+N8qvHKkYRUExqT5DNKZTWMOeLKmBh7RTTmaayH09Y0XkAjIYFZg2evxJlLMkopHsem/2skJY178ZrxfNj/Pffcs76NRsD3vOc9AAYDotNMFfNK+4iU/InEimLhUX3btm3rFjxwEtfReeM+7jjx7c83sqOqEkp4cQUZ9S3rim7omHt53IjoxtM1cmufOLYYu6/EH7qmuC51x3LzMaZQqOYQdJpDKwcCMLuOjtJUXX08jov8UwJRvGatrEPxmum26HJrSX7mgwQGV9255547dY4utyPPK14zpo2/7bbbAAyEoCuuuGJmf7UnOPLWWKTkTyRWFEsv0a2EHff2Uguts/rzLUuJ6XKs6ZquVybLrQ2VakuJ57QDF8jC7xr0EyVNq0BnHEePJKTtHT1YJaZbKzsiFaFUW+fFUNuFZmOKx+GnI/n0LN9K3nKeDndurTYalAUMEl7nI54Hx6iaR48aHUEtgGv/b33rWzNt3v72twMYtAyOK85Z0nsTicQo5MOfSKwoFq72q1rrVPD4OzCr9rvqtKruRjW5ZVR0cd+qivZU7F6VXar40VBElU3TUcfz4BKA42GS0WggUjeaK0ChhJU4n8rXp+vTnWtP7ee5OZVc55F9Om6/i4BU96Ez1KmazTmP177F2+8l0IzGPF4PNeZFwyONcOyD1zmej7qr4zXXOeL1iOo/4wWuvvpqAEOugDifzzzzzFw1BVPyJxIrioVLfk0vrBK/F6uvcJFqvawyfEu6SqmalceRdFrkGNeXug6BgWzEc3ax/oSW9HJjdS5QNfBp1qAIJbdEI5ZLyqn7taLq4vdeymzNq+D2790HatR0xr1WRKg7Ls+fyTqBtns3Xg91AxI9em6cO0097lK6M1sQIwff+c53AgCuv/769TZ79+5d1zrGICV/IrGiWLjkP3XqlJVYvWIEfJOrpIxvZW5z8esa69+jo6rE6eUL7I3VxdpTwmm5r7h+1LhtbmP6cmAg7Gh6bmDQGHgcJ/F1HtUVGvdz2omeP+c1SkCVhk4K6prfBd0oacmVLSNcSauWq8+lvKaLL675uY3zoaSlOEa1KbnSZC7vYquEl9NOeH3vuOOOmW3XX399M8W3Q0r+RGJFsfDAHl3zjym22JLGcY2qxJVe0I9bR7cywbqAGF0rO6+Bkzi6fqYFOZJKNDzWFbHk259SJFJ3+ZtqEBE6NpcVOYac6rZW+e6oJSh119G2XRCX9keJ6ewrmvXIree1IId6WuK5MiAnUmjVzuOozEpkctKX41aqOTDMVS+LcavAyZ133rne5tChQ+tegTFIyZ9IrCjy4U8kVhRL4fY7V1+rkESEGo0cUcJlY1E3oPL3gdk68twWiS9KGHEEmjGRfq4mnO6vbkkXecdlT1S3VU1mH67Wn6r9LkrSnY8ud1wKclX3VW2Ov7lKwDo3/D8aMHUeHcdfYzZcIlDOn+ZJAGaNq0riiufduz/VGNnLKORSeWuUJecxnsf+/fttotcWUvInEiuKciaZQOburJRjAH4C4PjCOn1xcD5efmMGXp7jzjGfGS6ttV4wpuFCH34AKKXcVWu9bqGdniFejmMGXp7jzjEvDqn2JxIrinz4E4kVxTIe/luW0OeZ4uU4ZuDlOe4c84Kw8DV/IpHYHEi1P5FYUSz04S+l/Hop5cFSysOllJsX2fdYlFL2lFL+TyllXynln0spH5v8vqOU8o+llIcmn9uXPVZFKWVrKeWeUsqXJ/9v6jGXUl5fSvlCKeWByXz/8mYfMwCUUn5/cm98r5Tyv0spr3w5jFuxsIe/lLIVwH8H8G8AXAXgw6WUqxbV/xx4HsAf1FrfAuDdAH5nMs6bAdxWa70cwG2T/zcbPgZgX/h/s4/50wD+vtb6ZgC/hLWxb+oxl1IuBvC7AK6rtV4NYCuAD2GTj9siRtq9lH8AfhnAP4T/Pwngk4vq/wzG/SUAvwbgQQC7Jr/tAvDgsscm49yNtZvuVwB8efLbph0zgHMB/AATu1P4fdOOeTKmiwEcALADa/T4LwP415t93O5vkWo/J414fPLbpkUpZS+AawHcAWBnrfUwAEw+L1zeyCz+FMAfAojxsZt5zG8EcAzAX0yWKp8ppZyDzT1m1FoPAvhjAI8BOAzgqVrrV7DJx+2wyIffRexsWldDKeU1AP4GwO/VWp9e9nh6KKX8BoCjtdZvL3ssc2AbgHcA+LNa67VYo31velV5spZ/P4DLAFwE4JxSykeWO6oXhkU+/I8D2BP+3w3g0AL7H41SyllYe/A/V2v94uTnJ0opuybbdwE4uqzxGbwXwG+WUh4B8FcAfqWU8pfY3GN+HMDjtVbmo/oC1l4Gm3nMAPCrAH5Qaz1Waz0J4IsA3oPNP+4ZLPLhvxPA5aWUy0opZ2PNSHLrAvsfhbIWw/rnAPbVWv8kbLoVwI2T7zdizRawKVBr/WStdXetdS/W5vWfaq0fweYe8xEAB0opV05+ugHA/djEY57gMQDvLqW8enKv3IA1Q+VmH/csFmwseR+A7wP4fwD+67INHo0x/kusLUfuA3Dv5O99AM7DmkHtocnnjmWPtTH+6zEY/Db1mAFcA+CuyVz/LYDtm33Mk3H/NwAPAPgegP8F4G/zfT8AAABBSURBVBUvh3HrXzL8EokVRTL8EokVRT78icSKIh/+RGJFkQ9/IrGiyIc/kVhR5MOfSKwo8uFPJFYU+fAnEiuK/w8b4AlPqTJnoQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# download training and test data from mnist and reshape it\n",
    "\n",
    "train_file = '../input/training/training.csv'\n",
    "test_file = '../input/test/test.csv'\n",
    "train_data = pd.read_csv(train_file)  \n",
    "test_data = pd.read_csv(test_file)\n",
    "train_data.isnull().any().value_counts()\n",
    "columns_nan = train_data.columns[train_data.isna().any()].tolist()\n",
    "train_data[columns_nan] = train_data[columns_nan].fillna(value=train_data[columns_nan].mean())\n",
    "\n",
    "x_train = []\n",
    "for i in range(train_data.shape[0]):\n",
    "    image = train_data['Image'][i].split(' ')\n",
    "    image = ['0' if x == '' else x for x in image]\n",
    "    x_train.append(image)\n",
    "    \n",
    "x_train = np.array(x_train,dtype = 'float')\n",
    "x_train = x_train/255.0\n",
    "x_train = x_train.reshape(-1,96,96,1)\n",
    "plt.imshow(x_train[2].reshape(96,96),cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "y_train = []\n",
    "for i in range(train_data.shape[0]):\n",
    "    label = train_data.iloc[i,0:30]\n",
    "    y_train.append(label)\n",
    "    \n",
    "y_train = np.array(y_train,dtype = 'float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5639, 96, 96, 1) (1410, 96, 96, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
    "print(x_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Convolution Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially we will use a convolution layer to detect low level features of an image.  Input size of image is 96x96, after applying this layer output size will be 6x6x256."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/madhuhegde/.pyenv/versions/miniconda3-latest/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "input_shape = Input(shape=(96,96,1))  # size of input image is 96*96\n",
    " \n",
    "# a convolution layer output shape = 20*20*256\n",
    "#conv1 = Conv2D(256, (9,9), activation = 'relu', padding = 'valid')(input_shape)\n",
    "x = Conv2D(32, (3,3), padding='same', use_bias=False)(input_shape)\n",
    "x = LeakyReLU(alpha = 0.1)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "x = Conv2D(64, (3,3), padding='same', use_bias=False)(x)\n",
    "x = LeakyReLU(alpha = 0.1)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "x = Conv2D(96, (3,3), padding='same', use_bias=False)(x)\n",
    "x = LeakyReLU(alpha = 0.1)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "x = Conv2D(128, (3,3),padding='same', use_bias=False)(x)\n",
    "x = LeakyReLU(alpha = 0.1)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "x = Conv2D(256, (3,3),padding='same',use_bias=False)(x)\n",
    "x = LeakyReLU(alpha = 0.1)(x)\n",
    "conv1 = BatchNormalization()(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primary Capsule Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is then reshaped into 8-dimensional vector. So shape will be 6x6x32 capsules each of which will be 8-dimensional. Then it will pass through a non-linear function(squash) so that length of output vector can be maintained between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convolution layer with stride 2 and 256 filters of size 9*9\n",
    "#conv2 = Conv2D(256, (9,9), strides = 2, padding = 'valid')(conv1)\n",
    " \n",
    "# reshape into 1152 capsules of 8 dimensional vectors\n",
    "reshaped = Reshape((6*6*32,8))(conv1)\n",
    " \n",
    "def squash(inputs):\n",
    "    # take norm of input vectors\n",
    "    squared_norm = K.sum(K.square(inputs), axis = -1, keepdims = True)\n",
    " \n",
    "    # use the formula for non-linear function to return squashed output\n",
    "    return ((squared_norm/(1+squared_norm))/(K.sqrt(squared_norm+K.epsilon())))*inputs\n",
    "\n",
    "# squash the reshaped output to make length of vector b/w 0 and 1\n",
    "squashed_output = Lambda(squash)(reshaped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Digit Capsule Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we need to write a custom layer in keras. It will take 1152x8 as its input and produces output of size 30x16, where 30 capsules each represents an output class with 16 dimensional vector. Then each of these 30 capsules are converted into single value to predict the output class using a lambda layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-a3cb2bbfa729>:21: calling softmax (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n"
     ]
    }
   ],
   "source": [
    "class DigitCapsuleLayer(Layer):\n",
    "    # creating a layer class in keras\n",
    "    def __init__(self, **kwargs):\n",
    "        super(DigitCapsuleLayer, self).__init__(**kwargs)\n",
    "        self.kernel_initializer = initializers.get('glorot_uniform')\n",
    "    \n",
    "    def build(self, input_shape): \n",
    "        # initialize weight matrix for each capsule in lower layer\n",
    "        self.W = self.add_weight(shape = [30, 6*6*32, 16, 8], initializer = self.kernel_initializer, name = 'weights')\n",
    "        self.built = True\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        inputs = K.expand_dims(inputs, 1)\n",
    "        inputs = K.tile(inputs, [1, 30, 1, 1])\n",
    "        # matrix multiplication b/w previous layer output and weight matrix\n",
    "        inputs = K.map_fn(lambda x: K.batch_dot(x, self.W, [2, 3]), elems=inputs)\n",
    "        b = tf.zeros(shape = [K.shape(inputs)[0], 30, 6*6*32])\n",
    "        \n",
    "# routing algorithm with updating coupling coefficient c, using scalar product b/w input capsule and output capsule\n",
    "        for i in range(3):\n",
    "            c = tf.nn.softmax(b, dim=1)\n",
    "            s = K.batch_dot(c, inputs, [2, 2])\n",
    "            v = squash(s)\n",
    "            b = b + K.batch_dot(v, inputs, [2,3])\n",
    "            \n",
    "        return v \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return tuple([None, 30, 16])\n",
    "    \n",
    "    \n",
    "    \n",
    "def output_layer(inputs):\n",
    "    return K.sqrt(K.sum(K.square(inputs), -1) + K.epsilon())\n",
    " \n",
    "digit_caps = DigitCapsuleLayer()(squashed_output)\n",
    "outputs = Lambda(output_layer)(digit_caps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further boost the pose parameters learned by the digit capsule layer, we can add decoder network to reconstruct the input image. In this part, decoder network will be fed with an input of size 30x16 (digit capsule layer output) and will reconstruct back the original image of size 96x96. \n",
    "During training time input to the decoder is the output from digit capsule layer which is masked with original labels. It means that other vectors except the vector corresponding to correct label will be multiplied with zero. So that decoder can only be trained with correct digit capsule. In test time input to decoder will be the same output from digit capsule layer but masked with highest length vector in that layer. Lets see the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask(outputs):\n",
    " \n",
    "    if type(outputs) != list:  # mask at test time\n",
    "        norm_outputs = K.sqrt(K.sum(K.square(outputs), -1) + K.epsilon())\n",
    "        y  = K.one_hot(indices=K.argmax(norm_outputs, 1), num_classes = 30)\n",
    "        y = Reshape((30,1))(y)\n",
    "        return Flatten()(y*outputs)\n",
    " \n",
    "    else:    # mask at train time\n",
    "        y = Reshape((30,1))(outputs[1])\n",
    "        masked_output = y*outputs[0]\n",
    "        return Flatten()(masked_output)\n",
    "    \n",
    "inputs = Input(shape = (30,))\n",
    "masked = Lambda(mask)([digit_caps, inputs])\n",
    "masked_for_test = Lambda(mask)(digit_caps)\n",
    " \n",
    "decoded_inputs = Input(shape = (16*30,))\n",
    "dense1 = Dense(2048, activation = 'relu')(decoded_inputs)\n",
    "dense2 = Dense(8192, activation = 'relu')(dense1)\n",
    "decoded_outputs = Dense(9216, activation = 'sigmoid')(dense2)\n",
    "decoded_outputs = Reshape((96,96,1))(decoded_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define a probabilistic loss function used for classifying digits image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    return K.mean(K.square(y_pred - y_true), axis=-1)\n",
    "    #L = y_true * K.square(K.maximum(0., 0.9 - y_pred)) + 0.5 * (1 - y_true) * K.square(K.maximum(0., y_pred - 0.1))\n",
    " \n",
    "    #return K.mean(K.sum(L, 1))\n",
    "    \n",
    "# Custom RMSE metric\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5639 samples, validate on 1410 samples\n",
      "Epoch 1/10\n",
      "4096/5639 [====================>.........] - ETA: 56s - loss: 2539.3232 - lambda_2_loss: 2539.3232 - model_7_loss: 0.0671 - lambda_2_rmse: 50.3813 - model_7_rmse: 0.2074 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-69362a444c9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m           \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m           \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m           validation_data = ([x_test, y_test],[y_test,x_test]))\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/miniconda3-latest/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.pyenv/versions/miniconda3-latest/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/miniconda3-latest/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/miniconda3-latest/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/miniconda3-latest/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "decoder = Model(decoded_inputs, decoded_outputs)\n",
    "model = Model([input_shape,inputs],[outputs,decoder(masked)])\n",
    "test_model = Model(input_shape,[outputs,decoder(masked_for_test)])\n",
    "#model = Model(input_shape, outputs) \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/madhuhegde/.pyenv/versions/miniconda3-latest/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 5639 samples, validate on 1410 samples\n",
      "Epoch 1/10\n",
      "5639/5639 [==============================] - 171s 30ms/step - loss: 2625.1038 - rmse: 51.2240 - val_loss: 2637.5860 - val_rmse: 51.3534\n",
      "Epoch 2/10\n",
      "5639/5639 [==============================] - 155s 27ms/step - loss: 2624.4949 - rmse: 51.2181 - val_loss: 2636.3479 - val_rmse: 51.3413\n",
      "Epoch 3/10\n",
      "3072/5639 [===============>..............] - ETA: 1:04 - loss: 2622.6920 - rmse: 51.2016"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-daa26c7e9066>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m                     \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                     \u001b[0mvalidation_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m                     \u001b[0;31m#callbacks = callbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                     )\n",
      "\u001b[0;32m~/.pyenv/versions/miniconda3-latest/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.pyenv/versions/miniconda3-latest/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/miniconda3-latest/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/miniconda3-latest/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/miniconda3-latest/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Use Nadam optimizer with variable learning rate\n",
    "optimizer = Nadam(lr=0.00001,\n",
    "                  beta_1=0.9,\n",
    "                  beta_2=0.999,\n",
    "                  epsilon=1e-08,\n",
    "                  schedule_decay=0.004)\n",
    "\n",
    "# Loss: MSE and Metric = RMSE\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "model.compile(optimizer=optimizer, #keras.optimizers.Adam(lr=0.001),\n",
    "              loss=[loss_fn,'mse'],\n",
    "              loss_weights = [1. ,0.0005],\n",
    "              metrics=[rmse])\n",
    "\n",
    "history = model.fit([x_train, y_train],\n",
    "                    [y_train, x_train], \n",
    "                    batch_size = batch_size, \n",
    "                    epochs = epochs, \n",
    "                    validation_data = ([x_test, y_test],[y_test,x_test]))\n",
    "\n",
    "\n",
    "#Callback to save the best model\n",
    "#saveBase_Model = CNN_ModelCheckpoint(model, model_dir+\"base_model.h5\")\n",
    "\n",
    "#define callback functions\n",
    "#callbacks = [#EarlyStopping(monitor='val_rmse', patience=3, verbose=2),\n",
    "            #saveBase_Model]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict from above trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_predicted, image_predicted = model.predict([x_test, y_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
