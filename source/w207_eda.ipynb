{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "from sklearn.impute import KNNImputer\n",
    "import os, gc\n",
    "import cv2\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pixels(data):\n",
    "    \"\"\"\n",
    "    Convert pixels to the right intensity 0-1 and in a square matrix.\n",
    "    \"\"\"\n",
    "    data = np.array([row.split(' ') for row in data['Image']],dtype='float') / 255.0\n",
    "    data = data.reshape(-1,96,96,1)\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_img(sample_img,coord=None):\n",
    "    \"\"\"\n",
    "    Display an image. For debugging, display a coordinate on the image.\n",
    "    input:\n",
    "        - sample_img: numpy array. image to be displayed\n",
    "        - coord: lst. of coordinates in form [[x_coordinate,y_coordinate],[x_coordinate,y_coordinate]]\n",
    "    TODO handle multiple coordinates. Work out bugs with multiple coordinates\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.imshow(sample_img.reshape(96,96),cmap='gray')\n",
    "    if coord is not None:\n",
    "        plt.scatter(coord[0],coord[1],marker = '*',c='r')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_facial_keypoints(data,ind):\n",
    "    \"\"\"\n",
    "    Structure the coordinates for all facial keypoints for a single image.\n",
    "    inputs:\n",
    "        - data: numpy array containing rows as each image sample and columns as facial keypoint coordinates\n",
    "        - ind: index of the image\n",
    "    output:\n",
    "        - numpy array with format [[list of x-coordinates],[list of y-coordinates]]\n",
    "    \"\"\"\n",
    "    data[ind]\n",
    "    it = iter(data[ind])\n",
    "    x_coord = []\n",
    "    y_coord = []\n",
    "\n",
    "    for x in it:\n",
    "        x_coord.append(x)\n",
    "        y_coord.append(next(it))\n",
    "    \n",
    "    return(np.array([x_coord,y_coord]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = '../input/training/training.csv'\n",
    "test_file = '../input/test/test.csv'\n",
    "train_data = pd.read_csv(train_file)  \n",
    "test_data = pd.read_csv(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep the relevant columns for the labels in the training data\n",
    "y_train = train_data[[col for col in train_data.columns if col != 'Image']].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = convert_pixels(train_data)\n",
    "x_test = convert_pixels(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA  \n",
    "**train labels**\n",
    "1. [x] Explore imputation strategies  \n",
    "    - Completed code. Will require running entire model  \n",
    "    - Perhaps take a look at images that have missing values to determine if there are inherent reasons why this is not valid.  \n",
    "    - Create a summary slide of common reasons why things are not labeled.  \n",
    "2. [ ] Explore outlier correction strategies to identify if areas of the face are mislabeled  \n",
    "    - Identified several outlier images/labeling that we may be able to throw out of the training dataset\n",
    "3. [ ] Label as -1,-1 to identify cutoff images  \n",
    "\n",
    "**train image**  \n",
    "1. [ ] Add noise via rotation, flipped image, blurring, etc. Ideally, not have to do any of these for capsule networks  \n",
    "    - contrast\n",
    "    - rotation\n",
    "    - flip horizontal\n",
    "    - blurring\n",
    "2. [x] Explore imputation strategies  \n",
    "    - There are no missing pixels in the training or testing images.  \n",
    "3. [ ] Explore unique images and see if the labels are the same or correct\n",
    "\n",
    "**test image**. \n",
    "1. [x] Review list of images in testing to see if poor quality images can be kept\n",
    "    - Defined a list of images to definitely throw out\n",
    "\n",
    "**overall**   \n",
    "1. [x] Update to RMSE scoring  \n",
    "2. [ ] Add code to troubleshoot the most wrongly predicted images to identify images causing large sources of error  \n",
    "  \n",
    "**Timeline**  \n",
    "Nov 29: data augmentation  \n",
    "Dec 6: finish fine tuning + report (ask Doris about final report)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore NA in labels\n",
    "Based on the below analysis, distributions are fairly normal with some outliers, and mean and median are fairly similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nNumber of columns with any NA value:')\n",
    "train_data.isnull().any().value_counts()\n",
    "columns_nan = train_data.columns[train_data.isna().any()].tolist()\n",
    "\n",
    "print('\\nNumber of images missing coordinates for each feature:')\n",
    "train_data[columns_nan].isna().sum(axis=0)\n",
    "\n",
    "print('\\nTest skew of each distribution:')\n",
    "[(col,['Not normal' if stats.skewtest(train_data[col],nan_policy='omit').pvalue < .05 else 'Normal'][0]) for col in columns_nan]\n",
    "\n",
    "print('\\nCompare mean and median of these distribution:')\n",
    "train_data.describe().loc[['mean','50%']]\n",
    "sns.distplot(train_data[columns_nan[0]])\n",
    "(train_data[columns_nan[0]].mean() - 1.96*train_data[columns_nan[0]].std(), train_data[columns_nan[0]].mean() + 1.96*train_data[columns_nan[0]].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Nearest Neighbors imputation. Takes closest 3 neighbors with the most important neighbor being the closest neighbor (`weights='distance' vs weights='uniform'`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputer = KNNImputer(n_neighbors=3, weights='distance')\n",
    "# y_train = imputer.fit_transform(y_train)\n",
    "\n",
    "# y_train = y_train.fillna(y_train.mean())\n",
    "# np.count_nonzero(~np.isnan(y_train))\n",
    "# y_train = np.nan_to_num(y_train,nan=y_train.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare imputed value with the actual image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_coord(feature:str):\n",
    "    \"\"\"\n",
    "    Compare missing labels for a specific feature with the imputed values.\n",
    "    input:\n",
    "        - feature: str. a facial keypoint we'd like to explore\n",
    "    \"\"\"\n",
    "    print(f'\\n========== Assessing missing feature: {feature} ==========')\n",
    "    x_coord = feature + '_x'\n",
    "    y_coord = feature + '_y'\n",
    "    missing_feature = train_data.loc[(train_data[x_coord].isnull()) | (train_data[y_coord].isnull()),\n",
    "                                 [x_coord,y_coord]]\n",
    "    num_col = [ind for ind,el in \n",
    "               enumerate(train_data.columns.isin([x_coord,y_coord])) if el==True]\n",
    "\n",
    "    for enum_ind,ind in enumerate(missing_feature.index):\n",
    "        if enum_ind > 10: print('Too many images missing coordinates. Only printing 10.'); break\n",
    "        impute_coord = y_train[ind][num_col]\n",
    "        view_img(x_train[ind],impute_coord)\n",
    "        print(f'Imputed {feature} coordinates for image {ind}:{impute_coord}\\n')\n",
    "\n",
    "# missing_coord('right_eye_center')\n",
    "# missing_coord('left_eye_center')\n",
    "# missing_coord('mouth_center_top_lip')\n",
    "# missing_coord('left_eyebrow_outer_end')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore NA in images\n",
    "There are no missing pixels in the training or testing images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = [set(row.split(' ')) for row in train_data['Image']]\n",
    "['' in row for row in img].count(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = [set(row.split(' ')) for row in test_data['Image']]\n",
    "['' in row for row in img].count(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore outliers in labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_cutoff(feature:str):\n",
    "    \"\"\"\n",
    "    DEPRECATED\n",
    "    Compare outlier labels for a specific feature with the imputed values.\n",
    "    input:\n",
    "        - feature: str. a facial keypoint we'd like to explore\n",
    "    \"\"\"\n",
    "    print(f'\\n========== Assessing outlier feature: {feature} ==========')\n",
    "    x_coord = feature + '_x'\n",
    "    y_coord = feature + '_y'\n",
    "    \n",
    "    right_bound_y = train_data[y_coord].mean() + 3.5 * train_data[y_coord].std()\n",
    "    left_bound_y  = train_data[y_coord].mean() - 3.5 * train_data[y_coord].std()    \n",
    "    \n",
    "    print(f'\\ny coordinate confidence interval: {left_bound_y,right_bound_y}')\n",
    "    \n",
    "    outlier_feature = train_data.loc[(train_data[x_coord] <= left_bound_x) | (train_data[x_coord] >= right_bound_x) |\n",
    "                                     (train_data[y_coord] <= left_bound_y) | (train_data[y_coord] >= right_bound_y),\n",
    "                                 [x_coord,y_coord]]\n",
    "    \n",
    "    num_col = [ind for ind,el in \n",
    "               enumerate(train_data.columns.isin([x_coord,y_coord])) if el==True]\n",
    "\n",
    "    for enum_ind,ind in enumerate(outlier_feature.index):\n",
    "        if enum_ind > 10: print('Too many images have outlier coordinates. Only printing 10.'); break\n",
    "        impute_coord = y_train[ind][num_col]\n",
    "        view_img(x_train[ind],impute_coord)\n",
    "        print(f'Imputed {feature} coordinates for image {ind}:{impute_coord}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find eyes that are situated lower than average\n",
    "left_eye_upper_bound  = train_data['left_eye_center_y'].mean()  + 3 * train_data['left_eye_center_y'].std()\n",
    "right_eye_upper_bound = train_data['right_eye_center_y'].mean() + 3 * train_data['right_eye_center_y'].std()\n",
    "mouth_center_top_lip_upper_bound = train_data['mouth_center_top_lip_y'].mean() + 3 * train_data['mouth_center_top_lip_y'].std()\n",
    "\n",
    "cutoff_faces = train_data[(train_data['left_eye_center_y'] >= left_eye_upper_bound)  |\n",
    "                          (train_data['right_eye_center_y'] >= right_eye_upper_bound) |\n",
    "                          (train_data['mouth_center_top_lip_y'] >= mouth_center_top_lip_upper_bound)]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove these indices from the training dataset:\n",
    "- 1877\n",
    "- 1907\n",
    "- 2199\n",
    "- 6493"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ind in cutoff_faces.index:\n",
    "#     print('Image Index:',ind)\n",
    "#     view_img(x_train[ind],get_facial_keypoints(train_data[[col for col in train_data.columns if col != 'Image']].values,ind))\n",
    "#     view_img(x_train[ind],get_facial_keypoints(y_train,ind))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove these indices that are poor labeling and poor image quality\n",
    "mask = np.ones(x_train.shape[0],dtype=bool)\n",
    "mask[[1877,1907,2199,6493]] = False\n",
    "test = x_train[mask,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore test images  \n",
    "- Will need cartoons/drawings of faces\n",
    "- Will need some cutoff images (missing bottom lip, etc.)\n",
    "- Will need different ethnicities\n",
    "- Will need blurred images\n",
    "- Will need some images with dark lighting\n",
    "- Will need some images with sun glasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # look at all images in the test set\n",
    "# for ind in range(x_test.shape[0]):\n",
    "#     view_img(x_test[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore preprocessing for data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flip images horizontally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_img_horiz():\n",
    "    \"\"\"\n",
    "    Flip images horizontally for all training images\n",
    "    \"\"\"\n",
    "    # Flip images\n",
    "    flip_img = np.array([np.fliplr(x_train[[ind]][0]) for ind in range(x_train.shape[0])])\n",
    "    \n",
    "    # Flip x coordinates\n",
    "    flip_train_data = train_data.copy()\n",
    "    x_columns = [col for col in flip_train_data.columns if '_x' in col]\n",
    "    flip_train_data[x_columns] = flip_train_data[x_columns].applymap(lambda x: 96-x)\n",
    "\n",
    "    # Flip left and right values\n",
    "    real_left_val = flip_train_data[[col for col in flip_train_data if 'right_' in col]]\n",
    "    real_right_val = flip_train_data[[col for col in flip_train_data if 'left_' in col]]\n",
    "    flip_train_data[[col for col in flip_train_data if 'right_' in col]] = real_right_val\n",
    "    flip_train_data[[col for col in flip_train_data if 'left_' in col]] = real_left_val\n",
    "    \n",
    "    # Output in correct format\n",
    "    flip_coord = flip_train_data[[col for col in flip_train_data if col != 'Image']].to_numpy()\n",
    "    return(flip_img,flip_coord)\n",
    "\n",
    "flipped_img,flipped_coord = flip_img_horiz()\n",
    "\n",
    "view_img(x_train[[5]],coord=get_facial_keypoints(y_train,5))\n",
    "view_img(flipped_img[[5]],coord=get_facial_keypoints(y_train,5))\n",
    "view_img(flipped_img[[5]],coord=get_facial_keypoints(flipped_coord,5))\n",
    "\n",
    "# add to training array\n",
    "x_train.shape\n",
    "y_train.shape\n",
    "# x_train = np.append(x_train,flipped_img,axis=0)\n",
    "# y_train = np.append(y_train,flipped_coord,axis=0)\n",
    "x_train.shape\n",
    "y_train.shape\n",
    "\n",
    "compare_coord = train_data.iloc[[5]].T.copy().iloc[:-1]\n",
    "compare_coord['flipped'] = flipped_coord[[5]].T\n",
    "\n",
    "compare_coord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add blurring to the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_img(x_train[[5]])\n",
    "view_img(cv2.GaussianBlur(x_train[[5]][0],(5,5),0))\n",
    "view_img(cv2.GaussianBlur(x_train[[5]][0],(5,5),2),coord=get_facial_keypoints(y_train,5))\n",
    "\n",
    "blurr_img = np.array([cv2.GaussianBlur(x_train[[ind]][0],(5,5),2).reshape(96,96,1) for ind in range(x_train.shape[0])])\n",
    "blurr_img.shape\n",
    "x_train.shape\n",
    "# x_train = np.append(x_train,blurr_img)\n",
    "# y_train = np.append(y_train,y_train,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows,cols = (96,96)\n",
    "\n",
    "M = cv2.getRotationMatrix2D((cols/2,rows/2),45,1)\n",
    "dst = cv2.warpAffine(x_train[[5]].reshape(96,96,1),M,(cols,rows))\n",
    "\n",
    "#TODO get random rotation (-15,15) degrees\n",
    "view_img(dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
