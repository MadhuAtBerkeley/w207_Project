{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import os, gc\n",
    "import cv2\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pixels(data):\n",
    "    \"\"\"\n",
    "    Convert pixels to the right intensity 0-1 and in a square matrix.\n",
    "    \"\"\"\n",
    "    data = np.array([row.split(' ') for row in data['Image']],dtype='float') / 255.0\n",
    "    data = data.reshape(-1,96,96,1)\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_img(sample_img,coord=None):\n",
    "    \"\"\"\n",
    "    Display an image. For debugging, display a coordinate on the image.\n",
    "    input:\n",
    "        - sample_img: numpy array. image to be displayed\n",
    "        - coord: lst. of coordinates in form [[x_coordinate,y_coordinate],[x_coordinate,y_coordinate]]\n",
    "    TODO handle multiple coordinates. Work out bugs with multiple coordinates\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.imshow(sample_img.reshape(96,96),cmap='gray')\n",
    "    if coord is not None:\n",
    "        plt.scatter(coord[0],coord[1],marker = '*',c='r')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_facial_keypoints(data,ind):\n",
    "    \"\"\"\n",
    "    Structure the coordinates for all facial keypoints for a single image.\n",
    "    inputs:\n",
    "        - data: numpy array containing rows as each image sample and columns as facial keypoint coordinates\n",
    "        - ind: index of the image\n",
    "    output:\n",
    "        - numpy array with format [[list of x-coordinates],[list of y-coordinates]]\n",
    "    \"\"\"\n",
    "    data[ind]\n",
    "    it = iter(data[ind])\n",
    "    x_coord = []\n",
    "    y_coord = []\n",
    "\n",
    "    for x in it:\n",
    "        x_coord.append(x)\n",
    "        y_coord.append(next(it))\n",
    "    \n",
    "    return(np.array([x_coord,y_coord]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = '../input/training/training.csv'\n",
    "test_file = '../input/test/test.csv'\n",
    "train_data = pd.read_csv(train_file)  \n",
    "test_data = pd.read_csv(test_file)\n",
    "\n",
    "print(f'Train set:{train_data.shape}, Test set:{test_data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep the relevant columns for the labels in the training data\n",
    "y_train = train_data[[col for col in train_data.columns if col != 'Image']].to_numpy()\n",
    "imputer = KNNImputer(n_neighbors=3, weights='distance')\n",
    "y_train = imputer.fit_transform(y_train)\n",
    "x_train = convert_pixels(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Bad Samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_samples = [1747, 1731, 1877, 1881, 1907, 1979, 2090, 2175, 2818, 2562, 2199, 2289, 2321, 2372, 2453, 2505, 2787, \n",
    "               2811,  3150, 3173, 3296, 3447, 4180, 4263, 4482,4490, 4636,5059, 5117,5952,6493,6585,6587,6859, 6906,6906]\n",
    "               \n",
    "\n",
    "def visualize_labels(samples):\n",
    "    \"\"\"\n",
    "    Visualize bad labels.\n",
    "    \"\"\"\n",
    "    num_rows = len(samples)//6\n",
    "    fig, ax = plt.subplots(num_rows,6, figsize=(15, 5*(num_rows//2)) )\n",
    "    ind = 0\n",
    "    for i in range(num_rows):\n",
    "        for j in range(6):\n",
    "            \n",
    "            coord0 = [y_train[samples[ind]][0:27:2], y_train[samples[ind]][1:28:2]]\n",
    "            coord1 = [y_train[samples[ind]][28], y_train[samples[ind]][29]]\n",
    "            ax[i,j].imshow(x_train[samples[ind]].reshape(96,96),cmap='gray')\n",
    "            ax[i,j].scatter(coord0[0],coord0[1],marker = '*',c='r')\n",
    "            ax[i,j].scatter(coord1[0],coord1[1],marker = '*',c='b')\n",
    "            ax[i,j].set_xticks([])\n",
    "            ax[i,j].set_yticks([])\n",
    "            ind = ind+1\n",
    "    fig.tight_layout()        \n",
    "    plt.show() \n",
    "visualize_labels(bad_samples) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA  \n",
    "**train labels**\n",
    "1. [x] Explore imputation strategies  \n",
    "    - Completed code. Will require running entire model  \n",
    "    - Perhaps take a look at images that have missing values to determine if there are inherent reasons why this is not valid.  \n",
    "    - Create a summary slide of common reasons why things are not labeled.  \n",
    "2. [ ] Explore outlier correction strategies to identify if areas of the face are mislabeled  \n",
    "    - Identified several outlier images/labeling that we may be able to throw out of the training dataset\n",
    "3. [ ] Label as -1,-1 to identify cutoff images  \n",
    "\n",
    "**train image**  \n",
    "1. [ ] Add noise via rotation, flipped image, blurring, etc. Ideally, not have to do any of these for capsule networks  \n",
    "2. [x] Explore imputation strategies  \n",
    "    - There are no missing pixels in the training or testing images.  \n",
    "3. [ ] Add preprocessing for auto-contrast adjustment  \n",
    "\n",
    "**test image**. \n",
    "1. [x] Review list of images in testing to see if poor quality images can be kept\n",
    "    - Defined a list of images to definitely throw out\n",
    "\n",
    "**overall**   \n",
    "1. [x] Update to RMSE scoring  \n",
    "2. [ ] Add code to troubleshoot the most wrongly predicted images to identify images causing large sources of error  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore NA in labels\n",
    "Based on the below analysis, distributions are fairly normal with some outliers, and mean and median are fairly similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nNumber of columns with any NA value:')\n",
    "train_data.isnull().any().value_counts()\n",
    "columns_nan = train_data.columns[train_data.isna().any()].tolist()\n",
    "\n",
    "print('\\nNumber of images missing coordinates for each feature:')\n",
    "train_data[columns_nan].isna().sum(axis=0)\n",
    "\n",
    "print('\\nTest skew of each distribution:')\n",
    "[(col,['Not normal' if stats.skewtest(train_data[col],nan_policy='omit').pvalue < .05 else 'Normal'][0]) for col in columns_nan]\n",
    "\n",
    "print('\\nCompare mean and median of these distribution:')\n",
    "train_data.describe().loc[['mean','50%']]\n",
    "sns.distplot(train_data[columns_nan[0]])\n",
    "(train_data[columns_nan[0]].mean() - 1.96*train_data[columns_nan[0]].std(), train_data[columns_nan[0]].mean() + 1.96*train_data[columns_nan[0]].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Nearest Neighbors imputation. Takes closest 3 neighbors with the most important neighbor being the closest neighbor (`weights='distance' vs weights='uniform'`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputer = KNNImputer(n_neighbors=3, weights='distance')\n",
    "# y_train = imputer.fit_transform(y_train)\n",
    "\n",
    "# y_train = y_train.fillna(y_train.mean())\n",
    "# np.count_nonzero(~np.isnan(y_train))\n",
    "# y_train = np.nan_to_num(y_train,nan=y_train.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare imputed value with the actual image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_coord(feature:str):\n",
    "    \"\"\"\n",
    "    Compare missing labels for a specific feature with the imputed values.\n",
    "    input:\n",
    "        - feature: str. a facial keypoint we'd like to explore\n",
    "    \"\"\"\n",
    "    print(f'\\n========== Assessing missing feature: {feature} ==========')\n",
    "    x_coord = feature + '_x'\n",
    "    y_coord = feature + '_y'\n",
    "    missing_feature = train_data.loc[(train_data[x_coord].isnull()) | (train_data[y_coord].isnull()),\n",
    "                                 [x_coord,y_coord]]\n",
    "    num_col = [ind for ind,el in \n",
    "               enumerate(train_data.columns.isin([x_coord,y_coord])) if el==True]\n",
    "\n",
    "    for enum_ind,ind in enumerate(missing_feature.index):\n",
    "        if enum_ind > 10: print('Too many images missing coordinates. Only printing 10.'); break\n",
    "        impute_coord = y_train[ind][num_col]\n",
    "        view_img(x_train[ind],impute_coord)\n",
    "        print(f'Imputed {feature} coordinates for image {ind}:{impute_coord}\\n')\n",
    "\n",
    "# missing_coord('right_eye_center')\n",
    "# missing_coord('left_eye_center')\n",
    "# missing_coord('mouth_center_top_lip')\n",
    "# missing_coord('left_eyebrow_outer_end')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore NA in images\n",
    "There are no missing pixels in the training or testing images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = [set(row.split(' ')) for row in train_data['Image']]\n",
    "['' in row for row in img].count(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = [set(row.split(' ')) for row in test_data['Image']]\n",
    "['' in row for row in img].count(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore outliers in labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_cutoff(feature:str):\n",
    "    \"\"\"\n",
    "    DEPRECATED\n",
    "    Compare outlier labels for a specific feature with the imputed values.\n",
    "    input:\n",
    "        - feature: str. a facial keypoint we'd like to explore\n",
    "    \"\"\"\n",
    "    print(f'\\n========== Assessing outlier feature: {feature} ==========')\n",
    "    x_coord = feature + '_x'\n",
    "    y_coord = feature + '_y'\n",
    "    \n",
    "    right_bound_y = train_data[y_coord].mean() + 3.5 * train_data[y_coord].std()\n",
    "    left_bound_y  = train_data[y_coord].mean() - 3.5 * train_data[y_coord].std()    \n",
    "    \n",
    "    print(f'\\ny coordinate confidence interval: {left_bound_y,right_bound_y}')\n",
    "    \n",
    "    outlier_feature = train_data.loc[(train_data[x_coord] <= left_bound_x) | (train_data[x_coord] >= right_bound_x) |\n",
    "                                     (train_data[y_coord] <= left_bound_y) | (train_data[y_coord] >= right_bound_y),\n",
    "                                 [x_coord,y_coord]]\n",
    "    \n",
    "    num_col = [ind for ind,el in \n",
    "               enumerate(train_data.columns.isin([x_coord,y_coord])) if el==True]\n",
    "\n",
    "    for enum_ind,ind in enumerate(outlier_feature.index):\n",
    "        if enum_ind > 10: print('Too many images have outlier coordinates. Only printing 10.'); break\n",
    "        impute_coord = y_train[ind][num_col]\n",
    "        view_img(x_train[ind],impute_coord)\n",
    "        print(f'Imputed {feature} coordinates for image {ind}:{impute_coord}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find eyes that are situated lower than average\n",
    "left_eye_upper_bound  = train_data['left_eye_center_y'].mean()  + 3 * train_data['left_eye_center_y'].std()\n",
    "right_eye_upper_bound = train_data['right_eye_center_y'].mean() + 3 * train_data['right_eye_center_y'].std()\n",
    "mouth_center_top_lip_upper_bound = train_data['mouth_center_top_lip_y'].mean() + 3 * train_data['mouth_center_top_lip_y'].std()\n",
    "\n",
    "cutoff_faces = train_data[(train_data['left_eye_center_y'] >= left_eye_upper_bound)  |\n",
    "                          (train_data['right_eye_center_y'] >= right_eye_upper_bound) |\n",
    "                          (train_data['mouth_center_top_lip_y'] >= mouth_center_top_lip_upper_bound)]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove these indices from the training dataset:\n",
    "- 1877\n",
    "- 1907\n",
    "- 2199\n",
    "- 6493"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ind in cutoff_faces.index:\n",
    "#     print('Image Index:',ind)\n",
    "#     view_img(x_train[ind],get_facial_keypoints(train_data[[col for col in train_data.columns if col != 'Image']].values,ind))\n",
    "#     view_img(x_train[ind],get_facial_keypoints(y_train,ind))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove these indices that are poor labeling and poor image quality\n",
    "mask = np.ones(x_train.shape[0],dtype=bool)\n",
    "mask[[1877,1907,2199,6493]] = False\n",
    "test = x_train[mask,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore test images  \n",
    "- Will need cartoons/drawings of faces\n",
    "- Will need some cutoff images (missing bottom lip, etc.)\n",
    "- Will need different ethnicities\n",
    "- Will need blurred images\n",
    "- Will need some images with dark lighting\n",
    "- Will need some images with sun glasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # look at all images in the test set\n",
    "# for ind in range(x_test.shape[0]):\n",
    "#     view_img(x_test[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore preprocessing for data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flip images horizontally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_img_horiz():\n",
    "    \"\"\"\n",
    "    Flip images horizontally for all training images\n",
    "    \"\"\"\n",
    "    # Flip images\n",
    "    flip_img = np.array([np.fliplr(x_train[[ind]][0]) for ind in range(x_train.shape[0])])\n",
    "    \n",
    "    # Flip coordinates\n",
    "    x_columns = [col for col in train_data.columns if '_x' in col]\n",
    "    train_data[x_columns] = train_data[x_columns].applymap(lambda x: 96-x)\n",
    "    flip_coord = train_data[[col for col in train_data if col != 'Image']].to_numpy()\n",
    "    return(flip_img,flip_coord)\n",
    "\n",
    "flipped_img,flipped_coord = flip_img_horiz()\n",
    "\n",
    "view_img(x_train[[5]],coord=get_facial_keypoints(y_train,5))\n",
    "view_img(flipped_img[[5]],coord=get_facial_keypoints(y_train,5))\n",
    "view_img(flipped_img[[5]],coord=get_facial_keypoints(flipped_coord,5))\n",
    "\n",
    "# add to training array\n",
    "x_train.shape\n",
    "y_train.shape\n",
    "# x_train = np.append(x_train,flipped_img,axis=0)\n",
    "# y_train = np.append(y_train,flipped_coord,axis=0)\n",
    "x_train.shape\n",
    "y_train.shape\n",
    "\n",
    "y_train[0]\n",
    "flipped_coord[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add blurring to the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_img(x_train[[5]])\n",
    "view_img(cv2.GaussianBlur(x_train[[5]][0],(5,5),0))\n",
    "view_img(cv2.GaussianBlur(x_train[[5]][0],(5,5),2),coord=get_facial_keypoints(y_train,5))\n",
    "\n",
    "blurr_img = np.array([cv2.GaussianBlur(x_train[[ind]][0],(5,5),2).reshape(96,96,1) for ind in range(x_train.shape[0])])\n",
    "blurr_img.shape\n",
    "x_train.shape\n",
    "# x_train = np.append(x_train,blurr_img)\n",
    "# y_train = np.append(y_train,y_train,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add rotation to the face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows,cols = (96,96)\n",
    "\n",
    "M = cv2.getRotationMatrix2D((cols/2,rows/2),45,1)\n",
    "dst = cv2.warpAffine(x_train[[5]].reshape(96,96,1),M,(cols,rows))\n",
    "\n",
    "\n",
    "view_img(dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add translation to the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_img(x_train[[5]],get_facial_keypoints(y_train,5))\n",
    "def translate_img():\n",
    "    \"\"\"\n",
    "    Add translational shift to the images randomly\n",
    "    \"\"\"\n",
    "    trans_train_data = train_data.copy()\n",
    "    shift = np.random.uniform(low=-.3,high=.3,size=trans_train_data.shape[0])\n",
    "    \n",
    "    rows,cols = (96,96)\n",
    "    shift_x = 96*shift\n",
    "    \n",
    "    shift_y = 96*shift\n",
    "    M = [np.float32([[1,0,shift_x[ind]],[0,1,shift_y[ind]]]) for ind in range(trans_train_data.shape[0])]\n",
    "    trans_img = np.array([cv2.warpAffine(x_train[[ind]].reshape(96,96,1),M[ind],(cols,rows)) for ind in range(trans_train_data.shape[0])])\n",
    "    \n",
    "    x_col = [col for col in train_data.columns if ((col != 'Image') & ('_x' in col))]\n",
    "    y_col = [col for col in train_data.columns if ((col != 'Image') & ('_y' in col))]\n",
    "\n",
    "    shift_x_array = np.array([np.repeat(x,len(x_col))for x in shift_x])\n",
    "    shift_y_array = np.array([np.repeat(y,len(y_col))for y in shift_y])\n",
    "#     print(shift_y_array)\n",
    "#     print(trans_train_data[x_col].shape)\n",
    "#     print(shift_x_array.shape)\n",
    "    \n",
    "    trans_train_data = np.array(trans_train_data[[col for col in trans_train_data.columns if col != 'Image']])\n",
    "\n",
    "    # TODO should we force these to be nan or leave as is?\n",
    "    trans_train_data[trans_train_data > 96]= np.nan\n",
    "    return(trans_img,trans_train_data)\n",
    "\n",
    "trans_img,trans_train_data = translate_img()\n",
    "view_img(trans_img[[5]],get_facial_keypoints(trans_train_data,5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shift_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zoom in/out on images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = x_train[[5]]\n",
    "view_img(img)\n",
    "scale_percent = 60 # percent of original size\n",
    "width = int(96 * scale_percent / 100)\n",
    "height = int(96 * scale_percent / 100)\n",
    "dim = (width, height)\n",
    "img.shape\n",
    "\n",
    "# resize image\n",
    "resized = cv2.resize(img[0], dim, interpolation = cv2.INTER_AREA)\n",
    "# TODO move coordinates for y_train\n",
    "view_img(resized,get_facial_keypoints(y_train,5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing value Imputation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_bottom_lip = [210, 350, 499, 512, 810, 839, 895, 1058, 1194,1230, 1245, 1546, 1548]\n",
    "\n",
    "visualize_labels(bad_bottom_lip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in bad_bottom_lip:\n",
    "    y_train[sample][29] = 94\n",
    "    y_train[sample][28] = y_train[sample][26]\n",
    "    \n",
    "visualize_labels(bad_bottom_lip)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation with mean values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_mouth_coord(y_train):\n",
    "    \n",
    "    count1, count2, count3 = 0,0,0\n",
    "    MOUTH_BOTTOM = 29\n",
    "    MOUTH_TOP = 27\n",
    "    MOUTH_LEFT = 23\n",
    "    MOUTH_RIGHT = 25\n",
    "   \n",
    "    for sample in range(len(y_train)):\n",
    "        if(y_train[sample][MOUTH_TOP] > y_train[sample][MOUTH_BOTTOM]-2):\n",
    "             y_train[sample][MOUTH_TOP] = y_train[sample][MOUTH_BOTTOM] -1\n",
    "             count1 = count1+1   \n",
    "   \n",
    "        if((y_train[sample][MOUTH_LEFT] + y_train[sample][MOUTH_RIGHT]) > (2*y_train[sample][MOUTH_BOTTOM]-2)):\n",
    "             diff = y_train[sample][MOUTH_LEFT] - y_train[sample][MOUTH_RIGHT]\n",
    "            \n",
    "             count2 = count2+1 \n",
    "             if(diff > 0):\n",
    "                y_train[sample][MOUTH_LEFT] = y_train[sample][MOUTH_BOTTOM] -1\n",
    "                y_train[sample][MOUTH_RIGHT] = y_train[sample][MOUTH_BOTTOM] -1 - diff\n",
    "           \n",
    "             else:\n",
    "                y_train[sample][MOUTH_LEFT] = y_train[sample][MOUTH_BOTTOM] -1 + diff\n",
    "                y_train[sample][MOUTH_RIGHT] = y_train[sample][MOUTH_BOTTOM] -1\n",
    "               \n",
    "        if((y_train[sample][MOUTH_LEFT] + y_train[sample][MOUTH_RIGHT]) < (2*y_train[sample][MOUTH_TOP]+2)):    \n",
    "           \n",
    "             diff = y_train[sample][MOUTH_LEFT] - y_train[sample][MOUTH_RIGHT]\n",
    "             count3 = count3+1 \n",
    "       \n",
    "             if(diff > 0):\n",
    "                y_train[sample][MOUTH_LEFT] = y_train[sample][MOUTH_TOP] +1\n",
    "                y_train[sample][MOUTH_RIGHT] = y_train[sample][MOUTH_TOP] +1 - diff\n",
    "           \n",
    "             else:\n",
    "                y_train[sample][MOUTH_LEFT] = y_train[sample][MOUTH_TOP] +1 + diff\n",
    "                y_train[sample][MOUTH_RIGHT] = y_train[sample][MOUTH_TOP] +1\n",
    "                \n",
    "                \n",
    "    #print(count1, count2, count3)       \n",
    "               \n",
    "    return(y_train,[count1, count2, count3]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_na = True\n",
    "y_train = train_data[[col for col in train_data.columns if col != 'Image']].to_numpy()\n",
    "if fill_na:\n",
    "    # https://stackoverflow.com/questions/18689235/numpy-array-replace-nan-values-with-average-of-columns\n",
    "    # get column means\n",
    "    col_mean = np.nanmean(y_train,axis=0)\n",
    "\n",
    "    # find the x,y indices that are missing from y_train\n",
    "    inds = np.where(np.isnan(y_train))\n",
    "\n",
    "    # fill in missing values in y_train with the column means. \"take\" is much more efficient than fancy indexing\n",
    "    y_train[inds] = np.take(col_mean, inds[1])\n",
    "    \n",
    "    y_impute = y_train.copy()\n",
    "    y_impute, num_misalignments = fill_mouth_coord(y_impute)\n",
    "    print(num_misalignments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_fill = True\n",
    "if knn_fill:\n",
    "    y_train = train_data[[col for col in train_data.columns if col != 'Image']].to_numpy()\n",
    "    imputer = KNNImputer(n_neighbors=3, weights='distance')\n",
    "    y_train = imputer.fit_transform(y_train)\n",
    "\n",
    "    y_impute = y_train.copy()\n",
    "    y_impute, num_misalignments = fill_mouth_coord(y_impute)\n",
    "    print(num_misalignments)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_fill = True\n",
    "if iter_fill:\n",
    "    y_train = train_data[[col for col in train_data.columns if col != 'Image']].to_numpy()\n",
    "    imputer = IterativeImputer(max_iter=10000, tol=0.01, random_state=1, n_nearest_features=3)\n",
    "    y_train = imputer.fit_transform(y_train)\n",
    "\n",
    "    y_impute = y_train.copy()\n",
    "    y_impute, num_misalignments = fill_mouth_coord(y_impute)\n",
    "    print(num_misalignments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of fill_mouth_coord()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_data[[col for col in train_data.columns if col != 'Image']].to_numpy()\n",
    "imputer = KNNImputer(n_neighbors=3, weights='distance')\n",
    "y_train = imputer.fit_transform(y_train)\n",
    "fill_mouth_samples = [2912, 2937, 2959, 3075,4196, 4207, 4229, 4258, 4264,4268, 4272, 4276]\n",
    "visualize_labels(fill_mouth_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, num_misalignments = fill_mouth_coord(y_train)\n",
    "visualize_labels(fill_mouth_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
