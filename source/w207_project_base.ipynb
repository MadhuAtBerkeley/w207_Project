{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4dbced162440c393a0a5b7e5aee344711a30e994"
   },
   "source": [
    "# Facial Keypoint Detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "369fa247a546e39d82bdfdc5b7d4ed58baa40e4f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/user1/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/user1/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/user1/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/user1/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/user1/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/user1/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.backend.tensorflow_backend import clear_session\n",
    "from keras.backend.tensorflow_backend import get_session\n",
    "from tensorflow.keras.layers import LeakyReLU, ReLU\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Activation, Convolution2D, BatchNormalization, Flatten\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, MaxPool2D\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "from keras import backend\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import cv2\n",
    "import os, gc, json, math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n",
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print(keras.__version__)\n",
    "import tensorflow\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pixels(data):\n",
    "    \"\"\"\n",
    "    Convert pixels to the right intensity 0-1 and in a square matrix.\n",
    "    \"\"\"\n",
    "    data = np.array([row.split(' ') for row in data['Image']],dtype='float') / 255.0\n",
    "    data = data.reshape(-1,96,96,1)\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_img(sample_img,coord=None):\n",
    "    \"\"\"\n",
    "    Display an image. For debugging, display a coordinate on the image.\n",
    "    input:\n",
    "        - sample_img: numpy array. image to be displayed\n",
    "        - coord: lst. of coordinates in form [[x_coordinate,y_coordinate],[x_coordinate,y_coordinate]]\n",
    "    TODO handle multiple coordinates. Work out bugs with multiple coordinates\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.imshow(sample_img.reshape(96,96),cmap='gray')\n",
    "    if coord is not None:\n",
    "        plt.scatter(coord[0],coord[1],marker = '*',c='r')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_facial_keypoints(data,ind):\n",
    "    \"\"\"\n",
    "    Structure the coordinates for all facial keypoints for a single image.\n",
    "    inputs:\n",
    "        - data: numpy array containing rows as each image sample and columns as facial keypoint coordinates\n",
    "        - ind: index of the image\n",
    "    output:\n",
    "        - numpy array with format [[list of x-coordinates],[list of y-coordinates]]\n",
    "    \"\"\"\n",
    "    data[ind]\n",
    "    it = iter(data[ind])\n",
    "    x_coord = []\n",
    "    y_coord = []\n",
    "\n",
    "    for x in it:\n",
    "        x_coord.append(x)\n",
    "        y_coord.append(next(it))\n",
    "    \n",
    "    return(np.array([x_coord,y_coord]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "# Reset Keras Session\n",
    "def reset_keras():\n",
    "    sess = get_session()\n",
    "    clear_session()\n",
    "    sess.close()\n",
    "    sess = get_session()\n",
    "\n",
    "    try:\n",
    "        del model # this is from global space - change this as you need\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    print(gc.collect()) # if it's done something you should see a number being outputted\n",
    "\n",
    "    # use the same config as you used to create the session\n",
    "    config = tensorflow.ConfigProto()\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = 1\n",
    "    config.gpu_options.visible_device_list = \"0\"\n",
    "    set_session(tensorflow.Session(config=config))\n",
    "    \n",
    "reset_keras()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "fa1b76273d02502e3fd668dddf74ecf522044524"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('../output/'):\n",
    "    os.makedirs('../output/model')\n",
    "    os.makedirs('../output/history')\n",
    "    \n",
    "    \n",
    "model_dir = \"../output/model/\"\n",
    "history_dir = \"../output/history/\"\n",
    "\n",
    "train_file = '../input/training/training.csv'\n",
    "test_file = '../input/test/test.csv'\n",
    "train_data = pd.read_csv(train_file)  \n",
    "#test_data = pd.read_csv(test_file)\n",
    "\n",
    "\n",
    "bad_samples = [1747, 1877, 1881, 1979, 2154, 2199, 2289, 2321, 2453, 3173, 3296, 3447, 4180, 6859,\n",
    "              2090, 2175, 1907, 2562, 2818, 3296, 3447, 4263, 4482, 4490, 4636, 5059, 6493, 6906]\n",
    "\n",
    "#train_data = train_data.drop(bad_samples).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cfd045f7166f9cce2e2075b3ead83813d07012c8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e1b88f1528838c0a8fec61f9a02a70b5077312e9"
   },
   "source": [
    "Create training vector with images and normalize thee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = convert_pixels(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a833f4cc5e559774d3a310fd09d40d31e49e71da"
   },
   "source": [
    "Generate labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "e9d804a035809cdf8ffda19f41ce3feb278a38fb"
   },
   "outputs": [],
   "source": [
    "y_train = train_data[[col for col in train_data.columns if col != 'Image']].to_numpy()\n",
    "imputer = KNNImputer(n_neighbors=3, weights='distance')\n",
    "\n",
    "#imputer = IterativeImputer(max_iter=1000, tol=0.01)\n",
    "y_train = imputer.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set feature engineering parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_na = False\n",
    "add_flip_horiz = False\n",
    "add_blur_img = False\n",
    "add_rotate_img = False\n",
    "orig_x_train = x_train.copy()\n",
    "orig_y_train = y_train.copy()\n",
    "y_train = imputer.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill NA in the training labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if fill_na:\n",
    "    # https://stackoverflow.com/questions/18689235/numpy-array-replace-nan-values-with-average-of-columns\n",
    "    # get column means\n",
    "    col_mean = np.nanmean(y_train,axis=0)\n",
    "\n",
    "    # find the x,y indices that are missing from y_train\n",
    "    inds = np.where(np.isnan(y_train))\n",
    "\n",
    "    # fill in missing values in y_train with the column means. \"take\" is much more efficient than fancy indexing\n",
    "    y_train[inds] = np.take(col_mean, inds[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flip images horizontally and add to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_img_horiz():\n",
    "    \"\"\"\n",
    "    Flip images horizontally for all training images\n",
    "    \"\"\"\n",
    "    # Flip images\n",
    "    flip_img = np.array([np.fliplr(orig_x_train[[ind]][0]) for ind in range(orig_x_train.shape[0])])\n",
    "    \n",
    "    # Flip coordinates\n",
    "    train_data_flip = train_data.copy()\n",
    "    x_columns = [col for col in train_data.columns if '_x' in col]\n",
    "    train_data_flip[x_columns] = train_data[x_columns].applymap(lambda x: 96-x)\n",
    "    \n",
    "    #left and right are swapped so undo\n",
    "    left_columns = [col for col in train_data.columns if 'left' in col]\n",
    "    right_columns = [col for col in train_data.columns if 'right' in col]\n",
    "    train_data_flip[left_columns+right_columns] = train_data_flip[right_columns+left_columns]\n",
    "    \n",
    "    flip_coord = train_data_flip[[col for col in train_data if col != 'Image']].to_numpy()\n",
    "    return(flip_img,flip_coord)\n",
    "\n",
    "if add_flip_horiz:\n",
    "    # Apply the augmentation and add the new data to the training set\n",
    "    flipped_img,flipped_coord = flip_img_horiz()\n",
    "    flipped_coord = imputer.fit_transform(flipped_coord)\n",
    "    #x_train = np.append(x_train,flipped_img,axis=0)\n",
    "    #y_train = np.append(y_train,flipped_coord,axis=0)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Gaussian blurring with a 5x5 filter with $\\sigma$ = 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blur_img():\n",
    "    \"\"\"\n",
    "    Add Gaussian blurring to the images\n",
    "    \"\"\"\n",
    "    # https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_filtering/py_filtering.html\n",
    "    blur_img = np.array([cv2.GaussianBlur(orig_x_train[[ind]][0],(5,5),2).reshape(96,96,1) for ind in range(orig_x_train.shape[0])])\n",
    "    \n",
    "    return(blur_img)\n",
    "\n",
    "if add_blur_img:\n",
    "    x_train = np.append(x_train,blur_img(),axis=0)\n",
    "    y_train = np.append(y_train,orig_y_train,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_img(x_train, y_train):\n",
    "    \"\"\"\"\n",
    "    Rotate images by angles between [5, 10, 15 degrees]\n",
    "    \"\"\"\n",
    "    angles = [7, -7, 10, -10, 15, -15]\n",
    "    b = np.ones((1,3))\n",
    "    rows,cols = (96,96)\n",
    "    x_train_rot = []\n",
    "    y_train_rot = y_train.copy()\n",
    "    M_angles = [cv2.getRotationMatrix2D((cols/2,rows/2),angle,1) for angle in angles]\n",
    "    \n",
    "    for i in range(x_train.shape[0]):\n",
    "        #M = cv2.getRotationMatrix2D((cols/2,rows/2),np.random.choice(angles,1),1)\n",
    "        M = M_angles[np.random.choice(len(M_angles))]\n",
    "        x_train_rot.append((cv2.warpAffine(x_train[[i]].reshape(rows,cols,1),M,(cols,rows)).reshape(96,96,1)))\n",
    "       \n",
    "        #apply affine transformation to (x,y) labels\n",
    "        for j in range(int(y_train.shape[1]/2)):\n",
    "            b[:,0:2] = y_train[i,2*j:2*j+2]\n",
    "            y_train_rot[i,2*j:2*j+2] = np.dot(b,M.transpose()) \n",
    "    \n",
    "    x_train_rot = np.array(x_train_rot)\n",
    "    return x_train_rot, y_train_rot\n",
    "\n",
    "if add_rotate_img:\n",
    "    if add_flip_horiz:\n",
    "        x_rotate, y_rotate = rotate_img(flipped_img,flipped_coord) #x_train,y_train)\n",
    "    else:\n",
    "        x_rotate, y_rotate = rotate_img(x_train,y_train)\n",
    "    x_train = np.append(x_train,x_rotate,axis=0)\n",
    "    y_train = np.append(y_train,y_rotate,axis=0)   \n",
    "    \n",
    "else:\n",
    "    if add_flip_horiz:\n",
    "          x_train = np.append(x_train,flipped_image,axis=0)\n",
    "          y_train = np.append(y_train,flipped_coord,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callback function if detailed log required\n",
    "class History(tensorflow.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.train_loss = []\n",
    "        self.train_rmse = []\n",
    "        self.val_rmse = []\n",
    "        self.val_loss = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.train_loss.append(logs.get('loss'))\n",
    "        self.train_rmse.append(logs.get('rmse'))\n",
    "        \n",
    "    def on_epoch_end(self, batch, logs={}):    \n",
    "        self.val_rmse.append(logs.get('val_rmse'))\n",
    "        self.val_loss.append(logs.get('val_loss'))\n",
    "        \n",
    "# Implement ModelCheckPoint callback function to save CNN model\n",
    "class CNN_ModelCheckpoint(tensorflow.keras.callbacks.Callback):\n",
    "\n",
    "    def __init__(self, model, filename):\n",
    "        self.filename = filename\n",
    "        self.cnn_model = model\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.max_val_rmse = math.inf\n",
    "        \n",
    " \n",
    "    def on_epoch_end(self, batch, logs={}):    \n",
    "        val_rmse = logs.get('val_rmse')\n",
    "        if(val_rmse < self.max_val_rmse):\n",
    "           self.max_val_rmse = val_rmse\n",
    "           self.cnn_model.save_weights(self.filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/user1/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/user1/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 96, 96, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 96, 96, 32)        288       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 96, 96, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1 (Batc (None, 96, 96, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 48, 48, 64)        18432     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 48, 48, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_1 (Ba (None, 48, 48, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 96)        55296     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 24, 24, 96)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_2 (Ba (None, 24, 24, 96)        384       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 12, 12, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 12, 12, 128)       110592    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_3 (Ba (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 6, 6, 256)         294912    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_4 (Ba (None, 6, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 3, 3, 512)         1179648   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 3, 3, 512)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_5 (Ba (None, 3, 3, 512)         2048      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               2359808   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                15390     \n",
      "=================================================================\n",
      "Total params: 4,038,718\n",
      "Trainable params: 4,036,542\n",
      "Non-trainable params: 2,176\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def base_model():\n",
    "    model_input = Input(shape=(96,96,1))\n",
    "\n",
    "    x = Convolution2D(32, (3,3), padding='same', use_bias=False)(model_input)\n",
    "    x = LeakyReLU(alpha = 0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "    x = Convolution2D(64, (3,3), padding='same', use_bias=False)(x)\n",
    "    x = LeakyReLU(alpha = 0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "    x = Convolution2D(96, (3,3), padding='same', use_bias=False)(x)\n",
    "    x = LeakyReLU(alpha = 0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "    x = Convolution2D(128, (3,3),padding='same', use_bias=False)(x)\n",
    "    x = LeakyReLU(alpha = 0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "    x = Convolution2D(256, (3,3),padding='same',use_bias=False)(x)\n",
    "    x = LeakyReLU(alpha = 0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "    x = Convolution2D(512, (3,3), padding='same', use_bias=False)(x)\n",
    "    x = LeakyReLU(alpha = 0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(512,activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    model_output = Dense(30)(x)\n",
    "    model = Model(model_input, model_output, name=\"base_model\")\n",
    "    return model\n",
    "\n",
    "model = base_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/user1/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/utils/losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "# Custom RMSE metric\n",
    "def rmse(y_true, y_pred):\n",
    "    return backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))\n",
    "\n",
    "#from tensorflow.keras.optimizers.schedules import InverseTimeDecay\n",
    "\n",
    "# Use Nadam optimizer with variable learning rate\n",
    "optimizer = Nadam(lr=0.00001,\n",
    "                  beta_1=0.9,\n",
    "                  beta_2=0.999,\n",
    "                  epsilon=1e-08,\n",
    "                  schedule_decay=0.004)\n",
    "\n",
    "\n",
    "# Loss: MSE and Metric = RMSE\n",
    "model.compile(optimizer= optimizer, \n",
    "              loss='mean_squared_error',\n",
    "              metrics=[rmse])\n",
    "\n",
    "#Callback to save the best model\n",
    "saveBase_Model = CNN_ModelCheckpoint(model, model_dir+\"base_model_weights_1.h5\")\n",
    "\n",
    "#define callback functions\n",
    "callbacks = [#EarlyStopping(monitor='val_rmse', patience=3, verbose=2),\n",
    "             saveBase_Model]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e4cf4686b410841f2e34dbb081f3429d1b0f67e9"
   },
   "source": [
    "Run for 1000 epochs and keeping 20% train-valid split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "894af9cbfcf2dca50e7407946cad318157b77d0a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5639 samples, validate on 1410 samples\n",
      "WARNING:tensorflow:From /home/user1/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "5639/5639 [==============================] - 8s 1ms/sample - loss: 2573.6141 - rmse: 50.6997 - val_loss: 2676.4422 - val_rmse: 51.7079\n",
      "Epoch 2/500\n",
      "5639/5639 [==============================] - 4s 641us/sample - loss: 2356.3563 - rmse: 48.5006 - val_loss: 2673.7063 - val_rmse: 51.6815\n",
      "Epoch 3/500\n",
      "5639/5639 [==============================] - 4s 644us/sample - loss: 2071.5292 - rmse: 45.4438 - val_loss: 2671.9345 - val_rmse: 51.6643\n",
      "Epoch 4/500\n",
      "5639/5639 [==============================] - 4s 654us/sample - loss: 1734.9676 - rmse: 41.5427 - val_loss: 2666.3240 - val_rmse: 51.6100\n",
      "Epoch 5/500\n",
      "5639/5639 [==============================] - 4s 660us/sample - loss: 1395.8675 - rmse: 37.2092 - val_loss: 2646.2285 - val_rmse: 51.4149\n",
      "Epoch 6/500\n",
      "5639/5639 [==============================] - 4s 658us/sample - loss: 1092.1217 - rmse: 32.8539 - val_loss: 2596.8966 - val_rmse: 50.9329\n",
      "Epoch 7/500\n",
      "5639/5639 [==============================] - 4s 643us/sample - loss: 836.8956 - rmse: 28.6925 - val_loss: 2501.3337 - val_rmse: 49.9856\n",
      "Epoch 8/500\n",
      "5639/5639 [==============================] - 4s 650us/sample - loss: 629.7316 - rmse: 24.8228 - val_loss: 2359.2147 - val_rmse: 48.5420\n",
      "Epoch 9/500\n",
      "5639/5639 [==============================] - 4s 655us/sample - loss: 468.5300 - rmse: 21.3334 - val_loss: 2180.4604 - val_rmse: 46.6617\n",
      "Epoch 10/500\n",
      "5639/5639 [==============================] - 4s 652us/sample - loss: 343.4105 - rmse: 18.1797 - val_loss: 1975.3799 - val_rmse: 44.4045\n",
      "Epoch 11/500\n",
      "5639/5639 [==============================] - 4s 654us/sample - loss: 251.0367 - rmse: 15.4716 - val_loss: 1782.6492 - val_rmse: 42.1698\n",
      "Epoch 12/500\n",
      "5639/5639 [==============================] - 4s 643us/sample - loss: 183.6353 - rmse: 13.1672 - val_loss: 1568.0734 - val_rmse: 39.5356\n",
      "Epoch 13/500\n",
      "5639/5639 [==============================] - 4s 644us/sample - loss: 135.9425 - rmse: 11.2819 - val_loss: 1373.2436 - val_rmse: 36.9710\n",
      "Epoch 14/500\n",
      "5639/5639 [==============================] - 4s 648us/sample - loss: 102.7295 - rmse: 9.7759 - val_loss: 1203.6790 - val_rmse: 34.5789\n",
      "Epoch 15/500\n",
      "5639/5639 [==============================] - 4s 648us/sample - loss: 80.3747 - rmse: 8.6328 - val_loss: 1031.6897 - val_rmse: 31.9743\n",
      "Epoch 16/500\n",
      "5639/5639 [==============================] - 4s 647us/sample - loss: 66.0834 - rmse: 7.8169 - val_loss: 911.7642 - val_rmse: 29.9847\n",
      "Epoch 17/500\n",
      "5639/5639 [==============================] - 4s 643us/sample - loss: 57.3709 - rmse: 7.2995 - val_loss: 780.6754 - val_rmse: 27.7288\n",
      "Epoch 18/500\n",
      "5639/5639 [==============================] - 4s 644us/sample - loss: 51.2208 - rmse: 6.9038 - val_loss: 688.2678 - val_rmse: 25.9649\n",
      "Epoch 19/500\n",
      "5639/5639 [==============================] - 4s 650us/sample - loss: 47.6673 - rmse: 6.6691 - val_loss: 591.0060 - val_rmse: 24.0407\n",
      "Epoch 20/500\n",
      "5639/5639 [==============================] - 4s 644us/sample - loss: 44.9379 - rmse: 6.4890 - val_loss: 510.6545 - val_rmse: 22.2883\n",
      "Epoch 21/500\n",
      "5639/5639 [==============================] - 4s 644us/sample - loss: 43.4462 - rmse: 6.3801 - val_loss: 432.4324 - val_rmse: 20.5031\n",
      "Epoch 22/500\n",
      "5639/5639 [==============================] - 4s 645us/sample - loss: 42.5801 - rmse: 6.3177 - val_loss: 372.8171 - val_rmse: 18.9615\n",
      "Epoch 23/500\n",
      "5639/5639 [==============================] - 4s 644us/sample - loss: 41.5520 - rmse: 6.2495 - val_loss: 322.8011 - val_rmse: 17.5802\n",
      "Epoch 24/500\n",
      "5639/5639 [==============================] - 4s 646us/sample - loss: 41.1197 - rmse: 6.2235 - val_loss: 255.2549 - val_rmse: 15.6788\n",
      "Epoch 25/500\n",
      "5639/5639 [==============================] - 4s 652us/sample - loss: 40.4380 - rmse: 6.1704 - val_loss: 211.7301 - val_rmse: 14.2276\n",
      "Epoch 26/500\n",
      "5639/5639 [==============================] - 4s 651us/sample - loss: 40.1301 - rmse: 6.1545 - val_loss: 173.6909 - val_rmse: 12.8410\n",
      "Epoch 27/500\n",
      "5639/5639 [==============================] - 4s 649us/sample - loss: 40.0795 - rmse: 6.1484 - val_loss: 132.9246 - val_rmse: 11.2255\n",
      "Epoch 28/500\n",
      "5639/5639 [==============================] - 4s 644us/sample - loss: 39.5153 - rmse: 6.1073 - val_loss: 107.8606 - val_rmse: 10.0640\n",
      "Epoch 29/500\n",
      "5639/5639 [==============================] - 4s 645us/sample - loss: 39.2068 - rmse: 6.0844 - val_loss: 89.5130 - val_rmse: 9.0927\n",
      "Epoch 30/500\n",
      "5639/5639 [==============================] - 4s 650us/sample - loss: 39.0610 - rmse: 6.0730 - val_loss: 63.7209 - val_rmse: 7.6784\n",
      "Epoch 31/500\n",
      "5639/5639 [==============================] - 4s 646us/sample - loss: 38.9168 - rmse: 6.0569 - val_loss: 45.9810 - val_rmse: 6.4291\n",
      "Epoch 32/500\n",
      "5639/5639 [==============================] - 4s 650us/sample - loss: 38.4464 - rmse: 6.0211 - val_loss: 40.6258 - val_rmse: 6.0268\n",
      "Epoch 33/500\n",
      "5639/5639 [==============================] - 4s 653us/sample - loss: 38.3585 - rmse: 6.0164 - val_loss: 35.7652 - val_rmse: 5.6185\n",
      "Epoch 34/500\n",
      "5639/5639 [==============================] - 4s 654us/sample - loss: 37.9131 - rmse: 5.9750 - val_loss: 32.0979 - val_rmse: 5.2931\n",
      "Epoch 35/500\n",
      "5639/5639 [==============================] - 4s 647us/sample - loss: 37.7771 - rmse: 5.9738 - val_loss: 23.8173 - val_rmse: 4.4824\n",
      "Epoch 36/500\n",
      "5639/5639 [==============================] - 4s 634us/sample - loss: 37.6846 - rmse: 5.9643 - val_loss: 24.5440 - val_rmse: 4.5580\n",
      "Epoch 37/500\n",
      "5639/5639 [==============================] - 4s 632us/sample - loss: 37.6544 - rmse: 5.9597 - val_loss: 24.4000 - val_rmse: 4.5403\n",
      "Epoch 38/500\n",
      "5639/5639 [==============================] - 4s 647us/sample - loss: 37.2619 - rmse: 5.9291 - val_loss: 19.0794 - val_rmse: 3.9672\n",
      "Epoch 39/500\n",
      "5639/5639 [==============================] - 4s 649us/sample - loss: 37.2417 - rmse: 5.9319 - val_loss: 18.2600 - val_rmse: 3.8666\n",
      "Epoch 40/500\n",
      "5639/5639 [==============================] - 4s 650us/sample - loss: 36.9168 - rmse: 5.8963 - val_loss: 17.5605 - val_rmse: 3.7855\n",
      "Epoch 41/500\n",
      "5639/5639 [==============================] - 4s 635us/sample - loss: 36.6391 - rmse: 5.8808 - val_loss: 17.5963 - val_rmse: 3.7895\n",
      "Epoch 42/500\n",
      "5639/5639 [==============================] - 4s 658us/sample - loss: 36.4440 - rmse: 5.8690 - val_loss: 15.9861 - val_rmse: 3.5929\n",
      "Epoch 43/500\n",
      "5639/5639 [==============================] - 4s 656us/sample - loss: 36.2025 - rmse: 5.8474 - val_loss: 14.8176 - val_rmse: 3.4401\n",
      "Epoch 44/500\n",
      "5639/5639 [==============================] - 4s 639us/sample - loss: 36.4270 - rmse: 5.8619 - val_loss: 15.7965 - val_rmse: 3.5711\n",
      "Epoch 45/500\n",
      "5639/5639 [==============================] - 4s 642us/sample - loss: 36.1537 - rmse: 5.8442 - val_loss: 13.8380 - val_rmse: 3.3161\n",
      "Epoch 46/500\n",
      "5639/5639 [==============================] - 4s 636us/sample - loss: 36.0045 - rmse: 5.8290 - val_loss: 16.0237 - val_rmse: 3.5810\n",
      "Epoch 47/500\n",
      "5639/5639 [==============================] - 4s 649us/sample - loss: 35.6518 - rmse: 5.7989 - val_loss: 13.6407 - val_rmse: 3.2911\n",
      "Epoch 48/500\n",
      "5639/5639 [==============================] - 4s 635us/sample - loss: 35.6505 - rmse: 5.8005 - val_loss: 15.2176 - val_rmse: 3.4948\n",
      "Epoch 49/500\n",
      "5639/5639 [==============================] - 4s 635us/sample - loss: 35.2378 - rmse: 5.7699 - val_loss: 14.1592 - val_rmse: 3.3642\n",
      "Epoch 50/500\n",
      "5639/5639 [==============================] - 4s 649us/sample - loss: 35.0028 - rmse: 5.7518 - val_loss: 13.3920 - val_rmse: 3.2593\n",
      "Epoch 51/500\n",
      "5639/5639 [==============================] - 4s 633us/sample - loss: 35.1506 - rmse: 5.7606 - val_loss: 13.4375 - val_rmse: 3.2692\n",
      "Epoch 52/500\n",
      "5639/5639 [==============================] - 4s 630us/sample - loss: 34.8936 - rmse: 5.7345 - val_loss: 14.5783 - val_rmse: 3.4236\n",
      "Epoch 53/500\n",
      "5639/5639 [==============================] - 4s 633us/sample - loss: 34.4804 - rmse: 5.7068 - val_loss: 15.6485 - val_rmse: 3.5543\n",
      "Epoch 54/500\n",
      "5639/5639 [==============================] - 4s 638us/sample - loss: 34.4805 - rmse: 5.7054 - val_loss: 13.4260 - val_rmse: 3.2679\n",
      "Epoch 55/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5639/5639 [==============================] - 3s 609us/sample - loss: 34.4056 - rmse: 5.7032 - val_loss: 12.9606 - val_rmse: 3.2095\n",
      "Epoch 56/500\n",
      "5639/5639 [==============================] - 3s 608us/sample - loss: 34.1813 - rmse: 5.6772 - val_loss: 12.7667 - val_rmse: 3.1821\n",
      "Epoch 57/500\n",
      "5639/5639 [==============================] - 3s 597us/sample - loss: 34.3550 - rmse: 5.6943 - val_loss: 13.9688 - val_rmse: 3.3469\n",
      "Epoch 58/500\n",
      "5639/5639 [==============================] - 3s 606us/sample - loss: 33.7362 - rmse: 5.6399 - val_loss: 12.5619 - val_rmse: 3.1551\n",
      "Epoch 59/500\n",
      "5639/5639 [==============================] - 3s 599us/sample - loss: 33.6245 - rmse: 5.6364 - val_loss: 13.6610 - val_rmse: 3.3048\n",
      "Epoch 60/500\n",
      "5639/5639 [==============================] - 3s 595us/sample - loss: 33.5989 - rmse: 5.6300 - val_loss: 14.0694 - val_rmse: 3.3553\n",
      "Epoch 61/500\n",
      "5639/5639 [==============================] - 3s 612us/sample - loss: 33.6196 - rmse: 5.6315 - val_loss: 12.4969 - val_rmse: 3.1473\n",
      "Epoch 62/500\n",
      "5639/5639 [==============================] - 3s 600us/sample - loss: 33.1376 - rmse: 5.5878 - val_loss: 13.2864 - val_rmse: 3.2608\n",
      "Epoch 63/500\n",
      "5639/5639 [==============================] - 3s 594us/sample - loss: 33.2494 - rmse: 5.6010 - val_loss: 12.8566 - val_rmse: 3.1994\n",
      "Epoch 64/500\n",
      "5639/5639 [==============================] - 3s 598us/sample - loss: 32.9884 - rmse: 5.5786 - val_loss: 14.4369 - val_rmse: 3.4175\n",
      "Epoch 65/500\n",
      "5639/5639 [==============================] - 3s 594us/sample - loss: 32.8196 - rmse: 5.5626 - val_loss: 13.0294 - val_rmse: 3.2300\n",
      "Epoch 66/500\n",
      "5639/5639 [==============================] - 3s 598us/sample - loss: 32.6704 - rmse: 5.5492 - val_loss: 12.5557 - val_rmse: 3.1573\n",
      "Epoch 67/500\n",
      "5639/5639 [==============================] - 3s 595us/sample - loss: 32.3235 - rmse: 5.5227 - val_loss: 13.7926 - val_rmse: 3.3339\n",
      "Epoch 68/500\n",
      "5639/5639 [==============================] - 3s 594us/sample - loss: 32.2831 - rmse: 5.5207 - val_loss: 13.5839 - val_rmse: 3.3080\n",
      "Epoch 69/500\n",
      "5639/5639 [==============================] - 3s 607us/sample - loss: 32.0331 - rmse: 5.4988 - val_loss: 12.4088 - val_rmse: 3.1425\n",
      "Epoch 70/500\n",
      "5639/5639 [==============================] - 3s 598us/sample - loss: 32.3277 - rmse: 5.5165 - val_loss: 12.4662 - val_rmse: 3.1438\n",
      "Epoch 71/500\n",
      "5639/5639 [==============================] - 3s 612us/sample - loss: 32.1071 - rmse: 5.5051 - val_loss: 12.4071 - val_rmse: 3.1386\n",
      "Epoch 72/500\n",
      "5639/5639 [==============================] - 3s 596us/sample - loss: 31.7745 - rmse: 5.4723 - val_loss: 14.1977 - val_rmse: 3.3851\n",
      "Epoch 73/500\n",
      "5639/5639 [==============================] - 3s 606us/sample - loss: 31.5307 - rmse: 5.4536 - val_loss: 12.1812 - val_rmse: 3.1088\n",
      "Epoch 74/500\n",
      "5639/5639 [==============================] - 3s 597us/sample - loss: 31.2202 - rmse: 5.4220 - val_loss: 12.6681 - val_rmse: 3.1811\n",
      "Epoch 75/500\n",
      "5639/5639 [==============================] - 3s 594us/sample - loss: 31.2320 - rmse: 5.4271 - val_loss: 12.5432 - val_rmse: 3.1641\n",
      "Epoch 76/500\n",
      "5639/5639 [==============================] - 3s 612us/sample - loss: 31.2788 - rmse: 5.4331 - val_loss: 14.6400 - val_rmse: 3.4388\n",
      "Epoch 77/500\n",
      "5639/5639 [==============================] - 3s 599us/sample - loss: 31.0289 - rmse: 5.4091 - val_loss: 12.5524 - val_rmse: 3.1660\n",
      "Epoch 78/500\n",
      "5639/5639 [==============================] - 3s 609us/sample - loss: 30.8055 - rmse: 5.3956 - val_loss: 12.0178 - val_rmse: 3.0911\n",
      "Epoch 79/500\n",
      "5639/5639 [==============================] - 3s 598us/sample - loss: 30.7933 - rmse: 5.3864 - val_loss: 12.3182 - val_rmse: 3.1323\n",
      "Epoch 80/500\n",
      "5639/5639 [==============================] - 3s 600us/sample - loss: 30.5156 - rmse: 5.3630 - val_loss: 12.5445 - val_rmse: 3.1641\n",
      "Epoch 81/500\n",
      "5639/5639 [==============================] - 3s 612us/sample - loss: 30.4468 - rmse: 5.3595 - val_loss: 11.4789 - val_rmse: 3.0055\n",
      "Epoch 82/500\n",
      "5639/5639 [==============================] - 3s 600us/sample - loss: 30.6332 - rmse: 5.3779 - val_loss: 12.2537 - val_rmse: 3.1226\n",
      "Epoch 83/500\n",
      "5639/5639 [==============================] - 3s 597us/sample - loss: 30.2446 - rmse: 5.3412 - val_loss: 12.2267 - val_rmse: 3.1210\n",
      "Epoch 84/500\n",
      "5639/5639 [==============================] - 3s 598us/sample - loss: 30.0787 - rmse: 5.3249 - val_loss: 12.1472 - val_rmse: 3.1117\n",
      "Epoch 85/500\n",
      "5639/5639 [==============================] - 3s 597us/sample - loss: 30.1184 - rmse: 5.3275 - val_loss: 12.3868 - val_rmse: 3.1424\n",
      "Epoch 86/500\n",
      "5639/5639 [==============================] - 3s 597us/sample - loss: 29.7764 - rmse: 5.2994 - val_loss: 12.0605 - val_rmse: 3.0946\n",
      "Epoch 87/500\n",
      "5639/5639 [==============================] - 4s 632us/sample - loss: 29.7002 - rmse: 5.2939 - val_loss: 12.5462 - val_rmse: 3.1668\n",
      "Epoch 88/500\n",
      "5639/5639 [==============================] - 4s 637us/sample - loss: 29.5753 - rmse: 5.2805 - val_loss: 11.5356 - val_rmse: 3.0129\n",
      "Epoch 89/500\n",
      "5639/5639 [==============================] - 4s 673us/sample - loss: 29.4923 - rmse: 5.2756 - val_loss: 11.6982 - val_rmse: 3.0454\n",
      "Epoch 90/500\n",
      "5639/5639 [==============================] - 3s 606us/sample - loss: 29.3305 - rmse: 5.2552 - val_loss: 11.2233 - val_rmse: 2.9726\n",
      "Epoch 91/500\n",
      "5639/5639 [==============================] - 3s 602us/sample - loss: 29.2901 - rmse: 5.2532 - val_loss: 12.0946 - val_rmse: 3.1035\n",
      "Epoch 92/500\n",
      "5639/5639 [==============================] - 3s 589us/sample - loss: 29.1998 - rmse: 5.2446 - val_loss: 14.7864 - val_rmse: 3.4688\n",
      "Epoch 93/500\n",
      "5639/5639 [==============================] - 3s 587us/sample - loss: 28.8982 - rmse: 5.2222 - val_loss: 11.9775 - val_rmse: 3.0941\n",
      "Epoch 94/500\n",
      "5639/5639 [==============================] - 3s 589us/sample - loss: 28.7326 - rmse: 5.2064 - val_loss: 12.1522 - val_rmse: 3.1194\n",
      "Epoch 95/500\n",
      "5639/5639 [==============================] - 3s 585us/sample - loss: 28.5648 - rmse: 5.1918 - val_loss: 11.2844 - val_rmse: 2.9844\n",
      "Epoch 96/500\n",
      "5639/5639 [==============================] - 3s 586us/sample - loss: 28.4335 - rmse: 5.1799 - val_loss: 11.4851 - val_rmse: 3.0155\n",
      "Epoch 97/500\n",
      "5639/5639 [==============================] - 3s 603us/sample - loss: 28.3561 - rmse: 5.1707 - val_loss: 11.0624 - val_rmse: 2.9516\n",
      "Epoch 98/500\n",
      "5639/5639 [==============================] - 3s 588us/sample - loss: 28.2252 - rmse: 5.1616 - val_loss: 12.8842 - val_rmse: 3.2201\n",
      "Epoch 99/500\n",
      "5639/5639 [==============================] - 3s 586us/sample - loss: 27.9560 - rmse: 5.1384 - val_loss: 11.6196 - val_rmse: 3.0407\n",
      "Epoch 100/500\n",
      "5639/5639 [==============================] - 3s 588us/sample - loss: 28.1351 - rmse: 5.1480 - val_loss: 12.0234 - val_rmse: 3.1014\n",
      "Epoch 101/500\n",
      "5639/5639 [==============================] - 3s 589us/sample - loss: 27.7873 - rmse: 5.1204 - val_loss: 11.2248 - val_rmse: 2.9817\n",
      "Epoch 102/500\n",
      "5639/5639 [==============================] - 3s 584us/sample - loss: 27.7112 - rmse: 5.1084 - val_loss: 11.9602 - val_rmse: 3.0859\n",
      "Epoch 103/500\n",
      "5639/5639 [==============================] - 3s 601us/sample - loss: 27.5615 - rmse: 5.1013 - val_loss: 10.6083 - val_rmse: 2.8812\n",
      "Epoch 104/500\n",
      "5639/5639 [==============================] - 3s 587us/sample - loss: 27.3285 - rmse: 5.0804 - val_loss: 11.7793 - val_rmse: 3.0607\n",
      "Epoch 105/500\n",
      "5639/5639 [==============================] - 3s 587us/sample - loss: 27.3939 - rmse: 5.0831 - val_loss: 11.0202 - val_rmse: 2.9467\n",
      "Epoch 106/500\n",
      "5639/5639 [==============================] - 3s 588us/sample - loss: 27.3943 - rmse: 5.0763 - val_loss: 11.1335 - val_rmse: 2.9629\n",
      "Epoch 107/500\n",
      "5639/5639 [==============================] - 3s 597us/sample - loss: 27.1265 - rmse: 5.0580 - val_loss: 10.4709 - val_rmse: 2.8650\n",
      "Epoch 108/500\n",
      "5639/5639 [==============================] - 3s 604us/sample - loss: 27.0684 - rmse: 5.0503 - val_loss: 10.4429 - val_rmse: 2.8520\n",
      "Epoch 109/500\n",
      "5639/5639 [==============================] - 3s 597us/sample - loss: 26.9031 - rmse: 5.0331 - val_loss: 10.3827 - val_rmse: 2.8472\n",
      "Epoch 110/500\n",
      "5639/5639 [==============================] - 3s 590us/sample - loss: 26.5969 - rmse: 5.0068 - val_loss: 11.0245 - val_rmse: 2.9491\n",
      "Epoch 111/500\n",
      "5639/5639 [==============================] - 3s 587us/sample - loss: 26.4159 - rmse: 4.9924 - val_loss: 11.5770 - val_rmse: 3.0315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/500\n",
      "5639/5639 [==============================] - 3s 596us/sample - loss: 26.5224 - rmse: 4.9996 - val_loss: 10.4250 - val_rmse: 2.8499\n",
      "Epoch 113/500\n",
      "5639/5639 [==============================] - 3s 586us/sample - loss: 26.2834 - rmse: 4.9801 - val_loss: 14.6885 - val_rmse: 3.4630\n",
      "Epoch 114/500\n",
      "5639/5639 [==============================] - 3s 599us/sample - loss: 25.9838 - rmse: 4.9514 - val_loss: 10.2930 - val_rmse: 2.8330\n",
      "Epoch 115/500\n",
      "5639/5639 [==============================] - 3s 586us/sample - loss: 25.9196 - rmse: 4.9450 - val_loss: 10.9937 - val_rmse: 2.9483\n",
      "Epoch 116/500\n",
      "5639/5639 [==============================] - 3s 583us/sample - loss: 25.8699 - rmse: 4.9388 - val_loss: 11.1047 - val_rmse: 2.9638\n",
      "Epoch 117/500\n",
      "5639/5639 [==============================] - 3s 583us/sample - loss: 25.8484 - rmse: 4.9369 - val_loss: 10.3352 - val_rmse: 2.8407\n",
      "Epoch 118/500\n",
      "5639/5639 [==============================] - 3s 589us/sample - loss: 25.6843 - rmse: 4.9244 - val_loss: 11.6413 - val_rmse: 3.0490\n",
      "Epoch 119/500\n",
      "5639/5639 [==============================] - 3s 585us/sample - loss: 25.3385 - rmse: 4.8889 - val_loss: 11.5050 - val_rmse: 3.0318\n",
      "Epoch 120/500\n",
      "5639/5639 [==============================] - 3s 585us/sample - loss: 25.4943 - rmse: 4.9042 - val_loss: 11.1537 - val_rmse: 2.9698\n",
      "Epoch 121/500\n",
      "5639/5639 [==============================] - 3s 583us/sample - loss: 25.3984 - rmse: 4.8908 - val_loss: 12.5736 - val_rmse: 3.1703\n",
      "Epoch 122/500\n",
      "5639/5639 [==============================] - 3s 586us/sample - loss: 25.3383 - rmse: 4.8903 - val_loss: 10.9463 - val_rmse: 2.9388\n",
      "Epoch 123/500\n",
      "5639/5639 [==============================] - 3s 584us/sample - loss: 25.0278 - rmse: 4.8552 - val_loss: 11.3414 - val_rmse: 3.0080\n",
      "Epoch 124/500\n",
      "5639/5639 [==============================] - 3s 582us/sample - loss: 24.7636 - rmse: 4.8338 - val_loss: 10.9425 - val_rmse: 2.9411\n",
      "Epoch 125/500\n",
      "5639/5639 [==============================] - 3s 583us/sample - loss: 24.7074 - rmse: 4.8286 - val_loss: 10.5424 - val_rmse: 2.8813\n",
      "Epoch 126/500\n",
      "5639/5639 [==============================] - 3s 585us/sample - loss: 24.6503 - rmse: 4.8191 - val_loss: 10.5196 - val_rmse: 2.8774\n",
      "Epoch 127/500\n",
      "5639/5639 [==============================] - 3s 597us/sample - loss: 24.5783 - rmse: 4.8188 - val_loss: 10.1910 - val_rmse: 2.8268\n",
      "Epoch 128/500\n",
      "5639/5639 [==============================] - 3s 588us/sample - loss: 24.4813 - rmse: 4.8061 - val_loss: 11.6885 - val_rmse: 3.0658\n",
      "Epoch 129/500\n",
      "5639/5639 [==============================] - 3s 582us/sample - loss: 24.4460 - rmse: 4.7972 - val_loss: 11.9281 - val_rmse: 3.0923\n",
      "Epoch 130/500\n",
      "5639/5639 [==============================] - 3s 583us/sample - loss: 24.2394 - rmse: 4.7780 - val_loss: 11.0616 - val_rmse: 2.9699\n",
      "Epoch 131/500\n",
      "5639/5639 [==============================] - 3s 582us/sample - loss: 24.2756 - rmse: 4.7847 - val_loss: 11.0671 - val_rmse: 2.9638\n",
      "Epoch 132/500\n",
      "5639/5639 [==============================] - 3s 597us/sample - loss: 24.4157 - rmse: 4.7953 - val_loss: 9.9974 - val_rmse: 2.7939\n",
      "Epoch 133/500\n",
      "5639/5639 [==============================] - 3s 602us/sample - loss: 23.9148 - rmse: 4.7479 - val_loss: 11.0949 - val_rmse: 2.9725\n",
      "Epoch 134/500\n",
      "5639/5639 [==============================] - 3s 585us/sample - loss: 23.7035 - rmse: 4.7255 - val_loss: 10.4450 - val_rmse: 2.8668\n",
      "Epoch 135/500\n",
      "5639/5639 [==============================] - 3s 587us/sample - loss: 23.5567 - rmse: 4.7107 - val_loss: 11.0354 - val_rmse: 2.9565\n",
      "Epoch 136/500\n",
      "5639/5639 [==============================] - 3s 598us/sample - loss: 23.7117 - rmse: 4.7268 - val_loss: 9.7173 - val_rmse: 2.7495\n",
      "Epoch 137/500\n",
      "5639/5639 [==============================] - 3s 587us/sample - loss: 23.4949 - rmse: 4.7060 - val_loss: 11.8434 - val_rmse: 3.0790\n",
      "Epoch 138/500\n",
      "5639/5639 [==============================] - 3s 590us/sample - loss: 23.4183 - rmse: 4.7006 - val_loss: 12.2692 - val_rmse: 3.1331\n",
      "Epoch 139/500\n",
      "5639/5639 [==============================] - 3s 597us/sample - loss: 23.2155 - rmse: 4.6825 - val_loss: 9.5283 - val_rmse: 2.7155\n",
      "Epoch 140/500\n",
      "5639/5639 [==============================] - 3s 598us/sample - loss: 23.1586 - rmse: 4.6722 - val_loss: 9.4056 - val_rmse: 2.6972\n",
      "Epoch 141/500\n",
      "5639/5639 [==============================] - 3s 586us/sample - loss: 22.9346 - rmse: 4.6521 - val_loss: 9.7176 - val_rmse: 2.7517\n",
      "Epoch 142/500\n",
      "5639/5639 [==============================] - 3s 583us/sample - loss: 22.8543 - rmse: 4.6442 - val_loss: 11.1042 - val_rmse: 2.9730\n",
      "Epoch 143/500\n",
      "5639/5639 [==============================] - 3s 582us/sample - loss: 22.8873 - rmse: 4.6477 - val_loss: 10.1289 - val_rmse: 2.8215\n",
      "Epoch 144/500\n",
      "5639/5639 [==============================] - 3s 601us/sample - loss: 22.7374 - rmse: 4.6293 - val_loss: 9.2491 - val_rmse: 2.6707\n",
      "Epoch 145/500\n",
      "5639/5639 [==============================] - 3s 587us/sample - loss: 22.7893 - rmse: 4.6415 - val_loss: 10.3632 - val_rmse: 2.8689\n",
      "Epoch 146/500\n",
      "5639/5639 [==============================] - 3s 599us/sample - loss: 22.3278 - rmse: 4.5882 - val_loss: 9.0550 - val_rmse: 2.6405\n",
      "Epoch 147/500\n",
      "5639/5639 [==============================] - 3s 589us/sample - loss: 22.4213 - rmse: 4.6040 - val_loss: 9.9996 - val_rmse: 2.8023\n",
      "Epoch 148/500\n",
      "5639/5639 [==============================] - 3s 590us/sample - loss: 22.1157 - rmse: 4.5662 - val_loss: 9.4052 - val_rmse: 2.6993\n",
      "Epoch 149/500\n",
      "5639/5639 [==============================] - 3s 589us/sample - loss: 22.0534 - rmse: 4.5630 - val_loss: 10.3516 - val_rmse: 2.8596\n",
      "Epoch 150/500\n",
      "5639/5639 [==============================] - 3s 587us/sample - loss: 22.0461 - rmse: 4.5637 - val_loss: 9.4731 - val_rmse: 2.7162\n",
      "Epoch 151/500\n",
      "5639/5639 [==============================] - 3s 587us/sample - loss: 21.9445 - rmse: 4.5522 - val_loss: 10.3190 - val_rmse: 2.8612\n",
      "Epoch 152/500\n",
      "5639/5639 [==============================] - 3s 585us/sample - loss: 21.7397 - rmse: 4.5293 - val_loss: 9.6115 - val_rmse: 2.7415\n",
      "Epoch 153/500\n",
      "5639/5639 [==============================] - 3s 587us/sample - loss: 21.7039 - rmse: 4.5263 - val_loss: 9.3593 - val_rmse: 2.6959\n",
      "Epoch 154/500\n",
      "5639/5639 [==============================] - 3s 588us/sample - loss: 21.6062 - rmse: 4.5140 - val_loss: 9.7806 - val_rmse: 2.7728\n",
      "Epoch 155/500\n",
      "5639/5639 [==============================] - 3s 592us/sample - loss: 21.6504 - rmse: 4.5196 - val_loss: 9.3427 - val_rmse: 2.6964\n",
      "Epoch 156/500\n",
      "5639/5639 [==============================] - 3s 600us/sample - loss: 21.3886 - rmse: 4.4963 - val_loss: 8.9751 - val_rmse: 2.6295\n",
      "Epoch 157/500\n",
      "5639/5639 [==============================] - 3s 599us/sample - loss: 21.2065 - rmse: 4.4782 - val_loss: 8.7455 - val_rmse: 2.5883\n",
      "Epoch 158/500\n",
      "5639/5639 [==============================] - 3s 590us/sample - loss: 21.3148 - rmse: 4.4845 - val_loss: 11.3744 - val_rmse: 3.0342\n",
      "Epoch 159/500\n",
      "5639/5639 [==============================] - 3s 589us/sample - loss: 21.2839 - rmse: 4.4841 - val_loss: 10.4760 - val_rmse: 2.8846\n",
      "Epoch 160/500\n",
      "5639/5639 [==============================] - 3s 597us/sample - loss: 21.0110 - rmse: 4.4534 - val_loss: 8.6612 - val_rmse: 2.5779\n",
      "Epoch 161/500\n",
      "5639/5639 [==============================] - 3s 587us/sample - loss: 20.9644 - rmse: 4.4471 - val_loss: 9.1534 - val_rmse: 2.6626\n",
      "Epoch 162/500\n",
      "5639/5639 [==============================] - 3s 586us/sample - loss: 21.0200 - rmse: 4.4566 - val_loss: 9.4477 - val_rmse: 2.7228\n",
      "Epoch 163/500\n",
      "5639/5639 [==============================] - 3s 581us/sample - loss: 20.7410 - rmse: 4.4265 - val_loss: 9.4770 - val_rmse: 2.7260\n",
      "Epoch 164/500\n",
      "5639/5639 [==============================] - 3s 581us/sample - loss: 20.8440 - rmse: 4.4318 - val_loss: 8.8696 - val_rmse: 2.6106\n",
      "Epoch 165/500\n",
      "5639/5639 [==============================] - 3s 586us/sample - loss: 20.7557 - rmse: 4.4273 - val_loss: 11.6504 - val_rmse: 3.0854\n",
      "Epoch 166/500\n",
      "5639/5639 [==============================] - 3s 589us/sample - loss: 20.5789 - rmse: 4.4048 - val_loss: 9.0672 - val_rmse: 2.6546\n",
      "Epoch 167/500\n",
      "5639/5639 [==============================] - 3s 582us/sample - loss: 20.5580 - rmse: 4.4050 - val_loss: 9.0966 - val_rmse: 2.6630\n",
      "Epoch 168/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5639/5639 [==============================] - 3s 581us/sample - loss: 20.2418 - rmse: 4.3731 - val_loss: 9.0064 - val_rmse: 2.6479\n",
      "Epoch 169/500\n",
      "5639/5639 [==============================] - 3s 585us/sample - loss: 20.1559 - rmse: 4.3651 - val_loss: 9.9211 - val_rmse: 2.8039\n",
      "Epoch 170/500\n",
      "5639/5639 [==============================] - 3s 577us/sample - loss: 20.1194 - rmse: 4.3603 - val_loss: 8.8331 - val_rmse: 2.6147\n",
      "Epoch 171/500\n",
      "5639/5639 [==============================] - 3s 580us/sample - loss: 20.1386 - rmse: 4.3624 - val_loss: 9.9934 - val_rmse: 2.8151\n",
      "Epoch 172/500\n",
      "5639/5639 [==============================] - 3s 584us/sample - loss: 19.9580 - rmse: 4.3384 - val_loss: 10.7337 - val_rmse: 2.9465\n",
      "Epoch 173/500\n",
      "5639/5639 [==============================] - 3s 584us/sample - loss: 20.0737 - rmse: 4.3520 - val_loss: 9.4809 - val_rmse: 2.7341\n",
      "Epoch 174/500\n",
      "5639/5639 [==============================] - 3s 578us/sample - loss: 19.6704 - rmse: 4.3068 - val_loss: 9.2596 - val_rmse: 2.6954\n",
      "Epoch 175/500\n",
      "5639/5639 [==============================] - 3s 584us/sample - loss: 19.7404 - rmse: 4.3168 - val_loss: 8.9053 - val_rmse: 2.6307\n",
      "Epoch 176/500\n",
      "5639/5639 [==============================] - 3s 583us/sample - loss: 19.8066 - rmse: 4.3262 - val_loss: 9.8842 - val_rmse: 2.8048\n",
      "Epoch 177/500\n",
      "5639/5639 [==============================] - 3s 581us/sample - loss: 19.6148 - rmse: 4.3043 - val_loss: 8.6586 - val_rmse: 2.5841\n",
      "Epoch 178/500\n",
      "5639/5639 [==============================] - 3s 581us/sample - loss: 19.5944 - rmse: 4.3004 - val_loss: 9.3525 - val_rmse: 2.7075\n",
      "Epoch 179/500\n",
      "5639/5639 [==============================] - 3s 580us/sample - loss: 19.5353 - rmse: 4.2958 - val_loss: 8.7128 - val_rmse: 2.5861\n",
      "Epoch 180/500\n",
      "5639/5639 [==============================] - 3s 594us/sample - loss: 19.3942 - rmse: 4.2809 - val_loss: 8.3175 - val_rmse: 2.5268\n",
      "Epoch 181/500\n",
      "5639/5639 [==============================] - 3s 579us/sample - loss: 19.2957 - rmse: 4.2704 - val_loss: 8.7179 - val_rmse: 2.5975\n",
      "Epoch 182/500\n",
      "5639/5639 [==============================] - 3s 580us/sample - loss: 19.0961 - rmse: 4.2454 - val_loss: 10.0287 - val_rmse: 2.8272\n",
      "Epoch 183/500\n",
      "5639/5639 [==============================] - 3s 581us/sample - loss: 19.1534 - rmse: 4.2528 - val_loss: 9.0986 - val_rmse: 2.6712\n",
      "Epoch 184/500\n",
      "5639/5639 [==============================] - 3s 590us/sample - loss: 19.1030 - rmse: 4.2484 - val_loss: 9.0303 - val_rmse: 2.6569\n",
      "Epoch 185/500\n",
      "5639/5639 [==============================] - 3s 580us/sample - loss: 18.9059 - rmse: 4.2234 - val_loss: 9.2578 - val_rmse: 2.6950\n",
      "Epoch 186/500\n",
      "5639/5639 [==============================] - 3s 583us/sample - loss: 19.0722 - rmse: 4.2442 - val_loss: 8.3560 - val_rmse: 2.5354\n",
      "Epoch 187/500\n",
      "5639/5639 [==============================] - 3s 585us/sample - loss: 18.8857 - rmse: 4.2230 - val_loss: 8.6927 - val_rmse: 2.5962\n",
      "Epoch 188/500\n",
      "5639/5639 [==============================] - 3s 592us/sample - loss: 18.6124 - rmse: 4.1940 - val_loss: 8.3217 - val_rmse: 2.5264\n",
      "Epoch 189/500\n",
      "5639/5639 [==============================] - 3s 584us/sample - loss: 18.7751 - rmse: 4.2092 - val_loss: 9.6620 - val_rmse: 2.7691\n",
      "Epoch 190/500\n",
      "5639/5639 [==============================] - 3s 581us/sample - loss: 18.5917 - rmse: 4.1902 - val_loss: 9.0477 - val_rmse: 2.6598\n",
      "Epoch 191/500\n",
      "5639/5639 [==============================] - 3s 580us/sample - loss: 18.4119 - rmse: 4.1705 - val_loss: 9.4459 - val_rmse: 2.7126\n",
      "Epoch 192/500\n",
      "5639/5639 [==============================] - 3s 579us/sample - loss: 18.2963 - rmse: 4.1564 - val_loss: 9.8717 - val_rmse: 2.8080\n",
      "Epoch 193/500\n",
      "5639/5639 [==============================] - 3s 578us/sample - loss: 18.2366 - rmse: 4.1505 - val_loss: 9.7368 - val_rmse: 2.7861\n",
      "Epoch 194/500\n",
      "5639/5639 [==============================] - 3s 580us/sample - loss: 18.3659 - rmse: 4.1651 - val_loss: 9.7073 - val_rmse: 2.7773\n",
      "Epoch 195/500\n",
      "5639/5639 [==============================] - 3s 577us/sample - loss: 18.1320 - rmse: 4.1382 - val_loss: 9.3606 - val_rmse: 2.7187\n",
      "Epoch 196/500\n",
      "5639/5639 [==============================] - 3s 586us/sample - loss: 18.0005 - rmse: 4.1253 - val_loss: 9.5129 - val_rmse: 2.7300\n",
      "Epoch 197/500\n",
      "5639/5639 [==============================] - 3s 585us/sample - loss: 18.0550 - rmse: 4.1320 - val_loss: 8.6544 - val_rmse: 2.5942\n",
      "Epoch 198/500\n",
      "5639/5639 [==============================] - 3s 581us/sample - loss: 18.0852 - rmse: 4.1352 - val_loss: 8.9165 - val_rmse: 2.6394\n",
      "Epoch 199/500\n",
      "5639/5639 [==============================] - 3s 581us/sample - loss: 17.8181 - rmse: 4.1044 - val_loss: 9.1306 - val_rmse: 2.6794\n",
      "Epoch 200/500\n",
      "5639/5639 [==============================] - 3s 580us/sample - loss: 17.8441 - rmse: 4.1054 - val_loss: 8.7107 - val_rmse: 2.6002\n",
      "Epoch 201/500\n",
      "5639/5639 [==============================] - 3s 580us/sample - loss: 17.6971 - rmse: 4.0955 - val_loss: 9.2793 - val_rmse: 2.7044\n",
      "Epoch 202/500\n",
      "5639/5639 [==============================] - 3s 596us/sample - loss: 17.7680 - rmse: 4.0949 - val_loss: 8.1886 - val_rmse: 2.5114\n",
      "Epoch 203/500\n",
      "5639/5639 [==============================] - 3s 585us/sample - loss: 17.6056 - rmse: 4.0792 - val_loss: 9.5874 - val_rmse: 2.7595\n",
      "Epoch 204/500\n",
      "5639/5639 [==============================] - 3s 582us/sample - loss: 17.6480 - rmse: 4.0824 - val_loss: 10.2270 - val_rmse: 2.8594\n",
      "Epoch 205/500\n",
      "5639/5639 [==============================] - 3s 580us/sample - loss: 17.3216 - rmse: 4.0476 - val_loss: 8.9768 - val_rmse: 2.6514\n",
      "Epoch 206/500\n",
      "5639/5639 [==============================] - 3s 580us/sample - loss: 17.4705 - rmse: 4.0629 - val_loss: 10.6203 - val_rmse: 2.9366\n",
      "Epoch 207/500\n",
      "5639/5639 [==============================] - 3s 580us/sample - loss: 17.2737 - rmse: 4.0413 - val_loss: 8.4933 - val_rmse: 2.5676\n",
      "Epoch 208/500\n",
      "5639/5639 [==============================] - 3s 579us/sample - loss: 17.2206 - rmse: 4.0377 - val_loss: 9.6984 - val_rmse: 2.7854\n",
      "Epoch 209/500\n",
      "5639/5639 [==============================] - 3s 583us/sample - loss: 17.1195 - rmse: 4.0237 - val_loss: 9.1774 - val_rmse: 2.6922\n",
      "Epoch 210/500\n",
      "5639/5639 [==============================] - 3s 587us/sample - loss: 17.1049 - rmse: 4.0209 - val_loss: 9.4756 - val_rmse: 2.7397\n",
      "Epoch 211/500\n",
      "5639/5639 [==============================] - 3s 579us/sample - loss: 17.1703 - rmse: 4.0295 - val_loss: 10.2921 - val_rmse: 2.8844\n",
      "Epoch 212/500\n",
      "5639/5639 [==============================] - 3s 585us/sample - loss: 16.9060 - rmse: 3.9953 - val_loss: 8.3185 - val_rmse: 2.5385\n",
      "Epoch 213/500\n",
      "5639/5639 [==============================] - 3s 594us/sample - loss: 16.8766 - rmse: 3.9961 - val_loss: 8.1153 - val_rmse: 2.5013\n",
      "Epoch 214/500\n",
      "5639/5639 [==============================] - 3s 584us/sample - loss: 16.8713 - rmse: 3.9941 - val_loss: 8.4456 - val_rmse: 2.5568\n",
      "Epoch 215/500\n",
      "5639/5639 [==============================] - 3s 588us/sample - loss: 16.7455 - rmse: 3.9794 - val_loss: 8.5786 - val_rmse: 2.5854\n",
      "Epoch 216/500\n",
      "5639/5639 [==============================] - 3s 593us/sample - loss: 16.6786 - rmse: 3.9747 - val_loss: 8.0967 - val_rmse: 2.4872\n",
      "Epoch 217/500\n",
      "5639/5639 [==============================] - 3s 582us/sample - loss: 16.6449 - rmse: 3.9741 - val_loss: 9.4103 - val_rmse: 2.7354\n",
      "Epoch 218/500\n",
      "5639/5639 [==============================] - 3s 577us/sample - loss: 16.5772 - rmse: 3.9560 - val_loss: 8.2450 - val_rmse: 2.5273\n",
      "Epoch 219/500\n",
      "5639/5639 [==============================] - 3s 582us/sample - loss: 16.4292 - rmse: 3.9434 - val_loss: 9.3016 - val_rmse: 2.7135\n",
      "Epoch 220/500\n",
      "5639/5639 [==============================] - 3s 577us/sample - loss: 16.4141 - rmse: 3.9410 - val_loss: 8.0901 - val_rmse: 2.4984\n",
      "Epoch 221/500\n",
      "5639/5639 [==============================] - 3s 588us/sample - loss: 16.4019 - rmse: 3.9402 - val_loss: 8.6458 - val_rmse: 2.6008\n",
      "Epoch 222/500\n",
      "5639/5639 [==============================] - 3s 577us/sample - loss: 16.2730 - rmse: 3.9259 - val_loss: 9.7218 - val_rmse: 2.7893\n",
      "Epoch 223/500\n",
      "5639/5639 [==============================] - 3s 581us/sample - loss: 16.1956 - rmse: 3.9152 - val_loss: 8.2315 - val_rmse: 2.5203\n",
      "Epoch 224/500\n",
      "5639/5639 [==============================] - 3s 581us/sample - loss: 16.2318 - rmse: 3.9163 - val_loss: 8.0606 - val_rmse: 2.4909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 225/500\n",
      "5639/5639 [==============================] - 3s 576us/sample - loss: 16.0279 - rmse: 3.8947 - val_loss: 8.6577 - val_rmse: 2.6031\n",
      "Epoch 226/500\n",
      "5639/5639 [==============================] - 3s 574us/sample - loss: 16.0286 - rmse: 3.8941 - val_loss: 8.7667 - val_rmse: 2.6227\n",
      "Epoch 227/500\n",
      "5639/5639 [==============================] - 3s 578us/sample - loss: 15.8929 - rmse: 3.8781 - val_loss: 9.3034 - val_rmse: 2.7180\n",
      "Epoch 228/500\n",
      "5639/5639 [==============================] - 3s 581us/sample - loss: 15.9078 - rmse: 3.8776 - val_loss: 8.4925 - val_rmse: 2.5720\n",
      "Epoch 229/500\n",
      "5639/5639 [==============================] - 3s 585us/sample - loss: 15.8988 - rmse: 3.8782 - val_loss: 7.8326 - val_rmse: 2.4469\n",
      "Epoch 230/500\n",
      "5639/5639 [==============================] - 3s 580us/sample - loss: 15.8695 - rmse: 3.8753 - val_loss: 7.8875 - val_rmse: 2.4597\n",
      "Epoch 231/500\n",
      "5639/5639 [==============================] - 3s 578us/sample - loss: 15.7384 - rmse: 3.8604 - val_loss: 8.8064 - val_rmse: 2.6289\n",
      "Epoch 232/500\n",
      "5639/5639 [==============================] - 3s 578us/sample - loss: 15.5774 - rmse: 3.8362 - val_loss: 8.4550 - val_rmse: 2.5696\n",
      "Epoch 233/500\n",
      "5639/5639 [==============================] - 3s 575us/sample - loss: 15.6271 - rmse: 3.8486 - val_loss: 8.2157 - val_rmse: 2.5254\n",
      "Epoch 234/500\n",
      "5639/5639 [==============================] - 3s 583us/sample - loss: 15.4596 - rmse: 3.8255 - val_loss: 7.9092 - val_rmse: 2.4718\n",
      "Epoch 235/500\n",
      "5639/5639 [==============================] - 3s 579us/sample - loss: 15.4359 - rmse: 3.8237 - val_loss: 8.5614 - val_rmse: 2.5883\n",
      "Epoch 236/500\n",
      "5639/5639 [==============================] - 3s 578us/sample - loss: 15.4357 - rmse: 3.8220 - val_loss: 8.6410 - val_rmse: 2.6111\n",
      "Epoch 237/500\n",
      "5639/5639 [==============================] - 3s 581us/sample - loss: 15.3613 - rmse: 3.8147 - val_loss: 9.2666 - val_rmse: 2.7117\n",
      "Epoch 238/500\n",
      "5639/5639 [==============================] - 3s 577us/sample - loss: 15.3408 - rmse: 3.8144 - val_loss: 8.6939 - val_rmse: 2.6133\n",
      "Epoch 239/500\n",
      "5639/5639 [==============================] - 3s 575us/sample - loss: 15.1043 - rmse: 3.7849 - val_loss: 9.2359 - val_rmse: 2.7081\n",
      "Epoch 240/500\n",
      "5639/5639 [==============================] - 3s 577us/sample - loss: 15.1661 - rmse: 3.7886 - val_loss: 7.8587 - val_rmse: 2.4574\n",
      "Epoch 241/500\n",
      "5639/5639 [==============================] - 3s 576us/sample - loss: 15.0783 - rmse: 3.7768 - val_loss: 9.2109 - val_rmse: 2.6994\n",
      "Epoch 242/500\n",
      "5639/5639 [==============================] - 3s 577us/sample - loss: 15.0039 - rmse: 3.7718 - val_loss: 8.6068 - val_rmse: 2.5930\n",
      "Epoch 243/500\n",
      "5639/5639 [==============================] - 3s 581us/sample - loss: 15.0586 - rmse: 3.7698 - val_loss: 8.4027 - val_rmse: 2.5546\n",
      "Epoch 244/500\n",
      "5639/5639 [==============================] - 3s 575us/sample - loss: 14.8499 - rmse: 3.7512 - val_loss: 8.5689 - val_rmse: 2.5897\n",
      "Epoch 245/500\n",
      "5639/5639 [==============================] - 3s 593us/sample - loss: 14.8062 - rmse: 3.7443 - val_loss: 7.7741 - val_rmse: 2.4432\n",
      "Epoch 246/500\n",
      "5639/5639 [==============================] - 3s 580us/sample - loss: 14.8030 - rmse: 3.7456 - val_loss: 8.4938 - val_rmse: 2.5730\n",
      "Epoch 247/500\n",
      "5639/5639 [==============================] - 3s 578us/sample - loss: 14.8030 - rmse: 3.7462 - val_loss: 8.8656 - val_rmse: 2.6388\n",
      "Epoch 248/500\n",
      "5639/5639 [==============================] - 3s 578us/sample - loss: 14.6428 - rmse: 3.7226 - val_loss: 7.8792 - val_rmse: 2.4652\n",
      "Epoch 249/500\n",
      "5639/5639 [==============================] - 3s 577us/sample - loss: 14.7418 - rmse: 3.7364 - val_loss: 9.2967 - val_rmse: 2.7219\n",
      "Epoch 250/500\n",
      "5639/5639 [==============================] - 3s 578us/sample - loss: 14.5677 - rmse: 3.7174 - val_loss: 9.7586 - val_rmse: 2.8024\n",
      "Epoch 251/500\n",
      "5639/5639 [==============================] - 3s 579us/sample - loss: 14.5482 - rmse: 3.7147 - val_loss: 7.8907 - val_rmse: 2.4652\n",
      "Epoch 252/500\n",
      "5639/5639 [==============================] - 3s 579us/sample - loss: 14.5594 - rmse: 3.7152 - val_loss: 7.8935 - val_rmse: 2.4660\n",
      "Epoch 253/500\n",
      "5639/5639 [==============================] - 3s 577us/sample - loss: 14.4284 - rmse: 3.6973 - val_loss: 7.9858 - val_rmse: 2.4871\n",
      "Epoch 254/500\n",
      "5639/5639 [==============================] - 3s 584us/sample - loss: 14.3019 - rmse: 3.6829 - val_loss: 9.2120 - val_rmse: 2.6895\n",
      "Epoch 255/500\n",
      "5639/5639 [==============================] - 3s 576us/sample - loss: 14.3417 - rmse: 3.6868 - val_loss: 8.1865 - val_rmse: 2.5211\n",
      "Epoch 256/500\n",
      "5639/5639 [==============================] - 3s 586us/sample - loss: 14.2655 - rmse: 3.6765 - val_loss: 8.5243 - val_rmse: 2.5812\n",
      "Epoch 257/500\n",
      "5639/5639 [==============================] - 3s 590us/sample - loss: 14.1761 - rmse: 3.6653 - val_loss: 8.0025 - val_rmse: 2.4831\n",
      "Epoch 258/500\n",
      "5639/5639 [==============================] - 3s 583us/sample - loss: 14.1556 - rmse: 3.6598 - val_loss: 8.1763 - val_rmse: 2.5216\n",
      "Epoch 259/500\n",
      "5639/5639 [==============================] - 3s 578us/sample - loss: 14.0263 - rmse: 3.6483 - val_loss: 9.5732 - val_rmse: 2.7729\n",
      "Epoch 260/500\n",
      "5639/5639 [==============================] - 3s 578us/sample - loss: 13.9283 - rmse: 3.6340 - val_loss: 8.0021 - val_rmse: 2.4868\n",
      "Epoch 261/500\n",
      "5639/5639 [==============================] - 3s 575us/sample - loss: 13.9333 - rmse: 3.6337 - val_loss: 10.3597 - val_rmse: 2.8852\n",
      "Epoch 262/500\n",
      "5639/5639 [==============================] - 3s 578us/sample - loss: 13.9147 - rmse: 3.6306 - val_loss: 8.7130 - val_rmse: 2.6205\n",
      "Epoch 263/500\n",
      "5639/5639 [==============================] - 3s 578us/sample - loss: 13.8315 - rmse: 3.6195 - val_loss: 7.7936 - val_rmse: 2.4546\n",
      "Epoch 264/500\n",
      "5639/5639 [==============================] - 3s 579us/sample - loss: 13.7935 - rmse: 3.6194 - val_loss: 7.8055 - val_rmse: 2.4590\n",
      "Epoch 265/500\n",
      "5639/5639 [==============================] - 3s 579us/sample - loss: 13.6865 - rmse: 3.6028 - val_loss: 7.8474 - val_rmse: 2.4648\n",
      "Epoch 266/500\n",
      "5639/5639 [==============================] - 3s 578us/sample - loss: 13.4846 - rmse: 3.5781 - val_loss: 10.5480 - val_rmse: 2.9245\n",
      "Epoch 267/500\n",
      "5639/5639 [==============================] - 3s 581us/sample - loss: 13.7293 - rmse: 3.6086 - val_loss: 8.2357 - val_rmse: 2.5302\n",
      "Epoch 268/500\n",
      "5639/5639 [==============================] - 3s 578us/sample - loss: 13.6558 - rmse: 3.6005 - val_loss: 10.2381 - val_rmse: 2.8842\n",
      "Epoch 269/500\n",
      "5639/5639 [==============================] - 3s 578us/sample - loss: 13.5914 - rmse: 3.5911 - val_loss: 7.9340 - val_rmse: 2.4805\n",
      "Epoch 270/500\n",
      "5639/5639 [==============================] - 3s 583us/sample - loss: 13.4781 - rmse: 3.5751 - val_loss: 8.4758 - val_rmse: 2.5829\n",
      "Epoch 271/500\n",
      "5639/5639 [==============================] - 3s 578us/sample - loss: 13.4466 - rmse: 3.5712 - val_loss: 8.0506 - val_rmse: 2.4952\n",
      "Epoch 272/500\n",
      "5639/5639 [==============================] - 3s 579us/sample - loss: 13.4618 - rmse: 3.5717 - val_loss: 7.7541 - val_rmse: 2.4456\n",
      "Epoch 273/500\n",
      "5639/5639 [==============================] - 3s 580us/sample - loss: 13.2586 - rmse: 3.5467 - val_loss: 8.2261 - val_rmse: 2.5314\n",
      "Epoch 274/500\n",
      "5639/5639 [==============================] - 3s 595us/sample - loss: 13.4019 - rmse: 3.5661 - val_loss: 7.6627 - val_rmse: 2.4239\n",
      "Epoch 275/500\n",
      "5639/5639 [==============================] - 3s 582us/sample - loss: 13.2121 - rmse: 3.5398 - val_loss: 8.3983 - val_rmse: 2.5708\n",
      "Epoch 276/500\n",
      "5639/5639 [==============================] - 3s 579us/sample - loss: 13.2449 - rmse: 3.5430 - val_loss: 8.7453 - val_rmse: 2.6289\n",
      "Epoch 277/500\n",
      "5639/5639 [==============================] - 3s 581us/sample - loss: 13.1493 - rmse: 3.5323 - val_loss: 9.0365 - val_rmse: 2.6815\n",
      "Epoch 278/500\n",
      "5639/5639 [==============================] - 3s 581us/sample - loss: 13.1441 - rmse: 3.5303 - val_loss: 8.3321 - val_rmse: 2.5484\n",
      "Epoch 279/500\n",
      "5639/5639 [==============================] - 3s 578us/sample - loss: 12.9588 - rmse: 3.5087 - val_loss: 8.3495 - val_rmse: 2.5567\n",
      "Epoch 280/500\n",
      "5639/5639 [==============================] - 3s 576us/sample - loss: 12.9347 - rmse: 3.5025 - val_loss: 8.7298 - val_rmse: 2.6261\n",
      "Epoch 281/500\n",
      "5639/5639 [==============================] - 3s 582us/sample - loss: 12.9110 - rmse: 3.5002 - val_loss: 7.8364 - val_rmse: 2.4635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 282/500\n",
      "5639/5639 [==============================] - 3s 578us/sample - loss: 12.8230 - rmse: 3.4884 - val_loss: 7.8607 - val_rmse: 2.4708\n",
      "Epoch 283/500\n",
      "5639/5639 [==============================] - 3s 575us/sample - loss: 12.8831 - rmse: 3.4969 - val_loss: 9.7895 - val_rmse: 2.8097\n",
      "Epoch 284/500\n",
      "5639/5639 [==============================] - 3s 576us/sample - loss: 12.8416 - rmse: 3.4879 - val_loss: 8.3506 - val_rmse: 2.5548\n",
      "Epoch 285/500\n",
      "5639/5639 [==============================] - 3s 579us/sample - loss: 12.7339 - rmse: 3.4767 - val_loss: 8.8625 - val_rmse: 2.6483\n",
      "Epoch 286/500\n",
      "5639/5639 [==============================] - 3s 575us/sample - loss: 12.7009 - rmse: 3.4720 - val_loss: 7.9739 - val_rmse: 2.4937\n",
      "Epoch 287/500\n",
      "5639/5639 [==============================] - 3s 576us/sample - loss: 12.6272 - rmse: 3.4644 - val_loss: 8.0609 - val_rmse: 2.5090\n",
      "Epoch 288/500\n",
      "5639/5639 [==============================] - 3s 576us/sample - loss: 12.6469 - rmse: 3.4649 - val_loss: 7.8048 - val_rmse: 2.4617\n",
      "Epoch 289/500\n",
      "5639/5639 [==============================] - 3s 578us/sample - loss: 12.6425 - rmse: 3.4656 - val_loss: 9.0548 - val_rmse: 2.6815\n",
      "Epoch 290/500\n",
      "5639/5639 [==============================] - 3s 579us/sample - loss: 12.6324 - rmse: 3.4630 - val_loss: 9.4945 - val_rmse: 2.7631\n",
      "Epoch 291/500\n",
      "5639/5639 [==============================] - 3s 590us/sample - loss: 12.4676 - rmse: 3.4403 - val_loss: 7.5849 - val_rmse: 2.4172\n",
      "Epoch 292/500\n",
      "5639/5639 [==============================] - 3s 578us/sample - loss: 12.3488 - rmse: 3.4244 - val_loss: 8.2809 - val_rmse: 2.5411\n",
      "Epoch 293/500\n",
      "5639/5639 [==============================] - 3s 579us/sample - loss: 12.4514 - rmse: 3.4392 - val_loss: 9.1308 - val_rmse: 2.6991\n",
      "Epoch 294/500\n",
      "5639/5639 [==============================] - 3s 585us/sample - loss: 12.4007 - rmse: 3.4298 - val_loss: 8.4337 - val_rmse: 2.5733\n",
      "Epoch 295/500\n",
      "5639/5639 [==============================] - 3s 601us/sample - loss: 12.2316 - rmse: 3.4079 - val_loss: 7.5512 - val_rmse: 2.4113\n",
      "Epoch 296/500\n",
      "5639/5639 [==============================] - 3s 580us/sample - loss: 12.3055 - rmse: 3.4181 - val_loss: 7.7005 - val_rmse: 2.4383\n",
      "Epoch 297/500\n",
      "5639/5639 [==============================] - 3s 578us/sample - loss: 12.1990 - rmse: 3.4028 - val_loss: 9.2906 - val_rmse: 2.7186\n",
      "Epoch 298/500\n",
      "5639/5639 [==============================] - 3s 577us/sample - loss: 12.1535 - rmse: 3.3973 - val_loss: 7.6373 - val_rmse: 2.4262\n",
      "Epoch 299/500\n",
      "5639/5639 [==============================] - 3s 577us/sample - loss: 12.0739 - rmse: 3.3891 - val_loss: 8.1248 - val_rmse: 2.5195\n",
      "Epoch 300/500\n",
      "5639/5639 [==============================] - 3s 578us/sample - loss: 12.0840 - rmse: 3.3868 - val_loss: 8.8879 - val_rmse: 2.6564\n",
      "Epoch 301/500\n",
      "5639/5639 [==============================] - 3s 579us/sample - loss: 12.1153 - rmse: 3.3918 - val_loss: 8.1487 - val_rmse: 2.5293\n",
      "Epoch 302/500\n",
      "5639/5639 [==============================] - 3s 577us/sample - loss: 11.9021 - rmse: 3.3620 - val_loss: 7.7887 - val_rmse: 2.4542\n",
      "Epoch 303/500\n",
      "5639/5639 [==============================] - 3s 581us/sample - loss: 11.8910 - rmse: 3.3636 - val_loss: 8.6637 - val_rmse: 2.6201\n",
      "Epoch 304/500\n",
      "5639/5639 [==============================] - 3s 578us/sample - loss: 11.8477 - rmse: 3.3574 - val_loss: 8.1845 - val_rmse: 2.5239\n",
      "Epoch 305/500\n",
      "5639/5639 [==============================] - 3s 577us/sample - loss: 11.8338 - rmse: 3.3545 - val_loss: 8.6820 - val_rmse: 2.6152\n",
      "Epoch 306/500\n",
      "5639/5639 [==============================] - 3s 581us/sample - loss: 11.8064 - rmse: 3.3480 - val_loss: 7.6066 - val_rmse: 2.4225\n",
      "Epoch 307/500\n",
      "5639/5639 [==============================] - 3s 580us/sample - loss: 11.7469 - rmse: 3.3382 - val_loss: 7.9538 - val_rmse: 2.4863\n",
      "Epoch 308/500\n",
      "5639/5639 [==============================] - 3s 575us/sample - loss: 11.7374 - rmse: 3.3407 - val_loss: 8.5051 - val_rmse: 2.5940\n",
      "Epoch 309/500\n",
      "5639/5639 [==============================] - 3s 580us/sample - loss: 11.6670 - rmse: 3.3311 - val_loss: 7.5484 - val_rmse: 2.4121\n",
      "Epoch 310/500\n",
      "5639/5639 [==============================] - 3s 589us/sample - loss: 11.5806 - rmse: 3.3187 - val_loss: 7.4909 - val_rmse: 2.4005\n",
      "Epoch 311/500\n",
      "5639/5639 [==============================] - 3s 580us/sample - loss: 11.5729 - rmse: 3.3170 - val_loss: 9.9366 - val_rmse: 2.8402\n",
      "Epoch 312/500\n",
      "5639/5639 [==============================] - 3s 580us/sample - loss: 11.6381 - rmse: 3.3234 - val_loss: 8.1117 - val_rmse: 2.5067\n",
      "Epoch 313/500\n",
      "5639/5639 [==============================] - 3s 581us/sample - loss: 11.5394 - rmse: 3.3093 - val_loss: 7.9539 - val_rmse: 2.4835\n",
      "Epoch 314/500\n",
      "5639/5639 [==============================] - 3s 579us/sample - loss: 11.4046 - rmse: 3.2937 - val_loss: 8.0017 - val_rmse: 2.4969\n",
      "Epoch 315/500\n",
      "5639/5639 [==============================] - 3s 584us/sample - loss: 11.4541 - rmse: 3.2993 - val_loss: 7.5262 - val_rmse: 2.4104\n",
      "Epoch 316/500\n",
      "5639/5639 [==============================] - 4s 628us/sample - loss: 11.3496 - rmse: 3.2837 - val_loss: 7.9621 - val_rmse: 2.4899\n",
      "Epoch 317/500\n",
      "5639/5639 [==============================] - 3s 585us/sample - loss: 11.3412 - rmse: 3.2827 - val_loss: 8.2024 - val_rmse: 2.5328\n",
      "Epoch 318/500\n",
      "5639/5639 [==============================] - 3s 597us/sample - loss: 11.3431 - rmse: 3.2845 - val_loss: 7.6252 - val_rmse: 2.4299\n",
      "Epoch 319/500\n",
      "5639/5639 [==============================] - 3s 583us/sample - loss: 11.3890 - rmse: 3.2888 - val_loss: 8.3149 - val_rmse: 2.5587\n",
      "Epoch 320/500\n",
      "5639/5639 [==============================] - 3s 581us/sample - loss: 11.2230 - rmse: 3.2635 - val_loss: 7.6167 - val_rmse: 2.4228\n",
      "Epoch 321/500\n",
      "5639/5639 [==============================] - 3s 581us/sample - loss: 11.2228 - rmse: 3.2667 - val_loss: 7.5425 - val_rmse: 2.4137\n",
      "Epoch 322/500\n",
      "5639/5639 [==============================] - 3s 578us/sample - loss: 11.2462 - rmse: 3.2669 - val_loss: 10.1877 - val_rmse: 2.8815\n",
      "Epoch 323/500\n",
      "5639/5639 [==============================] - 3s 580us/sample - loss: 11.2450 - rmse: 3.2659 - val_loss: 7.9032 - val_rmse: 2.4765\n",
      "Epoch 324/500\n",
      "5639/5639 [==============================] - 3s 580us/sample - loss: 11.1575 - rmse: 3.2564 - val_loss: 7.9476 - val_rmse: 2.4897\n",
      "Epoch 325/500\n",
      "5639/5639 [==============================] - 3s 579us/sample - loss: 11.0046 - rmse: 3.2365 - val_loss: 9.2234 - val_rmse: 2.7139\n",
      "Epoch 326/500\n",
      "5639/5639 [==============================] - 3s 580us/sample - loss: 10.9395 - rmse: 3.2245 - val_loss: 8.1964 - val_rmse: 2.5334\n",
      "Epoch 327/500\n",
      "5639/5639 [==============================] - 3s 579us/sample - loss: 10.9201 - rmse: 3.2204 - val_loss: 7.6194 - val_rmse: 2.4255\n",
      "Epoch 328/500\n",
      "5639/5639 [==============================] - 3s 579us/sample - loss: 10.9414 - rmse: 3.2234 - val_loss: 7.5386 - val_rmse: 2.4087\n",
      "Epoch 329/500\n",
      "5639/5639 [==============================] - 3s 580us/sample - loss: 10.9348 - rmse: 3.2219 - val_loss: 7.8455 - val_rmse: 2.4652\n",
      "Epoch 330/500\n",
      "5639/5639 [==============================] - 3s 579us/sample - loss: 10.8142 - rmse: 3.2052 - val_loss: 7.5041 - val_rmse: 2.4011\n",
      "Epoch 331/500\n",
      "5639/5639 [==============================] - 3s 581us/sample - loss: 10.8023 - rmse: 3.2081 - val_loss: 7.5283 - val_rmse: 2.4065\n",
      "Epoch 332/500\n",
      "5639/5639 [==============================] - 3s 581us/sample - loss: 10.8282 - rmse: 3.2090 - val_loss: 8.0350 - val_rmse: 2.5044\n",
      "Epoch 333/500\n",
      "5639/5639 [==============================] - 3s 596us/sample - loss: 10.8196 - rmse: 3.2053 - val_loss: 7.4635 - val_rmse: 2.3973\n",
      "Epoch 334/500\n",
      "5639/5639 [==============================] - 3s 585us/sample - loss: 10.6279 - rmse: 3.1798 - val_loss: 9.1902 - val_rmse: 2.7117\n",
      "Epoch 335/500\n",
      "5639/5639 [==============================] - 3s 581us/sample - loss: 10.6220 - rmse: 3.1766 - val_loss: 7.7446 - val_rmse: 2.4546\n",
      "Epoch 336/500\n",
      "5639/5639 [==============================] - 3s 586us/sample - loss: 10.6748 - rmse: 3.1838 - val_loss: 9.4632 - val_rmse: 2.7562\n",
      "Epoch 337/500\n",
      "5639/5639 [==============================] - 3s 580us/sample - loss: 10.6119 - rmse: 3.1758 - val_loss: 9.7039 - val_rmse: 2.8076\n",
      "Epoch 338/500\n",
      "5639/5639 [==============================] - 3s 579us/sample - loss: 10.7121 - rmse: 3.1874 - val_loss: 11.4166 - val_rmse: 3.0801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 339/500\n",
      "5639/5639 [==============================] - 3s 575us/sample - loss: 10.6630 - rmse: 3.1803 - val_loss: 9.4363 - val_rmse: 2.7589\n",
      "Epoch 340/500\n",
      "5639/5639 [==============================] - 3s 577us/sample - loss: 10.3976 - rmse: 3.1428 - val_loss: 7.4881 - val_rmse: 2.4052\n",
      "Epoch 341/500\n",
      "5639/5639 [==============================] - 3s 577us/sample - loss: 10.4427 - rmse: 3.1546 - val_loss: 7.6664 - val_rmse: 2.4371\n",
      "Epoch 342/500\n",
      "5639/5639 [==============================] - 3s 589us/sample - loss: 10.3419 - rmse: 3.1369 - val_loss: 7.3807 - val_rmse: 2.3882\n",
      "Epoch 343/500\n",
      "5639/5639 [==============================] - 3s 577us/sample - loss: 10.3589 - rmse: 3.1376 - val_loss: 8.0990 - val_rmse: 2.5249\n",
      "Epoch 344/500\n",
      "5639/5639 [==============================] - 3s 575us/sample - loss: 10.4691 - rmse: 3.1517 - val_loss: 7.5006 - val_rmse: 2.4093\n",
      "Epoch 345/500\n",
      "5639/5639 [==============================] - 3s 577us/sample - loss: 10.2426 - rmse: 3.1201 - val_loss: 9.1205 - val_rmse: 2.7016\n",
      "Epoch 346/500\n",
      "5639/5639 [==============================] - 3s 575us/sample - loss: 10.4233 - rmse: 3.1429 - val_loss: 8.7644 - val_rmse: 2.6428\n",
      "Epoch 347/500\n",
      "5639/5639 [==============================] - 3s 578us/sample - loss: 10.3756 - rmse: 3.1397 - val_loss: 7.4513 - val_rmse: 2.4000\n",
      "Epoch 348/500\n",
      "5639/5639 [==============================] - 3s 578us/sample - loss: 10.1708 - rmse: 3.1097 - val_loss: 9.5837 - val_rmse: 2.7887\n",
      "Epoch 349/500\n",
      "5639/5639 [==============================] - 3s 592us/sample - loss: 10.2341 - rmse: 3.1188 - val_loss: 7.8288 - val_rmse: 2.4712\n",
      "Epoch 350/500\n",
      "5639/5639 [==============================] - 3s 591us/sample - loss: 10.1414 - rmse: 3.1065 - val_loss: 8.9732 - val_rmse: 2.6827\n",
      "Epoch 351/500\n",
      "5639/5639 [==============================] - 3s 577us/sample - loss: 10.1346 - rmse: 3.1050 - val_loss: 7.7069 - val_rmse: 2.4506\n",
      "Epoch 352/500\n",
      "5639/5639 [==============================] - 3s 585us/sample - loss: 10.1048 - rmse: 3.0978 - val_loss: 9.7568 - val_rmse: 2.8152\n",
      "Epoch 353/500\n",
      "5639/5639 [==============================] - 3s 578us/sample - loss: 10.1923 - rmse: 3.1110 - val_loss: 9.1554 - val_rmse: 2.7111\n",
      "Epoch 354/500\n",
      "5639/5639 [==============================] - 3s 579us/sample - loss: 10.1221 - rmse: 3.1020 - val_loss: 8.0011 - val_rmse: 2.5016\n",
      "Epoch 355/500\n",
      "5639/5639 [==============================] - 3s 578us/sample - loss: 10.0508 - rmse: 3.0948 - val_loss: 7.4430 - val_rmse: 2.3948\n",
      "Epoch 356/500\n",
      "5639/5639 [==============================] - 3s 581us/sample - loss: 9.9645 - rmse: 3.0762 - val_loss: 8.0310 - val_rmse: 2.5130\n",
      "Epoch 357/500\n",
      "5639/5639 [==============================] - 3s 592us/sample - loss: 9.9353 - rmse: 3.0726 - val_loss: 7.3582 - val_rmse: 2.3821\n",
      "Epoch 358/500\n",
      "5639/5639 [==============================] - 3s 578us/sample - loss: 9.9428 - rmse: 3.0724 - val_loss: 7.7244 - val_rmse: 2.4566\n",
      "Epoch 359/500\n",
      "5639/5639 [==============================] - 3s 578us/sample - loss: 9.8744 - rmse: 3.0656 - val_loss: 7.8758 - val_rmse: 2.4779\n",
      "Epoch 360/500\n",
      "5639/5639 [==============================] - 3s 578us/sample - loss: 9.8147 - rmse: 3.0573 - val_loss: 7.7090 - val_rmse: 2.4482\n",
      "Epoch 361/500\n",
      "5639/5639 [==============================] - 3s 578us/sample - loss: 9.8404 - rmse: 3.0589 - val_loss: 9.1723 - val_rmse: 2.7084\n",
      "Epoch 362/500\n",
      "5639/5639 [==============================] - 3s 581us/sample - loss: 9.7756 - rmse: 3.0518 - val_loss: 8.3994 - val_rmse: 2.5759\n",
      "Epoch 363/500\n",
      "5639/5639 [==============================] - 3s 577us/sample - loss: 9.7121 - rmse: 3.0386 - val_loss: 7.5428 - val_rmse: 2.4169\n",
      "Epoch 364/500\n",
      "5639/5639 [==============================] - 3s 576us/sample - loss: 9.6966 - rmse: 3.0382 - val_loss: 7.5814 - val_rmse: 2.4237\n",
      "Epoch 365/500\n",
      "5639/5639 [==============================] - 3s 577us/sample - loss: 9.6787 - rmse: 3.0381 - val_loss: 7.9536 - val_rmse: 2.4872\n",
      "Epoch 366/500\n",
      "5639/5639 [==============================] - 3s 576us/sample - loss: 9.7113 - rmse: 3.0378 - val_loss: 8.7172 - val_rmse: 2.6316\n",
      "Epoch 367/500\n",
      "5639/5639 [==============================] - 3s 585us/sample - loss: 9.6534 - rmse: 3.0303 - val_loss: 8.6074 - val_rmse: 2.6067\n",
      "Epoch 368/500\n",
      "5639/5639 [==============================] - 3s 581us/sample - loss: 9.5682 - rmse: 3.0159 - val_loss: 7.4391 - val_rmse: 2.3991\n",
      "Epoch 369/500\n",
      "5639/5639 [==============================] - 3s 580us/sample - loss: 9.6567 - rmse: 3.0309 - val_loss: 9.8723 - val_rmse: 2.8318\n",
      "Epoch 370/500\n",
      "5639/5639 [==============================] - 3s 578us/sample - loss: 9.6011 - rmse: 3.0213 - val_loss: 11.4871 - val_rmse: 3.0893\n",
      "Epoch 371/500\n",
      "5639/5639 [==============================] - 3s 582us/sample - loss: 9.6805 - rmse: 3.0323 - val_loss: 8.1414 - val_rmse: 2.5250\n",
      "Epoch 372/500\n",
      "5639/5639 [==============================] - 3s 583us/sample - loss: 9.5298 - rmse: 3.0073 - val_loss: 7.9665 - val_rmse: 2.4940\n",
      "Epoch 373/500\n",
      "5639/5639 [==============================] - 3s 581us/sample - loss: 9.4847 - rmse: 3.0009 - val_loss: 8.7509 - val_rmse: 2.6395\n",
      "Epoch 374/500\n",
      "5639/5639 [==============================] - 3s 593us/sample - loss: 9.5438 - rmse: 3.0104 - val_loss: 7.3717 - val_rmse: 2.3809\n",
      "Epoch 375/500\n",
      "5639/5639 [==============================] - 3s 580us/sample - loss: 9.3790 - rmse: 2.9883 - val_loss: 7.7274 - val_rmse: 2.4480\n",
      "Epoch 376/500\n",
      "5639/5639 [==============================] - 3s 577us/sample - loss: 9.4433 - rmse: 2.9943 - val_loss: 7.4052 - val_rmse: 2.3884\n",
      "Epoch 377/500\n",
      "5639/5639 [==============================] - 3s 578us/sample - loss: 9.3005 - rmse: 2.9769 - val_loss: 8.0508 - val_rmse: 2.5079\n",
      "Epoch 378/500\n",
      "5639/5639 [==============================] - 3s 578us/sample - loss: 9.3271 - rmse: 2.9773 - val_loss: 7.8009 - val_rmse: 2.4632\n",
      "Epoch 379/500\n",
      "5639/5639 [==============================] - 3s 576us/sample - loss: 9.4203 - rmse: 2.9919 - val_loss: 7.6079 - val_rmse: 2.4277\n",
      "Epoch 380/500\n",
      "5639/5639 [==============================] - 3s 578us/sample - loss: 9.2497 - rmse: 2.9683 - val_loss: 9.4082 - val_rmse: 2.7462\n",
      "Epoch 381/500\n",
      "5639/5639 [==============================] - 3s 581us/sample - loss: 9.2484 - rmse: 2.9646 - val_loss: 9.1177 - val_rmse: 2.6956\n",
      "Epoch 382/500\n",
      "5639/5639 [==============================] - 3s 578us/sample - loss: 9.2534 - rmse: 2.9667 - val_loss: 8.2955 - val_rmse: 2.5533\n",
      "Epoch 383/500\n",
      "5639/5639 [==============================] - 3s 582us/sample - loss: 9.2477 - rmse: 2.9642 - val_loss: 7.4057 - val_rmse: 2.3870\n",
      "Epoch 384/500\n",
      "5639/5639 [==============================] - 3s 577us/sample - loss: 9.1648 - rmse: 2.9550 - val_loss: 7.5227 - val_rmse: 2.4157\n",
      "Epoch 385/500\n",
      "5639/5639 [==============================] - 3s 578us/sample - loss: 9.2002 - rmse: 2.9599 - val_loss: 7.5323 - val_rmse: 2.4101\n",
      "Epoch 386/500\n",
      "5639/5639 [==============================] - 3s 581us/sample - loss: 9.0769 - rmse: 2.9388 - val_loss: 8.0246 - val_rmse: 2.5055\n",
      "Epoch 387/500\n",
      "5639/5639 [==============================] - 3s 583us/sample - loss: 9.1200 - rmse: 2.9441 - val_loss: 9.3932 - val_rmse: 2.7568\n",
      "Epoch 388/500\n",
      "5639/5639 [==============================] - 3s 580us/sample - loss: 9.0524 - rmse: 2.9346 - val_loss: 8.2386 - val_rmse: 2.5482\n",
      "Epoch 389/500\n",
      "5639/5639 [==============================] - 3s 578us/sample - loss: 9.0586 - rmse: 2.9343 - val_loss: 7.6528 - val_rmse: 2.4342\n",
      "Epoch 390/500\n",
      "5639/5639 [==============================] - 3s 583us/sample - loss: 8.9837 - rmse: 2.9233 - val_loss: 7.8693 - val_rmse: 2.4765\n",
      "Epoch 391/500\n",
      "5639/5639 [==============================] - 3s 578us/sample - loss: 8.8920 - rmse: 2.9077 - val_loss: 7.4933 - val_rmse: 2.4060\n",
      "Epoch 392/500\n",
      "5639/5639 [==============================] - 3s 585us/sample - loss: 8.9800 - rmse: 2.9240 - val_loss: 8.6691 - val_rmse: 2.6277\n",
      "Epoch 393/500\n",
      "5639/5639 [==============================] - 3s 584us/sample - loss: 8.9682 - rmse: 2.9191 - val_loss: 9.1249 - val_rmse: 2.7106\n",
      "Epoch 394/500\n",
      "5639/5639 [==============================] - 3s 582us/sample - loss: 9.0000 - rmse: 2.9243 - val_loss: 8.2129 - val_rmse: 2.5441\n",
      "Epoch 395/500\n",
      "5639/5639 [==============================] - 3s 583us/sample - loss: 8.9096 - rmse: 2.9091 - val_loss: 8.3749 - val_rmse: 2.5664\n",
      "Epoch 396/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5639/5639 [==============================] - 3s 578us/sample - loss: 8.9995 - rmse: 2.9242 - val_loss: 8.1908 - val_rmse: 2.5409\n",
      "Epoch 397/500\n",
      "5639/5639 [==============================] - 3s 575us/sample - loss: 8.8527 - rmse: 2.8997 - val_loss: 7.8042 - val_rmse: 2.4627\n",
      "Epoch 398/500\n",
      "5639/5639 [==============================] - 3s 576us/sample - loss: 8.8003 - rmse: 2.8946 - val_loss: 7.5801 - val_rmse: 2.4167\n",
      "Epoch 399/500\n",
      "5639/5639 [==============================] - 3s 575us/sample - loss: 8.7887 - rmse: 2.8909 - val_loss: 7.6120 - val_rmse: 2.4312\n",
      "Epoch 400/500\n",
      "5639/5639 [==============================] - 3s 576us/sample - loss: 8.7684 - rmse: 2.8880 - val_loss: 10.3752 - val_rmse: 2.9094\n",
      "Epoch 401/500\n",
      "5639/5639 [==============================] - 3s 575us/sample - loss: 8.8659 - rmse: 2.9009 - val_loss: 7.5976 - val_rmse: 2.4274\n",
      "Epoch 402/500\n",
      "5639/5639 [==============================] - 3s 577us/sample - loss: 8.6834 - rmse: 2.8724 - val_loss: 8.7031 - val_rmse: 2.6319\n",
      "Epoch 403/500\n",
      "5639/5639 [==============================] - 3s 577us/sample - loss: 8.7253 - rmse: 2.8776 - val_loss: 7.9153 - val_rmse: 2.4866\n",
      "Epoch 404/500\n",
      "5639/5639 [==============================] - 3s 580us/sample - loss: 8.6428 - rmse: 2.8682 - val_loss: 7.6335 - val_rmse: 2.4297\n",
      "Epoch 405/500\n",
      "5639/5639 [==============================] - 3s 578us/sample - loss: 8.5445 - rmse: 2.8514 - val_loss: 7.6289 - val_rmse: 2.4317\n",
      "Epoch 406/500\n",
      "5639/5639 [==============================] - 3s 576us/sample - loss: 8.6150 - rmse: 2.8619 - val_loss: 9.2098 - val_rmse: 2.7152\n",
      "Epoch 407/500\n",
      "5639/5639 [==============================] - 3s 576us/sample - loss: 8.5874 - rmse: 2.8556 - val_loss: 7.6452 - val_rmse: 2.4356\n",
      "Epoch 408/500\n",
      "5639/5639 [==============================] - 3s 576us/sample - loss: 8.5846 - rmse: 2.8574 - val_loss: 9.3455 - val_rmse: 2.7427\n",
      "Epoch 409/500\n",
      "5639/5639 [==============================] - 3s 579us/sample - loss: 8.5771 - rmse: 2.8550 - val_loss: 7.3654 - val_rmse: 2.3820\n",
      "Epoch 410/500\n",
      "5639/5639 [==============================] - 3s 579us/sample - loss: 8.5295 - rmse: 2.8485 - val_loss: 7.6143 - val_rmse: 2.4307\n",
      "Epoch 411/500\n",
      "5639/5639 [==============================] - 3s 586us/sample - loss: 8.5205 - rmse: 2.8444 - val_loss: 7.3583 - val_rmse: 2.3816\n",
      "Epoch 412/500\n",
      "5639/5639 [==============================] - 3s 575us/sample - loss: 8.5026 - rmse: 2.8462 - val_loss: 7.8034 - val_rmse: 2.4649\n",
      "Epoch 413/500\n",
      "5639/5639 [==============================] - 3s 581us/sample - loss: 8.4525 - rmse: 2.8354 - val_loss: 7.4948 - val_rmse: 2.4077\n",
      "Epoch 414/500\n",
      "5639/5639 [==============================] - 3s 575us/sample - loss: 8.4337 - rmse: 2.8324 - val_loss: 8.2988 - val_rmse: 2.5600\n",
      "Epoch 415/500\n",
      "5639/5639 [==============================] - 3s 590us/sample - loss: 8.4492 - rmse: 2.8334 - val_loss: 7.3201 - val_rmse: 2.3725\n",
      "Epoch 416/500\n",
      "5639/5639 [==============================] - 3s 578us/sample - loss: 8.3650 - rmse: 2.8219 - val_loss: 9.0074 - val_rmse: 2.6859\n",
      "Epoch 417/500\n",
      "5639/5639 [==============================] - 3s 578us/sample - loss: 8.3578 - rmse: 2.8164 - val_loss: 7.8177 - val_rmse: 2.4679\n",
      "Epoch 418/500\n",
      "5639/5639 [==============================] - 3s 575us/sample - loss: 8.3339 - rmse: 2.8160 - val_loss: 7.8834 - val_rmse: 2.4795\n",
      "Epoch 419/500\n",
      "5639/5639 [==============================] - 3s 576us/sample - loss: 8.3136 - rmse: 2.8131 - val_loss: 7.8791 - val_rmse: 2.4831\n",
      "Epoch 420/500\n",
      "5639/5639 [==============================] - 3s 576us/sample - loss: 8.2928 - rmse: 2.8089 - val_loss: 8.5338 - val_rmse: 2.6036\n",
      "Epoch 421/500\n",
      "5639/5639 [==============================] - 3s 579us/sample - loss: 8.2467 - rmse: 2.8012 - val_loss: 7.3672 - val_rmse: 2.3771\n",
      "Epoch 422/500\n",
      "5639/5639 [==============================] - 3s 577us/sample - loss: 8.2899 - rmse: 2.8092 - val_loss: 9.9788 - val_rmse: 2.8501\n",
      "Epoch 423/500\n",
      "5639/5639 [==============================] - 3s 576us/sample - loss: 8.2071 - rmse: 2.7945 - val_loss: 8.0375 - val_rmse: 2.5080\n",
      "Epoch 424/500\n",
      "5639/5639 [==============================] - 3s 578us/sample - loss: 8.2048 - rmse: 2.7948 - val_loss: 7.9761 - val_rmse: 2.5009\n",
      "Epoch 425/500\n",
      "5639/5639 [==============================] - 3s 578us/sample - loss: 8.1943 - rmse: 2.7931 - val_loss: 7.9435 - val_rmse: 2.4974\n",
      "Epoch 426/500\n",
      "5639/5639 [==============================] - 3s 578us/sample - loss: 8.2029 - rmse: 2.7947 - val_loss: 8.7261 - val_rmse: 2.6383\n",
      "Epoch 427/500\n",
      "5639/5639 [==============================] - 3s 579us/sample - loss: 8.1534 - rmse: 2.7848 - val_loss: 8.7315 - val_rmse: 2.6366\n",
      "Epoch 428/500\n",
      "5639/5639 [==============================] - 3s 578us/sample - loss: 8.2063 - rmse: 2.7920 - val_loss: 8.2011 - val_rmse: 2.5416\n",
      "Epoch 429/500\n",
      "5639/5639 [==============================] - 3s 579us/sample - loss: 8.0714 - rmse: 2.7702 - val_loss: 9.1213 - val_rmse: 2.7034\n",
      "Epoch 430/500\n",
      "5639/5639 [==============================] - 3s 582us/sample - loss: 8.1125 - rmse: 2.7754 - val_loss: 7.5554 - val_rmse: 2.4184\n",
      "Epoch 431/500\n",
      "5639/5639 [==============================] - 3s 582us/sample - loss: 7.9884 - rmse: 2.7562 - val_loss: 8.1961 - val_rmse: 2.5407\n",
      "Epoch 432/500\n",
      "5639/5639 [==============================] - 3s 584us/sample - loss: 8.0236 - rmse: 2.7636 - val_loss: 7.7623 - val_rmse: 2.4579\n",
      "Epoch 433/500\n",
      "5639/5639 [==============================] - 3s 579us/sample - loss: 8.0408 - rmse: 2.7664 - val_loss: 7.7544 - val_rmse: 2.4588\n",
      "Epoch 434/500\n",
      "5639/5639 [==============================] - 3s 578us/sample - loss: 8.0090 - rmse: 2.7614 - val_loss: 7.5908 - val_rmse: 2.4248\n",
      "Epoch 435/500\n",
      "5639/5639 [==============================] - 3s 581us/sample - loss: 8.0420 - rmse: 2.7685 - val_loss: 8.3357 - val_rmse: 2.5638\n",
      "Epoch 436/500\n",
      "5639/5639 [==============================] - 3s 578us/sample - loss: 7.8933 - rmse: 2.7395 - val_loss: 7.4647 - val_rmse: 2.4050\n",
      "Epoch 437/500\n",
      "5639/5639 [==============================] - 3s 584us/sample - loss: 7.9212 - rmse: 2.7470 - val_loss: 7.6234 - val_rmse: 2.4340\n",
      "Epoch 438/500\n",
      "5639/5639 [==============================] - 3s 579us/sample - loss: 7.8454 - rmse: 2.7330 - val_loss: 8.3168 - val_rmse: 2.5657\n",
      "Epoch 439/500\n",
      "5639/5639 [==============================] - 3s 581us/sample - loss: 7.9253 - rmse: 2.7438 - val_loss: 8.6430 - val_rmse: 2.6204\n",
      "Epoch 440/500\n",
      "5639/5639 [==============================] - 3s 605us/sample - loss: 7.9585 - rmse: 2.7507 - val_loss: 7.3134 - val_rmse: 2.3721\n",
      "Epoch 441/500\n",
      "5639/5639 [==============================] - 3s 591us/sample - loss: 7.8074 - rmse: 2.7234 - val_loss: 7.5285 - val_rmse: 2.4163\n",
      "Epoch 442/500\n",
      "5639/5639 [==============================] - 3s 579us/sample - loss: 7.8257 - rmse: 2.7283 - val_loss: 7.6420 - val_rmse: 2.4403\n",
      "Epoch 443/500\n",
      "5639/5639 [==============================] - 3s 579us/sample - loss: 7.8076 - rmse: 2.7253 - val_loss: 7.4267 - val_rmse: 2.3920\n",
      "Epoch 444/500\n",
      "5639/5639 [==============================] - 3s 578us/sample - loss: 7.8527 - rmse: 2.7321 - val_loss: 7.6701 - val_rmse: 2.4433\n",
      "Epoch 445/500\n",
      "5639/5639 [==============================] - 3s 582us/sample - loss: 7.8202 - rmse: 2.7261 - val_loss: 8.3859 - val_rmse: 2.5775\n",
      "Epoch 446/500\n",
      "5639/5639 [==============================] - 3s 579us/sample - loss: 7.7824 - rmse: 2.7192 - val_loss: 8.0952 - val_rmse: 2.5246\n",
      "Epoch 447/500\n",
      "5639/5639 [==============================] - 3s 579us/sample - loss: 7.6873 - rmse: 2.7037 - val_loss: 7.3242 - val_rmse: 2.3741\n",
      "Epoch 448/500\n",
      "5639/5639 [==============================] - 3s 583us/sample - loss: 7.7200 - rmse: 2.7100 - val_loss: 8.0218 - val_rmse: 2.5046\n",
      "Epoch 449/500\n",
      "5639/5639 [==============================] - 3s 579us/sample - loss: 7.6553 - rmse: 2.6995 - val_loss: 7.7455 - val_rmse: 2.4554\n",
      "Epoch 450/500\n",
      "5639/5639 [==============================] - 3s 581us/sample - loss: 7.6534 - rmse: 2.6997 - val_loss: 7.4140 - val_rmse: 2.3927\n",
      "Epoch 451/500\n",
      "5639/5639 [==============================] - 3s 583us/sample - loss: 7.6021 - rmse: 2.6886 - val_loss: 8.0738 - val_rmse: 2.5142\n",
      "Epoch 452/500\n",
      "5639/5639 [==============================] - 3s 584us/sample - loss: 7.6634 - rmse: 2.7000 - val_loss: 7.4966 - val_rmse: 2.4034\n",
      "Epoch 453/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5639/5639 [==============================] - 3s 575us/sample - loss: 7.7124 - rmse: 2.7063 - val_loss: 8.0882 - val_rmse: 2.5156\n",
      "Epoch 454/500\n",
      "5639/5639 [==============================] - 3s 574us/sample - loss: 7.6477 - rmse: 2.6973 - val_loss: 7.6145 - val_rmse: 2.4275\n",
      "Epoch 455/500\n",
      "5639/5639 [==============================] - 3s 576us/sample - loss: 7.4717 - rmse: 2.6664 - val_loss: 7.8955 - val_rmse: 2.4804\n",
      "Epoch 456/500\n",
      "5639/5639 [==============================] - 3s 576us/sample - loss: 7.5257 - rmse: 2.6750 - val_loss: 7.8417 - val_rmse: 2.4736\n",
      "Epoch 457/500\n",
      "5639/5639 [==============================] - 3s 579us/sample - loss: 7.5180 - rmse: 2.6733 - val_loss: 7.4144 - val_rmse: 2.3942\n",
      "Epoch 458/500\n",
      "5639/5639 [==============================] - 3s 576us/sample - loss: 7.5035 - rmse: 2.6690 - val_loss: 7.3288 - val_rmse: 2.3755\n",
      "Epoch 459/500\n",
      "5639/5639 [==============================] - 3s 575us/sample - loss: 7.5254 - rmse: 2.6740 - val_loss: 7.6622 - val_rmse: 2.4425\n",
      "Epoch 460/500\n",
      "5639/5639 [==============================] - 3s 578us/sample - loss: 7.4952 - rmse: 2.6680 - val_loss: 9.6658 - val_rmse: 2.7908\n",
      "Epoch 461/500\n",
      "5639/5639 [==============================] - 3s 580us/sample - loss: 7.5637 - rmse: 2.6808 - val_loss: 7.5770 - val_rmse: 2.4189\n",
      "Epoch 462/500\n",
      "5639/5639 [==============================] - 3s 581us/sample - loss: 7.4185 - rmse: 2.6565 - val_loss: 8.2280 - val_rmse: 2.5396\n",
      "Epoch 463/500\n",
      "5639/5639 [==============================] - 3s 574us/sample - loss: 7.3858 - rmse: 2.6509 - val_loss: 7.9336 - val_rmse: 2.4840\n",
      "Epoch 464/500\n",
      "5639/5639 [==============================] - 3s 577us/sample - loss: 7.4904 - rmse: 2.6694 - val_loss: 7.9175 - val_rmse: 2.4824\n",
      "Epoch 465/500\n",
      "5639/5639 [==============================] - 3s 578us/sample - loss: 7.4505 - rmse: 2.6621 - val_loss: 8.0256 - val_rmse: 2.5065\n",
      "Epoch 466/500\n",
      "5639/5639 [==============================] - 3s 578us/sample - loss: 7.3450 - rmse: 2.6410 - val_loss: 8.1290 - val_rmse: 2.5231\n",
      "Epoch 467/500\n",
      "5639/5639 [==============================] - 3s 578us/sample - loss: 7.3477 - rmse: 2.6421 - val_loss: 8.0246 - val_rmse: 2.5025\n",
      "Epoch 468/500\n",
      "5639/5639 [==============================] - 3s 578us/sample - loss: 7.3215 - rmse: 2.6370 - val_loss: 7.5473 - val_rmse: 2.4117\n",
      "Epoch 469/500\n",
      "5639/5639 [==============================] - 3s 575us/sample - loss: 7.3864 - rmse: 2.6495 - val_loss: 8.0382 - val_rmse: 2.4984\n",
      "Epoch 470/500\n",
      "5639/5639 [==============================] - 3s 585us/sample - loss: 7.3898 - rmse: 2.6484 - val_loss: 7.4164 - val_rmse: 2.3822\n",
      "Epoch 471/500\n",
      "5639/5639 [==============================] - 3s 578us/sample - loss: 7.2490 - rmse: 2.6229 - val_loss: 7.3393 - val_rmse: 2.3728\n",
      "Epoch 472/500\n",
      "5639/5639 [==============================] - 3s 578us/sample - loss: 7.2898 - rmse: 2.6318 - val_loss: 7.6272 - val_rmse: 2.4274\n",
      "Epoch 473/500\n",
      "5639/5639 [==============================] - 3s 582us/sample - loss: 7.2157 - rmse: 2.6203 - val_loss: 7.9315 - val_rmse: 2.4854\n",
      "Epoch 474/500\n",
      "5639/5639 [==============================] - 3s 578us/sample - loss: 7.2269 - rmse: 2.6212 - val_loss: 7.5734 - val_rmse: 2.4185\n",
      "Epoch 475/500\n",
      "5639/5639 [==============================] - 3s 579us/sample - loss: 7.2195 - rmse: 2.6205 - val_loss: 7.5134 - val_rmse: 2.4101\n",
      "Epoch 476/500\n",
      "5639/5639 [==============================] - 3s 579us/sample - loss: 7.1939 - rmse: 2.6165 - val_loss: 7.5578 - val_rmse: 2.4197\n",
      "Epoch 477/500\n",
      "5639/5639 [==============================] - 3s 577us/sample - loss: 7.1685 - rmse: 2.6106 - val_loss: 7.4494 - val_rmse: 2.3967\n",
      "Epoch 478/500\n",
      "5639/5639 [==============================] - 3s 590us/sample - loss: 7.1883 - rmse: 2.6135 - val_loss: 7.6468 - val_rmse: 2.4360\n",
      "Epoch 479/500\n",
      "5639/5639 [==============================] - 3s 579us/sample - loss: 7.1537 - rmse: 2.6079 - val_loss: 7.8760 - val_rmse: 2.4772\n",
      "Epoch 480/500\n",
      "5639/5639 [==============================] - 3s 579us/sample - loss: 7.0449 - rmse: 2.5899 - val_loss: 7.5558 - val_rmse: 2.4108\n",
      "Epoch 481/500\n",
      "5639/5639 [==============================] - 3s 579us/sample - loss: 7.1593 - rmse: 2.6106 - val_loss: 7.4784 - val_rmse: 2.3981\n",
      "Epoch 482/500\n",
      "5639/5639 [==============================] - 3s 581us/sample - loss: 7.0857 - rmse: 2.5944 - val_loss: 7.5755 - val_rmse: 2.4175\n",
      "Epoch 483/500\n",
      "5639/5639 [==============================] - 3s 580us/sample - loss: 7.1606 - rmse: 2.6067 - val_loss: 7.4305 - val_rmse: 2.3893\n",
      "Epoch 484/500\n",
      "5639/5639 [==============================] - 3s 580us/sample - loss: 7.0775 - rmse: 2.5935 - val_loss: 10.0270 - val_rmse: 2.8577\n",
      "Epoch 485/500\n",
      "5639/5639 [==============================] - 3s 579us/sample - loss: 7.1401 - rmse: 2.6046 - val_loss: 8.2332 - val_rmse: 2.5430\n",
      "Epoch 486/500\n",
      "5639/5639 [==============================] - 3s 581us/sample - loss: 7.0104 - rmse: 2.5829 - val_loss: 10.5033 - val_rmse: 2.9413\n",
      "Epoch 487/500\n",
      "5639/5639 [==============================] - 3s 578us/sample - loss: 7.0751 - rmse: 2.5887 - val_loss: 7.5310 - val_rmse: 2.4088\n",
      "Epoch 488/500\n",
      "5639/5639 [==============================] - 3s 579us/sample - loss: 6.9763 - rmse: 2.5751 - val_loss: 8.3149 - val_rmse: 2.5621\n",
      "Epoch 489/500\n",
      "5639/5639 [==============================] - 3s 580us/sample - loss: 6.9586 - rmse: 2.5702 - val_loss: 8.0308 - val_rmse: 2.4992\n",
      "Epoch 490/500\n",
      "5639/5639 [==============================] - 3s 580us/sample - loss: 6.9812 - rmse: 2.5783 - val_loss: 9.3310 - val_rmse: 2.7422\n",
      "Epoch 491/500\n",
      "5639/5639 [==============================] - 3s 580us/sample - loss: 7.0554 - rmse: 2.5833 - val_loss: 7.6490 - val_rmse: 2.4396\n",
      "Epoch 492/500\n",
      "5639/5639 [==============================] - 3s 580us/sample - loss: 6.9867 - rmse: 2.5735 - val_loss: 7.3501 - val_rmse: 2.3803\n",
      "Epoch 493/500\n",
      "5639/5639 [==============================] - 3s 579us/sample - loss: 6.9021 - rmse: 2.5612 - val_loss: 7.4202 - val_rmse: 2.3916\n",
      "Epoch 494/500\n",
      "5639/5639 [==============================] - 3s 577us/sample - loss: 6.8725 - rmse: 2.5554 - val_loss: 7.4375 - val_rmse: 2.3973\n",
      "Epoch 495/500\n",
      "5639/5639 [==============================] - 3s 578us/sample - loss: 6.9012 - rmse: 2.5591 - val_loss: 8.1103 - val_rmse: 2.5233\n",
      "Epoch 496/500\n",
      "5639/5639 [==============================] - 3s 579us/sample - loss: 6.9341 - rmse: 2.5631 - val_loss: 7.5010 - val_rmse: 2.4070\n",
      "Epoch 497/500\n",
      "5639/5639 [==============================] - 3s 581us/sample - loss: 6.8691 - rmse: 2.5533 - val_loss: 7.6568 - val_rmse: 2.4284\n",
      "Epoch 498/500\n",
      "5639/5639 [==============================] - 3s 579us/sample - loss: 6.8853 - rmse: 2.5541 - val_loss: 7.3707 - val_rmse: 2.3748\n",
      "Epoch 499/500\n",
      "5639/5639 [==============================] - 3s 588us/sample - loss: 6.8433 - rmse: 2.5469 - val_loss: 7.3923 - val_rmse: 2.3849\n",
      "Epoch 500/500\n",
      "5639/5639 [==============================] - 3s 595us/sample - loss: 6.8489 - rmse: 2.5507 - val_loss: 7.3309 - val_rmse: 2.3711\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "USE_SAVED_MODEL = False\n",
    "\n",
    "if USE_SAVED_MODEL == False:\n",
    "    history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs = 500,\n",
    "                    batch_size = 256,\n",
    "                    validation_split = 0.2, #data = (x_test, y_test),\n",
    "                    callbacks = callbacks\n",
    "                    )\n",
    "    plt.style.use('seaborn-darkgrid')\n",
    "    plt.plot(history.history['rmse'])\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Root Mean Square Error')\n",
    "    plt.title('Model Training Error')\n",
    "    plt.show() \n",
    "    \n",
    "else:\n",
    "    model.load_weights(model_dir+\"base_model_weights_1.h5\")\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookid_dir = '../input/IdLookupTable.csv'\n",
    "lookid_data = pd.read_csv(lookid_dir)\n",
    "test_data = pd.read_csv(test_file)\n",
    "\n",
    "x_test = []\n",
    "for i in range(0,len(test_data)):\n",
    "    img = test_data['Image'][i].split(' ')\n",
    "    x_test.append(img)\n",
    "    \n",
    "x_test = np.array(x_test,dtype = 'float')\n",
    "x_test = x_test/255.0\n",
    "x_test = x_test.reshape(-1,96,96,1)    \n",
    "\n",
    "y_test = model.predict(x_test)\n",
    "y_test = np.clip(y_test,0,96)\n",
    "np.save('../output/y_train_base', y_test)\n",
    "\n",
    "lookid_list = list(lookid_data['FeatureName'])\n",
    "imageID = list(lookid_data['ImageId']-1)\n",
    "pred_list = list(y_test)\n",
    "\n",
    "rowid = list(lookid_data['RowId'])\n",
    "\n",
    "feature = []\n",
    "for f in list(lookid_data['FeatureName']):\n",
    "    feature.append(lookid_list.index(f))\n",
    "    \n",
    "    \n",
    "submit_data = []\n",
    "for x,y in zip(imageID,feature):\n",
    "    submit_data.append(pred_list[x][y])\n",
    "rowid = pd.Series(rowid,name = 'RowId')\n",
    "loc = pd.Series(submit_data,name = 'Location')\n",
    "submission = pd.concat([rowid,loc],axis = 1)\n",
    "submission.to_csv('../output/w207_base_submission.csv',index = False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    sess = get_session()\n",
    "    clear_session()\n",
    "    sess.close()\n",
    "    sess = get_session()\n",
    "    try:\n",
    "        del model # this is from global space - change this as you need\n",
    "    except:\n",
    "        print(\"Model clear Failed\")\n",
    "    print(gc.collect())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
