{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4dbced162440c393a0a5b7e5aee344711a30e994"
   },
   "source": [
    "# Facial Keypoint Detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "369fa247a546e39d82bdfdc5b7d4ed58baa40e4f"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.backend.tensorflow_backend import clear_session\n",
    "from keras.backend.tensorflow_backend import get_session\n",
    "#from tensorflow.keras.layers import LeakyReLU, ReLU\n",
    "#from tensorflow.keras.models import Sequential, Model\n",
    "#from tensorflow.keras.layers import Activation, Convolution2D, BatchNormalization, Flatten\n",
    "#from tensorflow.keras.layers import Dense, Dropout, Input, MaxPool2D, Layer, Reshape, Lambda\n",
    "#from tensorflow.keras.optimizers import Nadam\n",
    "#from keras import backend\n",
    "\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv2D, Dense, Input, Reshape, Lambda, Layer, Flatten\n",
    "from keras.layers import LeakyReLU, BatchNormalization, MaxPool2D\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow \n",
    "from keras import initializers\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers.core import Activation\n",
    "import pandas as pd\n",
    "from keras.optimizers import Nadam\n",
    "from keras import backend as K\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import cv2\n",
    "import os, gc, json, math\n",
    "from keras import initializers\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers.core import Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pixels(data):\n",
    "    \"\"\"\n",
    "    Convert pixels to the right intensity 0-1 and in a square matrix.\n",
    "    \"\"\"\n",
    "    data = np.array([row.split(' ') for row in data['Image']],dtype='float') / 255.0\n",
    "    data = data.reshape(-1,96,96,1)\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_img(sample_img,coord=None):\n",
    "    \"\"\"\n",
    "    Display an image. For debugging, display a coordinate on the image.\n",
    "    input:\n",
    "        - sample_img: numpy array. image to be displayed\n",
    "        - coord: lst. of coordinates in form [[x_coordinate,y_coordinate],[x_coordinate,y_coordinate]]\n",
    "    TODO handle multiple coordinates. Work out bugs with multiple coordinates\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.imshow(sample_img.reshape(96,96),cmap='gray')\n",
    "    if coord is not None:\n",
    "        plt.scatter(coord[0],coord[1],marker = '*',c='r')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_facial_keypoints(data,ind):\n",
    "    \"\"\"\n",
    "    Structure the coordinates for all facial keypoints for a single image.\n",
    "    inputs:\n",
    "        - data: numpy array containing rows as each image sample and columns as facial keypoint coordinates\n",
    "        - ind: index of the image\n",
    "    output:\n",
    "        - numpy array with format [[list of x-coordinates],[list of y-coordinates]]\n",
    "    \"\"\"\n",
    "    data[ind]\n",
    "    it = iter(data[ind])\n",
    "    x_coord = []\n",
    "    y_coord = []\n",
    "\n",
    "    for x in it:\n",
    "        x_coord.append(x)\n",
    "        y_coord.append(next(it))\n",
    "    \n",
    "    return(np.array([x_coord,y_coord]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "# Reset Keras Session\n",
    "def reset_keras():\n",
    "    sess = get_session()\n",
    "    clear_session()\n",
    "    sess.close()\n",
    "    sess = get_session()\n",
    "\n",
    "    try:\n",
    "        del model # this is from global space - change this as you need\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    print(gc.collect()) # if it's done something you should see a number being outputted\n",
    "\n",
    "    # use the same config as you used to create the session\n",
    "    config = tensorflow.ConfigProto()\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = 1\n",
    "    config.gpu_options.visible_device_list = \"0\"\n",
    "    set_session(tensorflow.Session(config=config))\n",
    "    \n",
    "reset_keras()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "fa1b76273d02502e3fd668dddf74ecf522044524"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('../output/'):\n",
    "    os.makedirs('../output/model')\n",
    "    os.makedirs('../output/history')\n",
    "    \n",
    "    \n",
    "model_dir = \"../output/model/\"\n",
    "history_dir = \"../output/history/\"\n",
    "\n",
    "train_file = '../input/training/training.csv'\n",
    "test_file = '../input/test/test.csv'\n",
    "train_data = pd.read_csv(train_file)  \n",
    "#test_data = pd.read_csv(test_file)\n",
    "\n",
    "\n",
    "bad_samples = [1747, 1731, 1877, 1881, 1979, 2199, 2289, 2321, 2453, 3173, 3296, 3447, 4180, 6859,\n",
    "              2090, 2175, 1907, 2562, 2818, 3296, 3447, 4263, 4482, 4490, 4636, 5059, 6493, 6585, 6906]\n",
    "\n",
    "train_data = train_data.drop(bad_samples).reset_index(drop=True)\n",
    "train_clean = train_data.dropna()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e1b88f1528838c0a8fec61f9a02a70b5077312e9"
   },
   "source": [
    "Create training vector with images and normalize thee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = convert_pixels(train_data)\n",
    "\n",
    "y_train = train_data[[col for col in train_data.columns if col != 'Image']].to_numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a833f4cc5e559774d3a310fd09d40d31e49e71da"
   },
   "source": [
    "Generate labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "e9d804a035809cdf8ffda19f41ce3feb278a38fb"
   },
   "outputs": [],
   "source": [
    "\n",
    "imputer = IterativeImputer(max_iter=1000, tol=0.01, random_state=1)\n",
    "y_train = imputer.fit_transform(y_train)\n",
    "\n",
    "#imputer = IterativeImputer(max_iter=1000, tol=0.01, random_state=2)\n",
    "#y_clean = imputer.fit_transform(y_clean)\n",
    "\n",
    "bad_bottom_lip = [210, 350, 499, 512, 810, 839, 895, 1058, 1194,1230, 1245, 1546, 1548]\n",
    "for sample in bad_bottom_lip:\n",
    "    y_train[sample][29] = 94\n",
    "    y_train[sample][28] = y_train[sample][26]\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set feature engineering parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_na = False\n",
    "add_flip_horiz = True\n",
    "add_blur_img = False\n",
    "add_rotate_img = False\n",
    "add_contrast_img = False\n",
    "add_translate_img = False\n",
    "orig_x_train = x_train.copy()\n",
    "orig_y_train = y_train.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill NA in the training labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if fill_na:\n",
    "    # https://stackoverflow.com/questions/18689235/numpy-array-replace-nan-values-with-average-of-columns\n",
    "    # get column means\n",
    "    col_mean = np.nanmean(y_train,axis=0)\n",
    "\n",
    "    # find the x,y indices that are missing from y_train\n",
    "    inds = np.where(np.isnan(y_train))\n",
    "\n",
    "    # fill in missing values in y_train with the column means. \"take\" is much more efficient than fancy indexing\n",
    "    y_train[inds] = np.take(col_mean, inds[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flip images horizontally and add to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_img_horiz(train_data):\n",
    "    \"\"\"\n",
    "    Flip images horizontally for all training images\n",
    "    \"\"\"\n",
    "    # Flip images\n",
    "    x_train = convert_pixels(train_data)\n",
    "    flip_img = np.array([np.fliplr(x_train[[ind]][0]) for ind in range(x_train.shape[0])])\n",
    "    \n",
    "    # Flip coordinates\n",
    "    train_data_flip = train_data.copy()\n",
    "    x_columns = [col for col in train_data.columns if '_x' in col]\n",
    "    train_data_flip[x_columns] = train_data[x_columns].applymap(lambda x: 96-x)\n",
    "    \n",
    "    #left and right are swapped so undo\n",
    "    left_columns = [col for col in train_data.columns if 'left' in col]\n",
    "    right_columns = [col for col in train_data.columns if 'right' in col]\n",
    "    train_data_flip[left_columns+right_columns] = train_data_flip[right_columns+left_columns]\n",
    "    \n",
    "    flip_coord = train_data_flip[[col for col in train_data if col != 'Image']].to_numpy()\n",
    "    return(flip_img,flip_coord)\n",
    "\n",
    "if add_flip_horiz:\n",
    "    # Apply the augmentation and add the new data to the training set\n",
    "    flipped_img,flipped_coord = flip_img_horiz(train_clean)\n",
    "    \n",
    "   \n",
    "    x_train = np.append(x_train,flipped_img,axis=0)\n",
    "    y_train = np.append(y_train,flipped_coord,axis=0)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Gaussian blurring with a 5x5 filter with $\\sigma$ = 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blur_img():\n",
    "    \"\"\"\n",
    "    Add Gaussian blurring to the images\n",
    "    \"\"\"\n",
    "    # https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_filtering/py_filtering.html\n",
    "    blur_img = np.array([cv2.GaussianBlur(orig_x_train[[ind]][0],(5,5),2).reshape(96,96,1) for ind in range(orig_x_train.shape[0])])\n",
    "    \n",
    "    return(blur_img)\n",
    "\n",
    "if add_blur_img:\n",
    "    x_train = np.append(x_train,blur_img(),axis=0)\n",
    "    y_train = np.append(y_train,orig_y_train,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add image rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_img(x_train, y_train):\n",
    "    \"\"\"\"\n",
    "    Rotate images by angles between [5, 10, 14 degrees]\n",
    "    \"\"\"\n",
    "    angles = [5, -5, 10, -10, 14, -14]\n",
    "    b = np.ones((1,3))\n",
    "    rows,cols = (96,96)\n",
    "    x_train_rot = []\n",
    "    y_train_rot = y_train.copy()\n",
    "    M_angles = [cv2.getRotationMatrix2D((cols/2,rows/2),angle,1) for angle in angles]\n",
    "    \n",
    "    for i in range(x_train.shape[0]):\n",
    "        #M = cv2.getRotationMatrix2D((cols/2,rows/2),np.random.choice(angles,1),1)\n",
    "        M = M_angles[np.random.choice(len(M_angles))]\n",
    "        x_train_rot.append((cv2.warpAffine(x_train[[i]].reshape(rows,cols,1),M,(cols,rows)).reshape(96,96,1)))\n",
    "       \n",
    "        #apply affine transformation to (x,y) labels\n",
    "        for j in range(int(y_train.shape[1]/2)):\n",
    "            b[:,0:2] = y_train[i,2*j:2*j+2]\n",
    "            y_train_rot[i,2*j:2*j+2] = np.dot(b,M.transpose()) \n",
    "    \n",
    "    x_train_rot = np.array(x_train_rot)\n",
    "    return x_train_rot, y_train_rot\n",
    "\n",
    "if add_rotate_img:\n",
    "    \n",
    "    x_rotate, y_rotate = rotate_img(x_train,y_train)\n",
    "    x_train = np.append(x_train,x_rotate,axis=0)\n",
    "    y_train = np.append(y_train,y_rotate,axis=0)   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add image contrast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrast_img(apply_random,brightness=.5):\n",
    "    \"\"\"\n",
    "    Add brighter and darker images to the training set with the range of pixel values allowed set at 0 and 1\n",
    "    Only applies contrast to the original images. Need to make sure we're not setting random seed\n",
    "    input:\n",
    "        - brightness: float between -1 and 1\n",
    "        - apply_random: boolean. Applies random brightness to every sample in the data set.\n",
    "          Ignores brightness setting\n",
    "    \"\"\"\n",
    "    if apply_random:\n",
    "        # uses a .1 increment [-.3,.8) to pick out a brightness number for each sample\n",
    "        # numbers are chosen such that they are still realistically visible and\n",
    "        # the added training data has an appreciable change in contrast\n",
    "        brightness = np.random.choice(np.round(np.arange(-.3,.8,.1),2),size=orig_x_train.shape[0])\n",
    "        bright_img = np.array([orig_x_train[[ind]][0]+brightness[ind] for ind in range(orig_x_train.shape[0])])\n",
    "    else:\n",
    "        bright_img = orig_x_train + brightness\n",
    "\n",
    "    bright_img[bright_img > 1] = 1\n",
    "    bright_img[bright_img < 0] = 0\n",
    "    return(bright_img)\n",
    "    \n",
    "if add_contrast_img:\n",
    "    # create two sets of images undergoing contrast changes\n",
    "    x_train = np.append(x_train,contrast_img(apply_random=False),axis=0)\n",
    "    y_train = np.append(y_train,orig_y_train,axis=0)\n",
    "    \n",
    "# testing code\n",
    "# view_img(contrast_img(apply_random=True)[[30]])\n",
    "# view_img(contrast_img(apply_random=True)[[30]])\n",
    "# view_img(contrast_img(apply_random=True)[[30]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_img():\n",
    "    \"\"\"\n",
    "    Add translational shift to the images randomly\n",
    "    \"\"\"\n",
    "    trans_train_data = train_data.copy()\n",
    "    \n",
    "    rows,cols = (96,96)\n",
    "    shift_x = 96*np.random.choice(np.arange(-.4,.4,.05),size=trans_train_data.shape[0])\n",
    "    shift_y = 96*np.random.choice(np.arange(-.4,.4,.05),size=trans_train_data.shape[0])\n",
    "    M = [np.float32([[1,0,shift_x[ind]],[0,1,shift_y[ind]]]) for ind in range(trans_train_data.shape[0])]\n",
    "    trans_img = np.array([cv2.warpAffine(x_train[[ind]].reshape(96,96,1),M[ind],(cols,rows)).reshape(96,96,1) for ind in range(trans_train_data.shape[0])])\n",
    "    \n",
    "    x_col = [col for col in train_data.columns if ((col != 'Image') & ('_x' in col))]\n",
    "    y_col = [col for col in train_data.columns if ((col != 'Image') & ('_y' in col))]\n",
    "\n",
    "    shift_x_array = np.array([np.repeat(x,len(x_col))for x in shift_x])\n",
    "    shift_y_array = np.array([np.repeat(y,len(y_col))for y in shift_y])\n",
    "    \n",
    "    trans_train_data[x_col] += shift_x_array\n",
    "    trans_train_data[y_col] += shift_y_array\n",
    "    \n",
    "    trans_coord = np.array(trans_train_data[[col for col in trans_train_data.columns if col != 'Image']])\n",
    "\n",
    "    # TODO should we force these to be nan or leave as is? If leave as is, comment this out.\n",
    "    trans_coord[trans_coord > 96]= np.nan\n",
    "    trans_coord[trans_coord < 0]= np.nan\n",
    "    return(trans_img,trans_coord)\n",
    "\n",
    "if add_translate_img:\n",
    "    trans_img,trans_coord = translate_img()\n",
    "    x_train = np.append(x_train,trans_img,axis=0)\n",
    "    y_train = np.append(y_train,trans_coord,axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callback function if detailed log required\n",
    "class History(tensorflow.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.train_loss = []\n",
    "        self.train_rmse = []\n",
    "        self.val_rmse = []\n",
    "        self.val_loss = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.train_loss.append(logs.get('loss'))\n",
    "        self.train_rmse.append(logs.get('rmse'))\n",
    "        \n",
    "    def on_epoch_end(self, batch, logs={}):    \n",
    "        self.val_rmse.append(logs.get('val_rmse'))\n",
    "        self.val_loss.append(logs.get('val_loss'))\n",
    "        \n",
    "# Implement ModelCheckPoint callback function to save CNN model\n",
    "class CNN_ModelCheckpoint(tensorflow.keras.callbacks.Callback):\n",
    "\n",
    "    def __init__(self, model, filename):\n",
    "        self.filename = filename\n",
    "        self.cnn_model = model\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.max_val_rmse = math.inf\n",
    "        \n",
    " \n",
    "    def on_epoch_end(self, batch, logs={}):    \n",
    "        val_rmse = logs.get('val_rmse')\n",
    "        if(val_rmse < self.max_val_rmse):\n",
    "           self.max_val_rmse = val_rmse\n",
    "           self.cnn_model.save_weights(self.filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Primary Capsula Layer\n",
    "The output is then reshaped into 8-dimensional vector. So shape will be 6x6x32 capsules each of which will be 8-dimensional. Then it will pass through a non-linear function(squash) so that length of output vector can be maintained between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convolution layer with stride 2 and 256 filters of size 9*9\n",
    "#conv2 = Conv2D(256, (9,9), strides = 2, padding = 'valid')(conv1)\n",
    " \n",
    "# reshape into 1152 capsules of 8 dimensional vectors\n",
    "#reshaped = Reshape((6*6*32,8))(conv1)\n",
    " \n",
    "def squash(inputs):\n",
    "    # take norm of input vectors\n",
    "    squared_norm = K.sum(K.square(inputs), axis = -1, keepdims = True)\n",
    " \n",
    "    # use the formula for non-linear function to return squashed output\n",
    "    return ((squared_norm/(1+squared_norm))/(K.sqrt(squared_norm+K.epsilon())))*inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Capsule Layer\n",
    "\n",
    "Here we need to write a custom layer in keras. It will take 1152x8 as its input and produces output of size 30x16, where 30 capsules each represents an output class with 16 dimensional vector. Then each of these 30 capsules are converted into single value to predict the output class using a lambda layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapsuleLayer(Layer):\n",
    "    # creating a layer class in keras\n",
    "    def __init__(self, **kwargs):\n",
    "        super(CapsuleLayer, self).__init__(**kwargs)\n",
    "        self.kernel_initializer = initializers.get('glorot_uniform')\n",
    "    \n",
    "    def build(self, input_shape): \n",
    "        # initialize weight matrix for each capsule in lower layer\n",
    "        self.W = self.add_weight(shape = [30, 6*6*32, 16, 8], initializer = self.kernel_initializer, name = 'weights')\n",
    "        self.built = True\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        inputs = K.expand_dims(inputs, 1)\n",
    "        inputs = K.tile(inputs, [1, 30, 1, 1])\n",
    "        # matrix multiplication b/w previous layer output and weight matrix\n",
    "        inputs = K.map_fn(lambda x: K.batch_dot(x, self.W, [2, 3]), elems=inputs)\n",
    "        b = tf.zeros(shape = [K.shape(inputs)[0], 30, 6*6*32])\n",
    "        \n",
    "# routing algorithm with updating coupling coefficient c, using scalar product b/w input capsule and output capsule\n",
    "        for i in range(3):\n",
    "            c = tf.nn.softmax(b, dim=1)\n",
    "            s = K.batch_dot(c, inputs, [2, 2])\n",
    "            v = squash(s)\n",
    "            b = b + K.batch_dot(v, inputs, [2,3])\n",
    "            \n",
    "        return v \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return tuple([None, 30, 16])\n",
    "    \n",
    "    \n",
    "    \n",
    "def output_layer(inputs):\n",
    "    return K.sqrt(K.sum(K.square(inputs), -1) + K.epsilon())\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 96, 96, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 96, 96, 32)        288       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 96, 96, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 96, 96, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 48, 48, 64)        18432     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 48, 48, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 48, 48, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 24, 24, 96)        55296     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 24, 24, 96)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 24, 24, 96)        384       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 12, 12, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 12, 12, 128)       110592    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 6, 6, 256)         294912    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 6, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 1152, 8)           0         \n",
      "_________________________________________________________________\n",
      "lambda_3 (Lambda)            (None, 1152, 8)           0         \n",
      "_________________________________________________________________\n",
      "capsule_layer_2 (CapsuleLaye (None, 30, 16)            4423680   \n",
      "_________________________________________________________________\n",
      "lambda_4 (Lambda)            (None, 30)                0         \n",
      "=================================================================\n",
      "Total params: 4,905,504\n",
      "Trainable params: 4,904,352\n",
      "Non-trainable params: 1,152\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def capsule_model():\n",
    "    input_shape = Input(shape=(96,96,1))  # size of input image is 96*96\n",
    " \n",
    "# a convolution layer output shape = 20*20*256\n",
    "#conv1 = Conv2D(256, (9,9), activation = 'relu', padding = 'valid')(input_shape)\n",
    "    x = Conv2D(32, (3,3), padding='same', use_bias=False)(input_shape)\n",
    "    x = LeakyReLU(alpha = 0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "    x = Conv2D(64, (3,3), padding='same', use_bias=False)(x)\n",
    "    x = LeakyReLU(alpha = 0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "    x = Conv2D(96, (3,3), padding='same', use_bias=False)(x)\n",
    "    x = LeakyReLU(alpha = 0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "    x = Conv2D(128, (3,3),padding='same', use_bias=False)(x)\n",
    "    x = LeakyReLU(alpha = 0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "    x = Conv2D(256, (3,3),padding='same',use_bias=False)(x)\n",
    "    x = LeakyReLU(alpha = 0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    #reshape into 1152 capsules of 8 dimensional vectors\n",
    "    reshaped = Reshape((6*6*32,8))(x)\n",
    "    \n",
    "    # squash the reshaped output to make length of vector b/w 0 and 1\n",
    "    squashed_output = Lambda(squash)(reshaped)\n",
    "    \n",
    "    caps_out = CapsuleLayer()(squashed_output)\n",
    "    outputs = Lambda(output_layer)(caps_out)\n",
    "    \n",
    "    model = Model(input_shape, outputs, name=\"capsule_model\")\n",
    "    return model\n",
    "\n",
    "model = capsule_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom RMSE metric\n",
    "def rmse(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))\n",
    "\n",
    "#from tensorflow.keras.optimizers.schedules import InverseTimeDecay\n",
    "\n",
    "# Use Nadam optimizer with variable learning rate\n",
    "optimizer = Nadam(lr=0.00001,\n",
    "                  beta_1=0.9,\n",
    "                  beta_2=0.999,\n",
    "                  epsilon=1e-08,\n",
    "                  schedule_decay=0.004)\n",
    "\n",
    "\n",
    "# Loss: MSE and Metric = RMSE\n",
    "model.compile(optimizer= optimizer, \n",
    "              loss='mean_squared_error',\n",
    "              metrics=[rmse])\n",
    "\n",
    "#Callback to save the best model\n",
    "saveBase_Model = CNN_ModelCheckpoint(model, model_dir+\"capsule_model_weights.h5\")\n",
    "\n",
    "#define callback functions\n",
    "callbacks = [#EarlyStopping(monitor='val_rmse', patience=3, verbose=2),\n",
    "             saveBase_Model]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e4cf4686b410841f2e34dbb081f3429d1b0f67e9"
   },
   "source": [
    "Run for 1000 epochs and keeping 20% train-valid split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "894af9cbfcf2dca50e7407946cad318157b77d0a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/madhuhegde/.pyenv/versions/miniconda3-latest/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 7329 samples, validate on 1833 samples\n",
      "Epoch 1/1000\n",
      "4864/7329 [==================>...........] - ETA: 1:11 - loss: 2676.8676 - rmse: 51.7113"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-90bb142b57d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m                     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                     \u001b[0mvalidation_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m#data = (x_test, y_test),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m                     )\n\u001b[1;32m     12\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'seaborn-darkgrid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/miniconda3-latest/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.pyenv/versions/miniconda3-latest/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/miniconda3-latest/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/miniconda3-latest/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/miniconda3-latest/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "USE_SAVED_MODEL = False\n",
    "\n",
    "if USE_SAVED_MODEL == False:\n",
    "    history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs = 1000,\n",
    "                    batch_size = 256,\n",
    "                    validation_split = 0.2, #data = (x_test, y_test),\n",
    "                    callbacks = callbacks\n",
    "                    )\n",
    "    plt.style.use('seaborn-darkgrid')\n",
    "    plt.plot(history.history['rmse'])\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Root Mean Square Error')\n",
    "    plt.title('Model Training Error')\n",
    "    plt.show() \n",
    "    \n",
    "else:\n",
    "    model.load_weights(model_dir+\"capsule_model_weights.h5\")\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookid_dir = '../input/IdLookupTable.csv'\n",
    "lookid_data = pd.read_csv(lookid_dir)\n",
    "test_data = pd.read_csv(test_file)\n",
    "\n",
    "x_test = []\n",
    "for i in range(0,len(test_data)):\n",
    "    img = test_data['Image'][i].split(' ')\n",
    "    x_test.append(img)\n",
    "    \n",
    "x_test = np.array(x_test,dtype = 'float')\n",
    "x_test = x_test/255.0\n",
    "x_test = x_test.reshape(-1,96,96,1)    \n",
    "\n",
    "y_test = model.predict(x_test)\n",
    "y_test = np.clip(y_test,0,96)\n",
    "\n",
    "lookid_list = list(lookid_data['FeatureName'])\n",
    "imageID = list(lookid_data['ImageId']-1)\n",
    "pred_list = list(y_test)\n",
    "\n",
    "rowid = list(lookid_data['RowId'])\n",
    "\n",
    "feature = []\n",
    "for f in list(lookid_data['FeatureName']):\n",
    "    feature.append(lookid_list.index(f))\n",
    "    \n",
    "    \n",
    "submit_data = []\n",
    "for x,y in zip(imageID,feature):\n",
    "    submit_data.append(pred_list[x][y])\n",
    "rowid = pd.Series(rowid,name = 'RowId')\n",
    "loc = pd.Series(submit_data,name = 'Location')\n",
    "submission = pd.concat([rowid,loc],axis = 1)\n",
    "submission.to_csv('../output/w207_capsule_submission.csv',index = False)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "    sess = get_session()\n",
    "    clear_session()\n",
    "    sess.close()\n",
    "\n",
    "    try:\n",
    "        del model # this is from global space - change this as you need\n",
    "    except:\n",
    "        print(\"Model clear Failed\")\n",
    "    print(gc.collect())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
